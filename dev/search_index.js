var documenterSearchIndex = {"docs":
[{"location":"assets/LyceumExamples/README/#LyceumDocs.jl-Examples-1","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"","category":"section"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"Included in this project are all the examples found at LyceumDocs.jl, each available as either a .jl script or Jupyter notebook. To start, open up a Julia REPL with the project activated by executing the following in the directory containing this README:","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"julia --project=.","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"Now press the ] charcter to enter the Pkg REPL-mode. Your prompt should now look like this:","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"(LyceumExamples) pkg>","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"First, we'll add the LyceumRegistry so the package manager knows where to find the Lyceum packages:","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"(LyceumExamples) pkg> registry add https://github.com/Lyceum/LyceumRegistry.git\n   Cloning registry from \"https://github.com/Lyceum/LyceumRegistry.git\"\n     Added registry `LyceumRegistry` to `~/.julia/registries/LyceumRegistry`\n\n(LyceumExamples) pkg>","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"Next, call instantiate to download the required packages:","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"(LyceumExamples) pkg> instantiate\n  Updating registry at `~/.julia/registries/General`\n  Updating git-repo `https://github.com/JuliaRegistries/General.git`\n  Updating registry at `~/.julia/registries/LyceumRegistry`\n  Updating git-repo `https://github.com/Lyceum/LyceumRegistry.git`\n   Cloning git-repo `https://github.com/Lyceum/Lyceum.jl.git`\n\n   ...","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"You can now press Backspace to exit Pkg REPL-mode, returning you to the regular REPL:","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"julia>","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"To run the Julia scripts, simply include them into your current session:","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"julia> include(\"scripts/path/to/example.jl\")","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"Alternative, you can run the notebooks using IJulia:","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"julia> using IJulia\njulia> notebook(dir=\"notebooks/\"; detached=true)","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"The Jupyter notebook should open in your browser automatically. If not, go to http://localhost:8888/ in your browser of choice. From there you can browse and execute the various notebooks.","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"If you run into any trouble, don't hesitate to open an issue on the LyceumDocs.jl repo.","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"Enjoy!","category":"page"},{"location":"lyceumai/models/policies/#","page":"Policies","title":"Policies","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"lyceumai/models/policies/#Policies-1","page":"Policies","title":"Policies","text":"","category":"section"},{"location":"lyceumai/models/policies/#","page":"Policies","title":"Policies","text":"CurrentModule = LyceumDocs.LyceumAI","category":"page"},{"location":"lyceumai/models/policies/#DiagGaussianPolicy-1","page":"Policies","title":"DiagGaussianPolicy","text":"","category":"section"},{"location":"lyceumai/models/policies/#","page":"Policies","title":"Policies","text":"DiagGaussianPolicy\nDiagGaussianPolicy(::Any, ::AbstractVector)\nsample!(::AbstractRNG, ::AbsVec, ::DiagGaussianPolicy, ::AbsVec)\ngetaction!(::AbsVec, ::DiagGaussianPolicy, ::AbsVec)\nloglikelihood","category":"page"},{"location":"lyceumai/models/policies/#LyceumAI.DiagGaussianPolicy","page":"Policies","title":"LyceumAI.DiagGaussianPolicy","text":"struct DiagGaussianPolicy{Mean, Logstd<:(AbstractArray{T,1} where T)}\n\nDiagGaussianPolicy policy represents a stochastic control policy, represented as a multivariate Gaussian distribution of the form:\n\npi_theta(a  o) = mathcalN(mu_theta_1(o) Sigma_theta_2)\n\nwhere mu_theta_1 is a neural network, parameterized by theta_1, that maps an observation to a mean action and Sigma_theta_2 is a diagonal covariance matrix parameterized by theta_2, the diagonal entries of the matrix. Rather than tracking Sigma_theta_2 directly, we track the log standard deviations, which are easier to learn. Note that mu_theta_1 is a state-dependent mean while Sigma_theta_2 is a global covariance.\n\n\n\n\n\n","category":"type"},{"location":"lyceumai/models/policies/#LyceumAI.DiagGaussianPolicy-Tuple{Any,AbstractArray{T,1} where T}","page":"Policies","title":"LyceumAI.DiagGaussianPolicy","text":"DiagGaussianPolicy(meanNN, logstd; fixedlogstd)\n\n\nConstruct a DiagGaussianPolicy with a state-dependent mean meanNN and initial log-standard deviation logstd. If fixedlogstd is true, logstd will be treated as a constant. meanNN should be object that is compatible with Flux.jl and have the following signatures:\n\nmeanNN(obs::AbstractVector) –> action::AbstractVector\nmeanNN(obs::AbstractMatrix) –> action::AbstractMatrix\n\n\n\n\n\n","category":"method"},{"location":"lyceumai/models/policies/#LyceumBase.Tools.sample!-Tuple{Random.AbstractRNG,AbstractArray{T,1} where T,LyceumAI.DiagGaussianPolicy,AbstractArray{T,1} where T}","page":"Policies","title":"LyceumBase.Tools.sample!","text":"sample!([rng = GLOBAL_RNG, ]action, policy, feature)\n\nTreating policy as a stochastic policy, sample an action from policy, conditioned on feature, and store it in action.\n\n\n\n\n\n","category":"method"},{"location":"lyceumai/models/policies/#LyceumBase.getaction!-Tuple{AbstractArray{T,1} where T,LyceumAI.DiagGaussianPolicy,AbstractArray{T,1} where T}","page":"Policies","title":"LyceumBase.getaction!","text":"getaction!(action, policy, feature)\n\n\nTreating policy as a deterministic policy, compute the mean action of policy, conditioned on feature, and store it in action.\n\n\n\n\n\n","category":"method"},{"location":"lyceumai/models/policies/#LyceumAI.loglikelihood","page":"Policies","title":"LyceumAI.loglikelihood","text":"loglikelihood(policy, action, feature)\n\n\nReturn loglikelihood of action conditioned on feature for policy.\n\n\n\n\n\nloglikelihood(policy, actions, features)\n\n\nTreating each column of actions and features as a single action/feature, return a vector of the loglikelihoods of actions conditioned on features for policy.\n\n\n\n\n\n","category":"function"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"lyceummujoco/lyceummujoco/#LyceumMuJoCo-1","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"","category":"section"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"CurrentModule = LyceumDocs.LyceumMuJoCo","category":"page"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"LyceumMuJoCo uses MuJoCo.jl to provide the following:","category":"page"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"MuJoCo-based environments that implement the AbstractEnvironment interface.\nThe MJSim type and related utilities for combining a jlModel and jlData from   MuJoCo.jl to provide a full simulation.","category":"page"},{"location":"lyceummujoco/lyceummujoco/#AbstractMuJoCoEnvironment-1","page":"LyceumMuJoCo","title":"AbstractMuJoCoEnvironment","text":"","category":"section"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"To create a new MuJoCo-based environment, you will need to:","category":"page"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"Define a type Env that subtypes AbstractMuJoCoEnvironment <: LyceumBase.AbstractEnvironment.\nImplement the AbstractEnvironment interface.\nAdditionally, implement the method getsim(env::Env) --> MJSim that  returns the underlying MJSim which is used to provide defaults and other features.","category":"page"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"AbstractMuJoCoEnvironment\ngetsim","category":"page"},{"location":"lyceummujoco/lyceummujoco/#LyceumMuJoCo.AbstractMuJoCoEnvironment","page":"LyceumMuJoCo","title":"LyceumMuJoCo.AbstractMuJoCoEnvironment","text":"abstract type AbstractMuJoCoEnvironment <: LyceumBase.AbstractEnvironment\n\nThe supertype for all MuJoCo-based environments. Subtypes of AbstractMuJoCoEnvironment provide default functions for state, action, and observation (e.g. setstate!). For more information, see the documentation for MJSim.\n\n\n\n\n\n","category":"type"},{"location":"lyceummujoco/lyceummujoco/#LyceumMuJoCo.getsim","page":"LyceumMuJoCo","title":"LyceumMuJoCo.getsim","text":"getsim(env::LyceumMuJoCo.AbstractMuJoCoEnvironment) -> MJSim\n\n\nReturn env's underlying MJSim that defines the MuJoCo physics simulation. This is used for providing default state, action, and observation functions as well as the visualizer provided by LyceumMuJoCoViz.\n\n\n\n\n\n","category":"function"},{"location":"lyceummujoco/lyceummujoco/#MJSim-1","page":"LyceumMuJoCo","title":"MJSim","text":"","category":"section"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"MJSim\nsetstate!(::MJSim, ::RealVec)\ngetstate!(::RealVec, ::MJSim)\nobsspace(::MJSim)\ngetobs!(::RealVec, ::MJSim)\ngetobs(::MJSim)\nzeroctrl!(::MJSim)\nzerofullctrl!(::MJSim)\nforward!(::MJSim)\ntimestep(::MJSim)\ntime(::MJSim)","category":"page"},{"location":"lyceummujoco/lyceummujoco/#LyceumMuJoCo.MJSim","page":"LyceumMuJoCo","title":"LyceumMuJoCo.MJSim","text":"MJSim\n\nThe MJSim type couples a jlModel and jlData from MuJoCo.jl to provide a full simulation.\n\nThe following are the official/internal/minimum set of fields from jlData for state, observation, and action in MuJoCo:\n\nState: (time, qpos, qvel, act, mocap_pos, mocap_quat, userdata, qacc_warmstart)\nObservation: sensordata\nAction: (ctrl, qfrc_applied, xfrc_applied)\n\nMJSim follows this definition except for actions (e.g. setaction!), which is composed of justctrl` by default.\n\nFor more information, see the \"State and control\" section of the MuJoCo Documentation\n\nFields\n\nm::jlModel: contains the model description and is expected to remain constant.\nd::jlData: contains all dynamic variables and intermediate results.\nmn::Tuple: named-access version of MJSim.m provided by AxisArrays.jl.\ndn::Tuple: named-access version of MJSim.d provided by AxisArrays.jl.\ninitstate::Vector{mjtNum}: The initial state vector at the time when this MJSim was constructed.\nskip::Int: the number of times the simulation is integrated, yielding an effective simulation timestep of skip * m.opt.timestep.\n\n\n\n\n\n","category":"type"},{"location":"lyceummujoco/lyceummujoco/#LyceumBase.setstate!-Tuple{MJSim,AbstractArray{#s12,1} where #s12<:Real}","page":"LyceumMuJoCo","title":"LyceumBase.setstate!","text":"setstate!(sim, state)\n\n\nCopy the components of state to their respective fields in sim.d, namely:\n\n(time, qpos, qvel, act, mocap_pos, mocap_quat, userdata, qacc_warmstart)\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#LyceumBase.getstate!-Tuple{AbstractArray{#s12,1} where #s12<:Real,MJSim}","page":"LyceumMuJoCo","title":"LyceumBase.getstate!","text":"getstate!(state, sim)\n\n\nCopy the following state fields from sim.d into state:\n\n(time, qpos, qvel, act, mocap_pos, mocap_quat, userdata, qacc_warmstart)\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#LyceumBase.obsspace-Tuple{MJSim}","page":"LyceumMuJoCo","title":"LyceumBase.obsspace","text":"obsspace(sim::MJSim) -> Any\n\n\nReturn a description of sim's observation space.\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#LyceumBase.getobs!-Tuple{AbstractArray{#s12,1} where #s12<:Real,MJSim}","page":"LyceumMuJoCo","title":"LyceumBase.getobs!","text":"getobs!(obs, sim)\n\n\nCopy sim.d.sensordata into obs.\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#LyceumBase.getobs-Tuple{MJSim}","page":"LyceumMuJoCo","title":"LyceumBase.getobs","text":"getobs(sim::MJSim)\n\n\nReturn a copy of sim.d.sensordata.\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#LyceumMuJoCo.zeroctrl!-Tuple{MJSim}","page":"LyceumMuJoCo","title":"LyceumMuJoCo.zeroctrl!","text":"zeroctrl!(sim::MJSim) -> MJSim\n\n\nZero out sim.d.ctrl and compute new forward dynamics.\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#LyceumMuJoCo.zerofullctrl!-Tuple{MJSim}","page":"LyceumMuJoCo","title":"LyceumMuJoCo.zerofullctrl!","text":"zerofullctrl!(sim::MJSim) -> MJSim\n\n\nZero out all of the fields in sim.d that contribute to forward dynamics calculations, namely ctrl, qfrc_applied, and xfrc_applied, and compute the forward dynamics.\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#LyceumMuJoCo.forward!-Tuple{MJSim}","page":"LyceumMuJoCo","title":"LyceumMuJoCo.forward!","text":"forward!(sim::MJSim) -> MJSim\n\n\nCompute the forward dynamics of the simulation and store them in sim.d. Equivalent to mj_forward(sim.m, sim.d).\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#LyceumBase.timestep-Tuple{MJSim}","page":"LyceumMuJoCo","title":"LyceumBase.timestep","text":"timestep(sim::MJSim) -> Float64\n\n\nReturn the effective timestep of sim. Equivalent to sim.skip * sim.m.opt.timestep.\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#Base.Libc.time-Tuple{MJSim}","page":"LyceumMuJoCo","title":"Base.Libc.time","text":"time(sim::MJSim) -> Float64\n\n\nReturn the current simulation time, in seconds, of sim. Equivalent to sim.d.time.\n\n\n\n\n\n","category":"method"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"lyceumbase/abstractenvironment/#AbstractEnvironment-1","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"CurrentModule = LyceumDocs.LyceumBase","category":"page"},{"location":"lyceumbase/abstractenvironment/#Overview-1","page":"AbstractEnvironment","title":"Overview","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"What follows is the AbstractEnvironment interface in its entirety. For users implementing new environments, only a subset of the methods discussed below are required. The remaining methods are built off of that subset and should not be implemented directly. Some of the required methods may have defaults.","category":"page"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"Required Methods","category":"page"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"State\nstatespace(env)\ngetstate!(state, env)\nsetstate!(env, state)\nObservation\nobsspace(env)\ngetobs!(obs, env)\nAction\nactionspace(env)\ngetaction!(action, env)\nsetaction!(env, action)\nReward\nrewardspace(env)\ngetreward(env)\nEvaluation\nevalspace(env)\ngeteval(env)\nSimulation\nreset!(env)\nrandreset!(env)\nstep!(env)\nisdone(env)\ntimestep(env)\nBase.time(env)","category":"page"},{"location":"lyceumbase/abstractenvironment/#API-1","page":"AbstractEnvironment","title":"API","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"AbstractEnvironment","category":"page"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.AbstractEnvironment","page":"AbstractEnvironment","title":"LyceumBase.AbstractEnvironment","text":"AbstractEnvironment\n\nSupertype for all environments.\n\n\n\n\n\n","category":"type"},{"location":"lyceumbase/abstractenvironment/#State-1","page":"AbstractEnvironment","title":"State","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"statespace\ngetstate!\nsetstate!\ngetstate","category":"page"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.statespace","page":"AbstractEnvironment","title":"LyceumBase.statespace","text":"statespace(env::AbstractEnvironment) --> Shapes.AbstractShape\n\nReturns a subtype of Shapes.AbstractShape describing the state space of env.\n\nSee also: getstate!, setstate!, getstate.\n\n\n\n\n\nstatespace(sim::MJSim) -> Any\n\n\nReturn a description of sim's statespace.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.getstate!","page":"AbstractEnvironment","title":"LyceumBase.getstate!","text":"getstate!(state, env::AbstractEnvironment)\n\nStore the current state of env in state, where state conforms to the state space returned by statespace(env).\n\nSee also: statespace, setstate!, getstate.\n\n\n\n\n\ngetstate!(state, sim)\n\n\nCopy the following state fields from sim.d into state:\n\n(time, qpos, qvel, act, mocap_pos, mocap_quat, userdata, qacc_warmstart)\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.setstate!","page":"AbstractEnvironment","title":"LyceumBase.setstate!","text":"setstate!(env::AbstractEnvironment, state)\n\nSet the state of env to state, where state conforms to the state space returned by statespace(env).\n\nSee also: statespace, getstate!, getstate.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes must guarantee that calls to other \"getter\" functions (e.g. getreward) after a call to setstate! reflect the new, passed-in state.\n\n\n\n\n\nsetstate!(sim, state)\n\n\nCopy the components of state to their respective fields in sim.d, namely:\n\n(time, qpos, qvel, act, mocap_pos, mocap_quat, userdata, qacc_warmstart)\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.getstate","page":"AbstractEnvironment","title":"LyceumBase.getstate","text":"getstate(env::AbstractEnvironment)\n\nGet the current state of env. The returned value will be an object conforming to the state space returned by statespace(env).\n\nSee also: statespace, getstate!, setstate!.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should implement statespace and getstate!, which are used internally by getstate.\n\n\n\n\n\ngetstate(sim)\n\n\nReturn a flattened vector of the following state fields from sim.d:\n\n(time, qpos, qvel, act, mocap_pos, mocap_quat, userdata, qacc_warmstart)\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#Observation-1","page":"AbstractEnvironment","title":"Observation","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"obsspace\ngetobs!\ngetobs","category":"page"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.obsspace","page":"AbstractEnvironment","title":"LyceumBase.obsspace","text":"obsspace(env::AbstractEnvironment) --> Shapes.AbstractShape\n\nReturns a subtype of Shapes.AbstractShape describing the observation space of env.\n\nSee also: getobs!, getobs.\n\n\n\n\n\nobsspace(sim::MJSim) -> Any\n\n\nReturn a description of sim's observation space.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.getobs!","page":"AbstractEnvironment","title":"LyceumBase.getobs!","text":"getobs!(obs, env::AbstractEnvironment)\n\nStore the current observation of env in obs, where obs conforms to the observation space returned by obsspace(env).\n\nSee also: obsspace, getobs.\n\n\n\n\n\ngetobs!(obs, sim)\n\n\nCopy sim.d.sensordata into obs.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.getobs","page":"AbstractEnvironment","title":"LyceumBase.getobs","text":"getobs(env::AbstractEnvironment)\n\nGet the current observation of env. The returned value will be an object conforming to the observation space returned by obsspace(env).\n\nSee also: obsspace, getobs!.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should implement obsspace and getobs!, which are used internally by getobs.\n\n\n\n\n\ngetobs(sim::MJSim)\n\n\nReturn a copy of sim.d.sensordata.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#Action-1","page":"AbstractEnvironment","title":"Action","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"actionspace\ngetaction!\nsetaction!\ngetaction","category":"page"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.actionspace","page":"AbstractEnvironment","title":"LyceumBase.actionspace","text":"actionspace(env::AbstractEnvironment) --> Shapes.AbstractShape\n\nReturns a subtype of Shapes.AbstractShape describing the action space of env.\n\nSee also: getaction!, setaction!, getaction.\n\n\n\n\n\nactionspace(sim::MJSim) -> Any\n\n\nReturn a description of sim's action space.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.getaction!","page":"AbstractEnvironment","title":"LyceumBase.getaction!","text":"getaction!(action, env::AbstractEnvironment)\n\nStore the current action of env in action, where action conforms to the action space returned by actionspace(env).\n\nSee also: actionspace, setaction!, getaction.\n\n\n\n\n\ngetaction!(action::AbstractArray{#s12,1} where #s12<:Real, sim::MJSim) -> Any\n\n\nCopy sim.d.ctrl into action.\n\n\n\n\n\ngetaction!(action, policy, feature)\n\n\nTreating policy as a deterministic policy, compute the mean action of policy, conditioned on feature, and store it in action.\n\n\n\n\n\ngetaction!(action, state, m; nthreads)\n\n\nStarting from the environment's state, perform one step of the MPPI algorithm and store the resulting action in action. The trajectory sampling portion of MPPI is done in parallel using nthreads threads.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.setaction!","page":"AbstractEnvironment","title":"LyceumBase.setaction!","text":"setaction!(env::AbstractEnvironment, action)\n\nSet the action of env to action, where action conforms to the action space returned by actionspace(env).\n\nSee also: actionspace, getaction!, getaction.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes must guarantee that calls to other \"getter\" functions (e.g. getreward) after a call to setaction! reflect the new, passed-in action.\n\n\n\n\n\nsetaction!(sim::MJSim, action::AbstractArray{#s12,1} where #s12<:Real) -> MJSim\n\n\nCopy action into sim.d.ctrl into action and compute the new forward dynamics.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.getaction","page":"AbstractEnvironment","title":"LyceumBase.getaction","text":"getaction(env::AbstractEnvironment)\n\nGet the current action of env. The returned value will be an object conforming to the action space returned by actionspace(env).\n\nSee also: actionspace, getaction!, setaction!.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should implement actionspace and getaction!, which are used internally by getaction.\n\n\n\n\n\ngetaction(sim::MJSim, action::AbstractArray{#s12,1} where #s12<:Real) -> Any\n\n\nReturn a copy of sim.d.ctrl.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#Reward-1","page":"AbstractEnvironment","title":"Reward","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"rewardspace\ngetreward","category":"page"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.rewardspace","page":"AbstractEnvironment","title":"LyceumBase.rewardspace","text":"rewardspace(env::AbstractEnvironment) --> Shapes.AbstractShape\n\nReturns a subtype of Shapes.AbstractShape describing the reward space of env. Defaults to Shapes.ScalarShape{Float64}().\n\nSee also: getreward.\n\nnote: Note\nCurrently, only scalar spaces are supported (e.g. Shapes.ScalarShape).\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.getreward","page":"AbstractEnvironment","title":"LyceumBase.getreward","text":"getreward(state, action, observation, env::AbstractEnvironment)\n\nGet the current reward of env as a function of state, action, and observation. The returned value will be an object conforming to the reward space returned by rewardspace(env).\n\nSee also: rewardspace.\n\nnote: Note\nCurrently, only scalar rewards are supported, so there is no in-place getreward!.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should be careful to ensure that the result of getreward is purely a function of state/action/observation and not any internal, dynamic state contained in env.\n\n\n\n\n\ngetreward(env::AbstractEnvironment)\n\nGet the current reward of env.\n\nInternally calls getreward(getstate(env), getaction(env), getobs(env), env).\n\nSee also: rewardspace.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should implement getreward(state, action, observation, env).\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#Evaluation-1","page":"AbstractEnvironment","title":"Evaluation","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"evalspace\ngeteval","category":"page"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.evalspace","page":"AbstractEnvironment","title":"LyceumBase.evalspace","text":"evalspace(env::AbstractEnvironment) --> Shapes.AbstractShape\n\nReturns a subtype of Shapes.AbstractShape describing the evaluation space of env. Defaults to Shapes.ScalarShape{Float64}().\n\nSee also: geteval.\n\nnote: Note\nCurrently, only scalar evaluation spaces are supported (e.g. Shapes.ScalarShape).\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.geteval","page":"AbstractEnvironment","title":"LyceumBase.geteval","text":"geteval(state, action, observation, env::AbstractEnvironment)\n\nGet the current evaluation metric of env as a function of state, action, and observation. The returned value will be an object conforming to the evaluation space returned by evalspace(env).\n\nOften times reward functions are heavily \"shaped\" and hard to interpret. For example, the reward function for bipedal walking may include root pose, ZMP terms, control costs, etc., while success can instead be simply evaluated by distance of the root along an axis. The evaluation metric serves to fill this gap.\n\nThe default behavior is to return getreward(state, action, observation, env::AbstractEnvironment).\n\nSee also: evalspace.\n\nnote: Note\nCurrently, only scalar evaluation metrics are supported, so there is no in-place geteval!.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should be careful to ensure that the result of geteval is purely a function of state/action/observation and not any internal, dynamic state contained in env.\n\n\n\n\n\ngeteval(env::AbstractEnvironment)\n\nGet the current evaluation metric of env.\n\nInternally calls geteval(getstate(env), getaction(env), getobs(env), env).\n\nSee also: evalspace.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should implement geteval(state, action, obs, env).\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#Simulation-1","page":"AbstractEnvironment","title":"Simulation","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"reset!\nrandreset!\nstep!\nisdone\ntimestep\nBase.time","category":"page"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.reset!","page":"AbstractEnvironment","title":"LyceumBase.reset!","text":"reset!(env::AbstractEnvironment)\n\nReset env to a fixed, initial state with zero/passive controls.\n\n\n\n\n\nreset!(m::LyceumAI.MPPI) -> LyceumAI.MPPI\n\n\nResets the canonical control vector to zeros.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.randreset!","page":"AbstractEnvironment","title":"LyceumBase.randreset!","text":"randreset!([rng::Random.AbstractRNG, ], env::AbstractEnvironment)\n\nReset env to a random state with zero/passive controls.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should implement randreset!(rng, env).\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.step!","page":"AbstractEnvironment","title":"LyceumBase.step!","text":"step!(env::AbstractEnvironment)\n\nAdvance env forward by one timestep.\n\nSee also: timestep.\n\n\n\n\n\nstep!(sim::MJSim) -> MJSim\nstep!(sim::MJSim, skip::Integer) -> MJSim\n\n\nStep the simulation by skip steps, where skip defaults to sim.skip.\n\nState-dependent controls (e.g. the ctrl, xfrc_applied, qfrc_applied fields of sim.d) should be set before calling step!.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.isdone","page":"AbstractEnvironment","title":"LyceumBase.isdone","text":"isdone(state, action, observation, env::AbstractEnvironment) --> Bool\n\nReturns true if state, action, and observation meet an early termination condition for env. Defaults to false.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should be careful to ensure that the result of isdone is purely a function of state/action/observation and not any internal, dynamic state contained in env.\n\n\n\n\n\nisdone(env::AbstractEnvironment)\n\nReturns true if env has met an early termination condition.\n\nInternally calls isdone(getstate(env), getaction(env), getobs(env), env).\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should implement isdone(state, action, obs, env).\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.timestep","page":"AbstractEnvironment","title":"LyceumBase.timestep","text":"timestep(env::AbstractEnvironment)\n\nReturn the internal simulation timestep, in seconds, of env.\n\nSee also: Base.time.\n\nExamples\n\nenv = FooEnv()\nreset!(env)\nt1 = time(env)\nstep!(env)\nt2 = time(env)\n@assert timestep(env) == (t2 - t1)\n\n\n\n\n\ntimestep(sim::MJSim) -> Float64\n\n\nReturn the effective timestep of sim. Equivalent to sim.skip * sim.m.opt.timestep.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#Base.Libc.time","page":"AbstractEnvironment","title":"Base.Libc.time","text":"Base.time(env::AbstractEnvironment)\n\nReturns the current simulation time, in seconds, of env. By convention, time(env) should return zero after a call to reset!(env) or randreset!(env).\n\nSee also: timestep.\n\n\n\n\n\ntime(sim::MJSim) -> Float64\n\n\nReturn the current simulation time, in seconds, of sim. Equivalent to sim.d.time.\n\n\n\n\n\n","category":"function"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"EditURL = \"https://github.com/Lyceum/LyceumDocs.jl/blob/master/docs/src/examples/NPG.jl\"","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"note: Running examples locally\nThis example and more are also available as Julia scripts and Jupyter notebooks.See the how-to page for more information.","category":"page"},{"location":"examples/NPG/#Learning-a-Control-Policy-1","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"","category":"section"},{"location":"examples/NPG/#Overview-1","page":"Learning a Control Policy","title":"Overview","text":"","category":"section"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"In this example we walk through the process of setting up an experiment that runs Natural Policy Gradient (or more recently in this work). This is an on-policy reinforcement learning method that is comparable to TRPO, PPO, and other policy gradient methods. See the documentation for NaturalPolicyGradient for full implementation details.","category":"page"},{"location":"examples/NPG/#The-Code-1","page":"Learning a Control Policy","title":"The Code","text":"","category":"section"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"First, let's go head and grab all the dependencies:","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"using LinearAlgebra, Random, Statistics # From Stdlib\nusing LyceumAI                          # For the NPG controller\nusing LyceumMuJoCo                      # For the Hopper environment\nusing Flux                              # For our neural networks needs\nusing UniversalLogger                   # For logging experiment data\nusing Plots                             # For plotting the results\nusing LyceumBase.Tools                  # Miscellaneous utilities","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"We first instantiate a HopperV2 environment to grab useful environment-specific values, such as the size of the observation and action vectors:","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"env = LyceumMuJoCo.HopperV2();\ndobs, dact = length(obsspace(env)), length(actionspace(env));\nnothing #hide","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"We'll also seed the per-thread global RNGs:","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"seed_threadrngs!(1)","category":"page"},{"location":"examples/NPG/#Policy-Gradient-Components-1","page":"Learning a Control Policy","title":"Policy Gradient Components","text":"","category":"section"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"Policy Gradient methods require a policy: a function that takes in the state/observations of the agent, and outputs an action i.e. action = π(obs). In much of Deep RL, the policy takes the form of a neural network which can be built on top of the Flux.jl library. We utilize a stochastic policy in this example. Specifically, our policy is represented as a multivariate Gaussian distribution of the form:","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"pi(a  o) = mathcalN(mu_theta_1(o) Sigma_theta_2)","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"where mu_theta_1 is a neural network, parameterized by theta_1, that maps an observation to a mean action and Sigma_theta_2 is a diagonal covariance matrix parameterized by theta_2, the diagonal entries of the matrix. For mu_theta_1 we utilize a 2-layer neural network, where each layer has a \"width\" of 32. We use tanh activations for each hidden layer and initialize the network weights with Glorot Uniform initializations. Rather than tracking Sigma_theta_2 directly, we track the log standard deviations, which are easier to learn. We initialize log textdiag(Sigma_theta_2) as zeros(dact), i.e. a Vector of length dact, initialized to 0. Both theta_1 and theta_2 are learned in this example. Note that mu_theta_1 is a state-dependent mean while Sigma_theta_2 is a global covariance.","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"const policy = DiagGaussianPolicy(\n    multilayer_perceptron(\n        dobs,\n        32,\n        32,\n        dact;\n        σ = tanh,\n        initb = Flux.glorot_uniform,\n        initb_final = Flux.glorot_uniform,\n        dtype = Float32,\n    ),\n    zeros(Float32, dact),\n);\nnothing #hide","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"This NPG implementation uses Generalized Advantaged Estimation, which requires an estimate of the value function, value(state), which we represent using a 2-layer, feedforward neural network where each layer has a width of 128 and uses the ReLU activation function. The model weights are initialized using Glorot Uniform initialization as above.","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"const value = multilayer_perceptron(\n    dobs,\n    128,\n    128,\n    1;\n    σ = Flux.relu,\n    initb = Flux.glorot_uniform,\n    initb_final = Flux.glorot_uniform,\n    dtype = Float32,\n);\nnothing #hide","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"Next, we set up the optimization pipeline for value. We use a mini-batch size of 64 and the ADAM optimizer. FluxTrainer is an iterator that loops on the model provided, performing a single step of gradient descent at each iteration. The result at each loop is passed to stopcb below, so you can quit after a number of epochs, convergence, or other criteria; here it's capped at two epochs. See the documentation for FluxTrainer for more information.","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"valueloss(bl, X, Y) = Flux.mse(vec(bl(X)), vec(Y))\nstopcb(x) = x.nepochs > 2\nconst valuetrainer = FluxTrainer(\n    optimiser = ADAM(1e-3),\n    szbatch = 64,\n    lossfn = valueloss,\n    stopcb = stopcb\n);\nnothing #hide","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"The NaturalPolicyGradient iterator is a type that pre-allocates all necesary data structures and performs one gradient update to policy at each iteration. We first pass in a constructor that given n returns n instances of LyceumMuJoCo.HopperV2, all sharing the same jlModel, to allow NaturalPolicyGradient to allocate per-thread environments and enable performant, parallel sampling from policy. We then pass in the policy, value, and valuetrainer instances constructed above and override a few of the default NaturalPolicyGradient parameters: gamma, gaelambda, and norm_step_size. Finally, we set the max trajectory length Hmax and total number of samples per iteration, N. Under the hood, NaturalPolicyGradient will use approximately div(N, Hmax) threads to perform the sampling.","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"const npg = NaturalPolicyGradient(\n    n -> tconstruct(LyceumMuJoCo.HopperV2, n),\n    policy,\n    value,\n    valuetrainer;\n    gamma = 0.995,\n    gaelambda = 0.97,\n    norm_step_size = 0.05,\n    Hmax = 1000,\n    N = 10240,\n);\nnothing #hide","category":"page"},{"location":"examples/NPG/#Running-Experiments-1","page":"Learning a Control Policy","title":"Running Experiments","text":"","category":"section"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"Finally, let's spin on our iterator 200 times, plotting every 20 iterations. This lets us break out of the loop if certain conditions are met, or re-start training manually if needed. We of course wish to track results, so we create a ULogger and Experiment to which we can save data. We also have useful timing information displayed every 20 iterations to better understand the performance of our algorithm and identify any potential bottlenecks. Rather than iterating on npg at the global scope, we'll do it inside of a function to avoid the performance issues associated with global variables as discussed in the Julia performance tips. Note, to keep the Markdown version of this tutorial readable, we skip the plots and performance statistics. To enable them, simply call hopper_NPG(npg, true).","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"function hopper_NPG(npg::NaturalPolicyGradient, plot::Bool)\n    exper = Experiment(\"/tmp/hopper_example.jlso\", overwrite = true)\n    # Walks, talks, and acts like a Julia logger. See the UniversalLogger.jl docs for more info.\n    lg = ULogger()\n    for (i, state) in enumerate(npg)\n        if i > 200\n            # serialize some stuff and quit\n            exper[:policy] = npg.policy\n            exper[:value] = npg.value\n            exper[:etype] = LyceumMuJoCo.HopperV2\n            exper[:meanstates] = state.meanbatch\n            exper[:stocstates] = state.stocbatch\n            break\n        end\n\n        # log everything in `state` except meanbatch and stocbatch\n        push!(lg, :algstate, filter_nt(state, exclude = (:meanbatch, :stocbatch)))\n\n        if plot && mod(i, 20) == 0\n            x = lg[:algstate]\n            # The following are helper functions for plotting to the terminal.\n            # The first plot displays the `geteval` function for our stochastic\n            # and mean policy rollouts.\n            display(expplot(\n                Line(x[:stocterminal_eval], \"StocLastE\"),\n                Line(x[:meanterminal_eval], \"MeanLastE\"),\n                title = \"Evaluation Score, Iter=$i\",\n                width = 60,\n                height = 8,\n            ))\n            # While the second one similarly plots `getreward`.\n            display(expplot(\n                Line(x[:stoctraj_reward], \"StocR\"),\n                Line(x[:meantraj_reward], \"MeanR\"),\n                title = \"Reward, Iter=$i\",\n                width = 60,\n                height = 8,\n            ))\n\n            # The following are timing values for various parts of the Natural Policy Gradient\n            # algorithm at the last iteration, useful for finding performance bottlenecks\n            # in the algorithm.\n            println(\"elapsed_sampled  = \", state.elapsed_sampled)\n            println(\"elapsed_gradll   = \", state.elapsed_gradll)\n            println(\"elapsed_vpg      = \", state.elapsed_vpg)\n            println(\"elapsed_cg       = \", state.elapsed_cg)\n            println(\"elapsed_valuefit = \", state.elapsed_valuefit)\n        end\n    end\n    exper, lg\nend\nexper, lg = hopper_NPG(npg, false);\nnothing #hide","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"Let's go ahead and plot the final reward trajectory for our stochastic and mean policies to see how we did:","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"plot(\n    [lg[:algstate][:meantraj_reward] lg[:algstate][:stoctraj_reward]],\n    labels = [\"Mean Policy\" \"Stochastic Policy\"],\n    title = \"HopperV2 Reward\",\n    legend = :bottomright,\n)","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"We'll also plot the evaluations:","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"plot(\n    [lg[:algstate][:meantraj_eval] lg[:algstate][:stoctraj_eval]],\n    labels = [\"Mean Policy\" \"Stochastic Policy\"],\n    title = \"HopperV2 Eval\",\n    legend = :bottomright,\n)","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"Finally, we save the logged results to exper for later review:","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"exper[:logs] = get(lg)\nfinish!(exper); # flushes everything to disk\nnothing #hide","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"","category":"page"},{"location":"examples/NPG/#","page":"Learning a Control Policy","title":"Learning a Control Policy","text":"This page was generated using Literate.jl.","category":"page"},{"location":"lyceumai/algorithms/mppi/#","page":"MPPI","title":"MPPI","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"lyceumai/algorithms/mppi/#Model-Predictive-Path-Integral-Control-1","page":"MPPI","title":"Model-Predictive Path Integral Control","text":"","category":"section"},{"location":"lyceumai/algorithms/mppi/#","page":"MPPI","title":"MPPI","text":"Implements Model-Predictive Path Integral Control, a stochastic sampling based model predictive control method. For further information, see the following papers:","category":"page"},{"location":"lyceumai/algorithms/mppi/#","page":"MPPI","title":"MPPI","text":"Information Theoretic MPC for Model-Based Reinforcement Learning\nAggressive Driving with Model Predictive Path Integral Control","category":"page"},{"location":"lyceumai/algorithms/mppi/#","page":"MPPI","title":"MPPI","text":"CurrentModule = LyceumDocs.LyceumAI","category":"page"},{"location":"lyceumai/algorithms/mppi/#","page":"MPPI","title":"MPPI","text":"MPPI\ngetaction!(::AbstractVector, ::Any, ::MPPI)\nreset!(::MPPI)","category":"page"},{"location":"lyceumai/algorithms/mppi/#LyceumAI.MPPI","page":"MPPI","title":"LyceumAI.MPPI","text":"struct MPPI{DT<:AbstractFloat, nu, Covar<:AbstractArray{DT<:AbstractFloat,2}, Value, Env, Init, Obs, State}\n\nMPPI{DT<:AbstractFloat}(args...; kwargs...) -> MPPI\nMPPI(args...; kwargs...) -> MPPI\n\nConstruct an instance of  MPPI with args and kwargs, where DT <: AbstractFloat is the element type used for pre-allocated buffers, which defaults to Float32.\n\nIn the following explanation of the MPPI constructor, we use the following notation:\n\nU::Matrix: the canonical control vector (u_1 u_2 dots u_H), where   size(U) == (length(actionspace(env)), H).\n\nKeywords\n\nenv_tconstructor: a function with signature env_tconstructor(n) that returns n   instances of T, where T <: AbstractEnvironment.\nH::Integer: Length of sampled trajectories.\nK::Integer: Number of trajectories to sample.\ncovar::AbstractMatrix: The covariance matrix for the Normal distribution from which   control pertubations are sampled from.\ngamma::Real: Reward discount, applied as gamma^(t - 1) * reward[t].\nlambda::Real: Temperature parameter for the exponential reweighting of sampled   trajectories. In the limit that lambda approaches 0, U is set to the highest reward   trajectory. Conversely, as lambda approaches infinity, U is computed as the   unweighted-average of the samples trajectories.\nvalue: a function mapping observations to scalar rewards, with the signature   value(obs::AbstractVector) --> reward::Real\ninitfn!: A function with the signature initfn!(U::Matrix) used for   re-initializing U after shifting it. Defaults to setting the last   element of U to 0.\n\n\n\n\n\n","category":"type"},{"location":"lyceumai/algorithms/mppi/#LyceumBase.getaction!-Tuple{AbstractArray{T,1} where T,Any,LyceumAI.MPPI}","page":"MPPI","title":"LyceumBase.getaction!","text":"getaction!(action, state, m; nthreads)\n\n\nStarting from the environment's state, perform one step of the MPPI algorithm and store the resulting action in action. The trajectory sampling portion of MPPI is done in parallel using nthreads threads.\n\n\n\n\n\n","category":"method"},{"location":"lyceumai/algorithms/mppi/#LyceumBase.reset!-Tuple{LyceumAI.MPPI}","page":"MPPI","title":"LyceumBase.reset!","text":"reset!(m::LyceumAI.MPPI) -> LyceumAI.MPPI\n\n\nResets the canonical control vector to zeros.\n\n\n\n\n\n","category":"method"},{"location":"examples/visualize/#","page":"Using the Visualizer","title":"Using the Visualizer","text":"EditURL = \"https://github.com/Lyceum/LyceumDocs.jl/blob/master/docs/src/examples/visualize.jl\"","category":"page"},{"location":"examples/visualize/#","page":"Using the Visualizer","title":"Using the Visualizer","text":"note: Running examples locally\nThis example and more are also available as Julia scripts and Jupyter notebooks.See the how-to page for more information.","category":"page"},{"location":"examples/visualize/#Using-the-Visualizer-1","page":"Using the Visualizer","title":"Using the Visualizer","text":"","category":"section"},{"location":"examples/visualize/#Overview-1","page":"Using the Visualizer","title":"Overview","text":"","category":"section"},{"location":"examples/visualize/#","page":"Using the Visualizer","title":"Using the Visualizer","text":"In this example, we walk through how to use LyceumMuJoCoViz.jl to playback saved trajectories and interact with a saved policy in real time using the policy we learned in the \"Learning a control policy\" example.","category":"page"},{"location":"examples/visualize/#The-Code-1","page":"Using the Visualizer","title":"The Code","text":"","category":"section"},{"location":"examples/visualize/#","page":"Using the Visualizer","title":"Using the Visualizer","text":"First, let's go head and grab all the dependencies:","category":"page"},{"location":"examples/visualize/#","page":"Using the Visualizer","title":"Using the Visualizer","text":"using LyceumAI         # For `NaturalPolicyGradient` and `DiagGaussianPolicy`\nusing Shapes           # For the `allocate` function\nusing LyceumMuJoCo     # For the HopperV2 environment\nusing LyceumMuJoCoViz  # For the visualizer itself\nusing FastClosures     # For helping avoid performance issues with closures, discussed below\nusing JLSO             # For loading saved data","category":"page"},{"location":"examples/visualize/#","page":"Using the Visualizer","title":"Using the Visualizer","text":"Here we demonstrate two modes of visualizing results of an algorithm like NaturalPolicyGradient or MPPI from LyceumAI: playing back saved trajectories and interacting with a policy or controller in real time. For the former, we need only pass to the visualize function our saved trajectories as a vector of matricies, where each element of the vector is a matrix of size (length(statespace(env)), T), where T is the length the trajectory. Note that each trajectory can be of a different length. For the latter, we pass a control callback to visualize that will be called each time step!(env) is called.","category":"page"},{"location":"examples/visualize/#","page":"Using the Visualizer","title":"Using the Visualizer","text":"function viz_hopper_NPG()\n    # Load our experiment results\n    x = JLSO.load(\"/tmp/hopper_example.jlso\")\n\n    env = LyceumMuJoCo.HopperV2()\n\n    # Load the states from our saved trajectory, as well as the learned policy.\n    states = x[\"stocstates\"].states\n    pol = x[\"policy\"]\n\n    # Allocate some buffers for our control callback.\n    a = allocate(actionspace(env))\n    o = allocate(obsspace(env))\n\n    # As discussed in the Julia performance tips, captured variables\n    # (e.g. in a closure) can sometimes hinder performance. To help with that,\n    # we use `let` blocks as suggested.\n    ctrlfn = let o = o, a = a, pol = pol\n        function (env)\n            getobs!(o, env)\n            a .= pol(o)\n            setaction!(env, a)\n        end\n    end\n\n    visualize(env, controller = ctrlfn, trajectories = states)\nend","category":"page"},{"location":"examples/visualize/#","page":"Using the Visualizer","title":"Using the Visualizer","text":"Now just call the function to see the visualizer appear!","category":"page"},{"location":"examples/visualize/#","page":"Using the Visualizer","title":"Using the Visualizer","text":"viz_hopper_NPG()","category":"page"},{"location":"examples/visualize/#","page":"Using the Visualizer","title":"Using the Visualizer","text":"You should see the following: (Image: visualizer)","category":"page"},{"location":"examples/visualize/#","page":"Using the Visualizer","title":"Using the Visualizer","text":"","category":"page"},{"location":"examples/visualize/#","page":"Using the Visualizer","title":"Using the Visualizer","text":"This page was generated using Literate.jl.","category":"page"},{"location":"lyceummujocoviz/lyceummujocoviz/#","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"lyceummujocoviz/lyceummujocoviz/#LyceumMuJoCoViz-1","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"","category":"section"},{"location":"lyceummujocoviz/lyceummujocoviz/#","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"CurrentModule = LyceumDocs.LyceumMuJoCoViz","category":"page"},{"location":"lyceummujocoviz/lyceummujocoviz/#Overview-1","page":"LyceumMuJoCoViz","title":"Overview","text":"","category":"section"},{"location":"lyceummujocoviz/lyceummujocoviz/#","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"LyceumMuJoCoViz is an OpenGL-based interactive visualizer for MuJoCo-based models and environments. It allows for visualizing passive dynamics, playing back recorded trajectories, and interacting with a policy or controller in real time. Several additional features are provided:","category":"page"},{"location":"lyceummujocoviz/lyceummujocoviz/#","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"Interact with the simulation using the mouse and keyboard.\n\"Burst mode\", the ability to render multiple snapshots along an entire trajectory at once.\nRun the simulation faster or slower than real time, or in reverse.\nRecord a high-resolution video of the rendered scene.\nStep through time, one or several timesteps at a time.","category":"page"},{"location":"lyceummujocoviz/lyceummujocoviz/#","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"All of these commands are controlled via keyboard shortcuts which are displayed in a help window when the visualizer is launched:","category":"page"},{"location":"lyceummujocoviz/lyceummujocoviz/#","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"<img class=\"center\" src=\"../examples/visualize.png\" width=\"75%\"/>","category":"page"},{"location":"lyceummujocoviz/lyceummujocoviz/#API-1","page":"LyceumMuJoCoViz","title":"API","text":"","category":"section"},{"location":"lyceummujocoviz/lyceummujocoviz/#","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"visualize","category":"page"},{"location":"lyceummujocoviz/lyceummujocoviz/#LyceumMuJoCoViz.visualize","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz.visualize","text":"visualize(model::Union{LyceumMuJoCo.AbstractMuJoCoEnvironment, MJSim}; trajectories, controller)\n\n\nStarts an interactive visualization of model, which can be either a valid subtype of AbstractMuJoCoEnvironment or just a MJSim simulation. The visualizer has several \"modes\" that allow you to visualize passive dynamics, play back recorded trajectories, and run a controller interactively. The passive dynamics mode depends only on model and is always available, while the other modes are specified by the keyword arguments below.\n\nFor more information, see the on-screen help menu.\n\nKeywords\n\ntrajectories::AbstractVector{<:AbstractMatrix}: a vector of trajectories, where each   trajectory is an AbstractMatrix of states with size (length(statespace(model)), T) and   T is the length of the trajectory. Note that each trajectory can have different length.\ncontroller: a callback function with the signature controller(model), called at each   timestep, that that applys a control input to the system.\n\nExamples\n\nusing LyceumMuJoCo, LyceumMuJoCoViz\nenv = LyceumMuJoCo.HopperV2()\nT = 100\nstates = Array(undef, statespace(env), T)\nfor t = 1:T\n    step!(env)\n    states[:, t] .= getstate(env)\nend\nvisualize(\n    env,\n    trajectories=[states],\n    controller = env -> setaction!(env, rand(actionspace(env)))\n)\n\n\n\n\n\n","category":"function"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"examples/example_howto/#Running-Examples-Locally-1","page":"Running Examples Locally","title":"Running Examples Locally","text":"","category":"section"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"All examples can be found as Julia scripts and Jupyter notebooks in a self-contained Julia project which is available here: examples.tar.gz.","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"Once downloaded, extract the archive with your tool of choice. On Linux machines, you can run:","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"tar xzf examples.tar.gz","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"which will produce a folder in the same directory named \"LyceumExamples\". Inside, you'll find a README.md, reproduced below, with further instructions.","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"","category":"page"},{"location":"examples/example_howto/#LyceumDocs.jl-Examples-1","page":"Running Examples Locally","title":"LyceumDocs.jl Examples","text":"","category":"section"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"Included in this project are all the examples found at LyceumDocs.jl, each available as either a .jl script or Jupyter notebook. To start, open up a Julia REPL with the project activated by executing the following in the directory containing this README:","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"julia --project=.","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"Now press the ] charcter to enter the Pkg REPL-mode. Your prompt should now look like this:","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"(LyceumExamples) pkg>","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"First, we'll add the LyceumRegistry so the package manager knows where to find the Lyceum packages:","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"(LyceumExamples) pkg> registry add https://github.com/Lyceum/LyceumRegistry.git\n   Cloning registry from \"https://github.com/Lyceum/LyceumRegistry.git\"\n     Added registry `LyceumRegistry` to `~/.julia/registries/LyceumRegistry`\n\n(LyceumExamples) pkg>","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"Next, call instantiate to download the required packages:","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"(LyceumExamples) pkg> instantiate\n  Updating registry at `~/.julia/registries/General`\n  Updating git-repo `https://github.com/JuliaRegistries/General.git`\n  Updating registry at `~/.julia/registries/LyceumRegistry`\n  Updating git-repo `https://github.com/Lyceum/LyceumRegistry.git`\n   Cloning git-repo `https://github.com/Lyceum/Lyceum.jl.git`\n\n   ...","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"You can now press Backspace to exit Pkg REPL-mode, returning you to the regular REPL:","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"julia>","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"To run the Julia scripts, simply include them into your current session:","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"julia> include(\"scripts/path/to/example.jl\")","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"Alternative, you can run the notebooks using IJulia:","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"julia> using IJulia\njulia> notebook(dir=\"notebooks/\"; detached=true)","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"The Jupyter notebook should open in your browser automatically. If not, go to http://localhost:8888/ in your browser of choice. From there you can browse and execute the various notebooks.","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"If you run into any trouble, don't hesitate to open an issue on the LyceumDocs.jl repo.","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"Enjoy!","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"lyceummujoco/environments/#Environments-1","page":"Environments","title":"Environments","text":"","category":"section"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"CurrentModule = LyceumDocs.LyceumMuJoCo","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"LyceumMuJoCo comes with a variety of environments:","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"Lyceum Suite: A collection of Lyceum-custom environments.\nGym: Ports of environments from OpenAI's gym.\ndm_control: Ports of environments from DeepMind's dm_control","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"We highly encourage that users get familiar with the source codes of these environments and modify them as you see fit. We also hope that they serve as inspiration for creating new, interesting environments. As always, if you make something cool we'd gladly welcome a pull request to incorporate it into Lyceum!","category":"page"},{"location":"lyceummujoco/environments/#Lyceum-Suite-1","page":"Environments","title":"Lyceum Suite","text":"","category":"section"},{"location":"lyceummujoco/environments/#PointMass-1","page":"Environments","title":"PointMass","text":"","category":"section"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"<img src=\"pointmass.png\" width=\"25%\"/>","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"PointMass","category":"page"},{"location":"lyceummujoco/environments/#LyceumMuJoCo.PointMass","page":"Environments","title":"LyceumMuJoCo.PointMass","text":"struct PointMass{S<:MJSim, O} <: LyceumMuJoCo.AbstractMuJoCoEnvironment\n\nPointMass is a simple environment useful for trying out and debugging new algorithms. The task is simply to move a 2D point mass to a target position by applying x and y forces to the mass.\n\nSpaces\n\nState: (13, )\nAction: (2, )\nObservation: (6, )\n\n\n\n\n\n","category":"type"},{"location":"lyceummujoco/environments/#ArmHandPickup-1","page":"Environments","title":"ArmHandPickup","text":"","category":"section"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"<img src=\"armhand.png\" width=\"25%\"/>","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"ArmHandPickup","category":"page"},{"location":"lyceummujoco/environments/#LyceumMuJoCo.ArmHandPickup","page":"Environments","title":"LyceumMuJoCo.ArmHandPickup","text":"struct ArmHandPickup{S<:MJSim, O<:Shapes.MultiShape} <: LyceumMuJoCo.AbstractMuJoCoEnvironment\n\nPickup a block using a robot arm modeled after the Modular Prosthetic Limb developed by the Applied Physics Laboratory, The Johns Hopkins University.\n\nSpaces\n\nState: (106, )\nAction: (36, )\nObservation: (19, )\n\n\n\n\n\n","category":"type"},{"location":"lyceummujoco/environments/#Gym-1","page":"Environments","title":"Gym","text":"","category":"section"},{"location":"lyceummujoco/environments/#SwimmerV2-1","page":"Environments","title":"SwimmerV2","text":"","category":"section"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"<img src=\"swimmer.png\" width=\"25%\"/>","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"SwimmerV2","category":"page"},{"location":"lyceummujoco/environments/#LyceumMuJoCo.SwimmerV2","page":"Environments","title":"LyceumMuJoCo.SwimmerV2","text":"mutable struct SwimmerV2{SIM<:MJSim, S<:Shapes.AbstractShape, O<:Shapes.AbstractShape} <: LyceumMuJoCo.AbstractMuJoCoEnvironment\n\nThis task involves a 3-link swimming robot in a viscous fluid, where the goal is to make it swim forward as fast as possible, by actuating the two joints. The origins of task can be traced back to Remi Coulom's thesis: \"Reinforcement Learning Using Neural Networks, with Applications to Motor Control\"\n\nState: (17, )\nAction: (2, )\nObservation: (8, )\n\n\n\n\n\n","category":"type"},{"location":"lyceummujoco/environments/#Walker2DV2-1","page":"Environments","title":"Walker2DV2","text":"","category":"section"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"<img src=\"walker2d.png\" width=\"25%\"/>","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"Walker2DV2","category":"page"},{"location":"lyceummujoco/environments/#LyceumMuJoCo.Walker2DV2","page":"Environments","title":"LyceumMuJoCo.Walker2DV2","text":"mutable struct Walker2DV2{SIM, S, O} <: LyceumMuJoCo.AbstractMuJoCoEnvironment\n\nMake a two-dimensional bipedal robot walk forward as fast as possible. The robot model is based on the work T. Erez, Y. Tassa, and E. Todorov: \"Infinite Horizon Model Predictive Control for Nonlinear Periodic Tasks\", 2011.\n\nState: (29, )\nAction: (6, )\nObservation: (7, )\n\n\n\n\n\n","category":"type"},{"location":"lyceummujoco/environments/#HopperV2-1","page":"Environments","title":"HopperV2","text":"","category":"section"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"<img src=\"hopper.png\" width=\"25%\"/>","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"HopperV2","category":"page"},{"location":"lyceummujoco/environments/#LyceumMuJoCo.HopperV2","page":"Environments","title":"LyceumMuJoCo.HopperV2","text":"mutable struct HopperV2{SIM, S, O} <: LyceumMuJoCo.AbstractMuJoCoEnvironment\n\nMake a two-dimensional, one-legged robot walk forward as fast as possible. The robot model is based on the work T. Erez, Y. Tassa, and E. Todorov: \"Infinite Horizon Model Predictive Control for Nonlinear Periodic Tasks\", 2011.\n\nSpaces\n\nState: (20, )\nAction: (3, )\nObservation: (11, )\n\n\n\n\n\n","category":"type"},{"location":"lyceummujoco/environments/#dm_control-1","page":"Environments","title":"dm_control","text":"","category":"section"},{"location":"lyceummujoco/environments/#CartpoleSwingup-1","page":"Environments","title":"CartpoleSwingup","text":"","category":"section"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"<img src=\"cartpole.png\" width=\"25%\"/>","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"CartpoleSwingup","category":"page"},{"location":"lyceummujoco/environments/#LyceumMuJoCo.CartpoleSwingup","page":"Environments","title":"LyceumMuJoCo.CartpoleSwingup","text":"struct CartpoleSwingup{S<:MJSim, O<:Shapes.MultiShape} <: LyceumMuJoCo.AbstractMuJoCoEnvironment\n\nSwing up and balance an unactuated pole by applying forces to a cart at its base. The physical model conforms to Neuronlike adaptive elements that can solve difficult learning control problems (Barto et al., 1983).\n\nSpaces\n\nState: (7, )\nAction: (1, )\nObservation: (5, )\n\n\n\n\n\n","category":"type"},{"location":"lyceumai/algorithms/naturalpolicygradient/#","page":"Natural Policy Gradient","title":"Natural Policy Gradient","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"lyceumai/algorithms/naturalpolicygradient/#Natural-Policy-Gradient-1","page":"Natural Policy Gradient","title":"Natural Policy Gradient","text":"","category":"section"},{"location":"lyceumai/algorithms/naturalpolicygradient/#","page":"Natural Policy Gradient","title":"Natural Policy Gradient","text":"CurrentModule = LyceumDocs.LyceumAI","category":"page"},{"location":"lyceumai/algorithms/naturalpolicygradient/#","page":"Natural Policy Gradient","title":"Natural Policy Gradient","text":"NaturalPolicyGradient","category":"page"},{"location":"lyceumai/algorithms/naturalpolicygradient/#LyceumAI.NaturalPolicyGradient","page":"Natural Policy Gradient","title":"LyceumAI.NaturalPolicyGradient","text":"NaturalPolicyGradient{DT<:AbstractFloat}(args...; kwargs...) -> NaturalPolicyGradient\nNaturalPolicyGradient(args...; kwargs...) -> NaturalPolicyGradient\n\nConstruct an instance of NaturalPolicyGradient with args and kwargs, where DT <: AbstractFloat is the element type used for pre-allocated buffers, which defaults to Float32.\n\nIn the following explanation of the NaturalPolicyGradient constructor, we use the following notation/definitions:\n\ndim_o = length(obsspace(env))\ndim_a = length(actionspace(env))\n\"terminal\" (e.g. terminal observation) refers to timestep T + 1 for a length T trajectory.\n\nArguments\n\nenv_tconstructor: a function with signature env_tconstructor(n) that returns n   instances of T, where T <: AbstractEnvironment.\npolicy: a function mapping observations to actions, with the following signatures:\npolicy(obs::AbstractVector) –> action::AbstractVector,   where size(obs) == (dim_o, ) and size(action) == (dim_a, ).\npolicy(obs::AbstractMatrix) –> action::AbstractMatrix,   where size(obs) == (dim_o, N) and size(action) == (dim_a, N).\nvalue: a function mapping observations to scalar rewards, with the following signatures:\nvalue(obs::AbstractVector) –> reward::Real, where size(obs) == (dim_o, )\nvalue(obs::AbstractMatrix) –> reward::AbstractVector,   where size(obs) == (dim_o, N) and size(reward) == (N, ).\nvaluefit!: a function with signature valuefit!(value, obs::AbstractMatrix, returns::AbstractVector),   where size(obs) == (dim_o, N) and size(returns) == (N, ), that fits value to   obs and returns.\n\nKeywords\n\nHmax::Integer: Maximum trajectory length for environments rollouts.\nN::Integer: Total number of data samples used for each policy gradient step.\nNmean::Integer: Total number of data samples for the mean policy (without stochastic   noise). Mean rollouts are used for evaluating policy and not used to improve policy   in any form.\nnorm_step_size::Real: Scaling for the applied gradient update after gradient normalization   has occured. This process makes training much more stable to step sizes;   see equation 5 in this paper for more details.\ngamma::Real: Reward discount, applied as gamma^(t - 1) * reward[t].\ngaelambda::Real: Generalized Advantage Estimate parameter, balances bias and variance when   computing advantages. See this paper for details.\nmax_cg_iter::Integer: Maximum number of Conjugate Gradient iterations when estimating   natural_gradient = alpha * inv(FIM) * gradient, where FIM is the Fisher Information Matrix.\ncg_tol::Real: Numerical tolerance for Conjugate Gradient convergence.\nwhiten_advantages::Bool: if true, apply statistical whitening to calculated advantages   (resulting in mean(returns) ≈ 0 && std(returns) ≈ 1).\nbootstrapped_nstep_returns::Bool: if true, bootstrap the returns calculation starting   value(terminal_observation) instead of 0. See \"Reinforcement Learning\" by   Sutton & Barto for further information.\nvalue_feature_op: a function with the below signatures that transforms environment   observations to a set of \"features\" to be consumed by value and valuefit!:\nvalue_feature_op(observations::AbstractVector{<:AbstractMatrix}) --> AbstractMatrix\nvalue_feature_op(terminal_observations::AbstractMatrix, trajlengths::Vector{<:Integer}) --> AbstractMatrix,   where observations is a vector of observations from each trajectory,   terminal_observations has size (dim_o, number_of_trajectories), and trajlengths   contains the lengths of each trajectory   (such that trajlengths[i] == size(observations[i], 2)).\n\nFor some continuous control tasks, one may consider the following notes when applying NaturalPolicyGradient to new tasks and environments:\n\nFor two policies that both learn to complete a task satisfactorially, the larger one  may not perform significantly better. A minimum amount of representational power is  necessary, but larger networks may not offer quantitative benefits. The same goes for  the value function approximator.\nHmax needs to be sufficiently long for the correct behavior to emerge; N needs to be  sufficiently large that the agent samples useful data. They may also be surprisingly  small for simple tasks. These parameters are the main tunables when applying NaturalPolicyGradient.\nOne might consider the norm_step_size and max_cg_iter parameters as the next most  important when initially testing NaturalPolicyGradient on new tasks, assuming Hmax and N are  appropriately chosen for the task. gamma has interaction with Hmax,  while the default value for gaelambda has been empirically found to be stable for a  wide range of tasks.\n\nFor more details, see Algorithm 1 in Towards Generalization and Simplicity in Continuous Control.\n\n\n\n\n\n","category":"type"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"EditURL = \"https://github.com/Lyceum/LyceumDocs.jl/blob/master/docs/src/examples/humanoid.jl\"","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"note: Running examples locally\nThis example and more are also available as Julia scripts and Jupyter notebooks.See the how-to page for more information.","category":"page"},{"location":"examples/humanoid/#Creating-a-MuJoCo-Environment-1","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"","category":"section"},{"location":"examples/humanoid/#Overview-1","page":"Creating a MuJoCo Environment","title":"Overview","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"Using LyceumMuJoCo, we will create the environment for a Humanoid \"get-up\" task that mostly relies on the defaults of LyceumBase and LyceumMuJoCo to propagate state, action, and observation data. We will have to implement reward and evaluation functions, of course, along with a few other parts of the AbstractEnvironment interface.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"We then solve the \"get-up\" task using a Model-Predictive Control method called \"Model Predictive Path Integral Control\" or MPPI, walking through how to log experiment data and plot the results.","category":"page"},{"location":"examples/humanoid/#The-Code-1","page":"Creating a MuJoCo Environment","title":"The Code","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"First we grab our dependencies of the Lyceum ecosystem and other helpful packages.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"using LinearAlgebra, Random, Statistics\nusing Plots, UnicodePlots, JLSO\nusing LyceumBase, LyceumBase.Tools, LyceumAI, LyceumMuJoCo, MuJoCo, UniversalLogger, Shapes","category":"page"},{"location":"examples/humanoid/#Humanoid-Type-1","page":"Creating a MuJoCo Environment","title":"Humanoid Type","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"This struct is our primary entry into the environment API. Environments utilizing the MuJoCo simulator through LyceumMuJoCo should subtype AbstractMuJoCoEnvironment <: AbstractEnvironment. As you can see, this simple example only wraps around the underlying simulator (the sim::MJSim field of Humanoid, referred to hereafter as just sim). The functions of the LyceumBase API will then dispatch on this struct through Julia's multiple dispatch mechanism. When an algorithm calls a function such as getobs!(obs, env), Julia will select from all functions with that name depending on typeof(obs) and typeof(env).","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"struct Humanoid{S<:MJSim} <: AbstractMuJoCoEnvironment\n    sim::S\nend\n\nLyceumMuJoCo.getsim(env::Humanoid) = env.sim #src (needs to be here for below example to work)","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"Humanoid (and all subtypes of AbstractEnvironment) are designed to be used in a single threaded context. To use Humanoid in a multi-threaded context, one could simply create Threads.nthreads() instances of Humanoid:","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"modelpath = joinpath(@__DIR__, \"humanoid.xml\")\nenvs = [Humanoid(MJSim(modelpath, skip = 2)) for i = 1:Threads.nthreads()]\nThreads.@threads for i = 1:Threads.nthreads()\n    thread_env = envs[Threads.threadid()]\n    step!(thread_env)\nend","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"As Humanoid only ever uses its internal jlModel (found at sim.m) in a read-only fashion, we can make a performance optimization by sharing a single instance of jlModel across each thread, resulting in improved cache efficiency. LyceumMuJoCo.tconstruct, short for \"thread construct\", helps us to do just that by providing a common interface for defining \"thread-aware\" constructors. Below, we make a call to tconstruct(MJSim, n, modelpath, skip = 2) which will construct n instances of MJSim constructed from modelpath and with a skip of 2, all sharing the exact same jlModel instance, and return n instances of Humanoid. All of the environments provided by LyceumMuJoCo feature similar definitions of tconstruct as found below.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"Humanoid() = first(tconstruct(Humanoid, 1))\nfunction LyceumMuJoCo.tconstruct(::Type{Humanoid}, n::Integer)\n    modelpath = joinpath(@__DIR__, \"humanoid.xml\")\n    return Tuple(Humanoid(s) for s in tconstruct(MJSim, n, modelpath, skip = 2))\nend","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"We can then use tconstruct as follows:","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"envs = tconstruct(Humanoid, Threads.nthreads())\nThreads.@threads for i = 1:Threads.nthreads()\n    thread_env = envs[Threads.threadid()]\n    step!(thread_env)\nend","category":"page"},{"location":"examples/humanoid/#Utilities-1","page":"Creating a MuJoCo Environment","title":"Utilities","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"The following are helpers for the \"get-up\" task we'd like to consider. We want the humanoid to stand up, thus we need to grab the model's height, as well as record a laying down position that we can use to set the state to. By exploring the model in the REPL or MJCF/XML file we can see that sim.d.qpos[3] is the index for the z-axis (height) of the root joint. The LAYING_QPOS data was collected externally by posing the model into a supine pose; one can either use LyceumMuJoCoViz or simulate.cpp included with a MuJoCo release to do this.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"_getheight(shapedstate::ShapedView, ::Humanoid) = shapedstate.qpos[3]\nconst LAYING_QPOS = [\n    -0.164158,\n    0.0265899,\n    0.101116,\n    0.684044,\n    -0.160277,\n    -0.70823,\n    -0.0693176,\n    -0.1321,\n    0.0203937,\n    0.298099,\n    0.0873523,\n    0.00634907,\n    0.117343,\n    -0.0320319,\n    -0.619764,\n    0.0204114,\n    -0.157038,\n    0.0512385,\n    0.115817,\n    -0.0320437,\n    -0.617078,\n    -0.00153819,\n    0.13926,\n    -1.01785,\n    -1.57189,\n    -0.0914509,\n    0.708539,\n    -1.57187,\n];\nnothing #hide","category":"page"},{"location":"examples/humanoid/#The-AbstractMuJoCoEnvironment-and-AbstractEnvironment-APIs-1","page":"Creating a MuJoCo Environment","title":"The AbstractMuJoCoEnvironment and AbstractEnvironment APIs","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"LyceumMuJoCo requires access to the underlying MJSim simulator, thus any LyceumMuJoCo environments need to point to the correct field in the environment struct that is the simulator; in our case there's only one field: sim.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"LyceumMuJoCo.getsim(env::Humanoid) = env.sim","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"Normally we could rely on MuJoCo to reset the model to the default configuration when the model XML is loaded; the humanoid.xml model, however, defaults to a vertical pose. To reset the model to our laying down or supine pose, we can copy in the data from LAYING_QPOS above to d.qpos. Calling forward! here is the same as mj_forward(env.sim.m, env.sim.d), for a pure MuJoCo reference.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"function LyceumMuJoCo.reset!(env::Humanoid)\n    reset!(env.sim)\n    env.sim.d.qpos .= LAYING_QPOS\n    forward!(env.sim)\n    return env\nend","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"This reward function uses the _getheight helper function above to get the model's height when the function is called. We also specify a target height of 1.25 and penalize the agent for deviating from the target height. There is also a small penalty for using large control activations; if the coefficient is made larger, the agent may not move at all!","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"function LyceumMuJoCo.getreward(state, action, obs, env::Humanoid)\n    height = _getheight(statespace(env)(state), env)\n    target = 1.25\n    reward = 1.0\n    if height < target\n        reward -= 2.0 * abs(target - height)\n    end\n    reward -= 1e-3 * norm(action)^2\n\n    return reward\nend","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"Finally, we can specify an evaluation function. The difference between geteval and getreward is that getreward is the shaped reward our algorithm is optimizing for, while geteval lets us track a useful value for monitoring performance, such as height. Plotting this eval function will show the agent's height over time and is very useful for reviewing actual desired behavior, regardless of the reward achieved, as it can be used to diagnose reward specification problems.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"function LyceumMuJoCo.geteval(state, action, obs, env::Humanoid)\n    return _getheight(statespace(env)(state), env)\nend","category":"page"},{"location":"examples/humanoid/#Running-Experiments-1","page":"Creating a MuJoCo Environment","title":"Running Experiments","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"As discussed in the Julia performance tips, globals can hinder performance. To avoid this, we construct the MPPI and ControllerIterator instances within a function. This also lets us easily run our experiment with different choices of parameters (e.g. H). Like most algorithms in LyceumAI, MPPI accepts a \"thread-aware\" environment constructor as well as any algorithm parameters. In this case, we just pass a closure around the tconstruct function we defined above. MPPI, being a single-step algorithm, is itself not iterable, so we wrap it in a ControllerIterator which simply calls the passed-in function ctrlfn for T timesteps, while simultaneously plotting and logging the trajectory rollout.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"function humanoid_MPPI(etype = Humanoid; T = 200, H = 64, K = 64)\n    env = etype()\n\n    # The following parameters work well for this get-up task, and may work for\n    # similar tasks, but are not invariant to the model.\n    mppi = MPPI(\n        env_tconstructor = n -> tconstruct(etype, n),\n        covar = Diagonal(0.05^2 * I, size(actionspace(env), 1)),\n        lambda = 0.4,\n        H = H,\n        K = K,\n        gamma = 1.0,\n    )\n\n    ctrlfn = (action, state, obs) -> getaction!(action, state, mppi)\n    iter = ControllerIterator(ctrlfn, env; T = T, plotiter = div(T, 10))\n\n    # We can time the following loop; if it ends up less than the time the\n    # MuJoCo models integrated forward in, then one could conceivably run this\n    # MPPI MPC controller interactively...\n    elapsed = @elapsed for (t, traj) in iter\n        # If desired, one can inspect `traj`, `env`, or `mppi` at each timestep.\n    end\n\n    if elapsed < time(env)\n        @info \"We ran in real time!\"\n    end\n\n    # Save our experiment results to a file for later review.\n    savepath = \"/tmp/opt_humanoid.jlso\"\n    exper = Experiment(savepath, overwrite = true)\n    exper[:etype] = etype\n\n    for (k, v) in pairs(iter.trajectory)\n        exper[k] = v\n    end\n    finish!(exper)\n\n    return mppi, env, iter.trajectory\nend","category":"page"},{"location":"examples/humanoid/#Checking-Results-1","page":"Creating a MuJoCo Environment","title":"Checking Results","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"The MPPI algorithm, and any that you develop, can and should use plotting tools to track progress as they go.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"mppi, env, traj = humanoid_MPPI();\nplot(\n    [traj.rewards traj.evaluations],\n    labels = [\"Reward\" \"Evaluation\"],\n    title = \"Humanoid Standup\",\n    legend = :bottomright,\n)","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"If one wanted to review the results after training, or prepare plots for presentations, one can load the data from disk instead.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"data = JLSO.load(\"/tmp/opt_humanoid.jlso\")\nplot(\n    [data[\"rewards\"] data[\"evaluations\"]],\n    labels = [\"Reward\" \"Evaluation\"],\n    title = \"Humanoid Standup\",\n    legend = :bottomright,\n)","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"#Home-1","page":"Home","title":"Home","text":"","category":"section"},{"location":"#Lyceum-1","page":"Home","title":"Lyceum","text":"","category":"section"},{"location":"#Package-Statuses-1","page":"Home","title":"Package Statuses","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"import LyceumDocs: package_table_markdown_nodocs, LYCEUM_PACKAGE_DEFS\npackage_table_markdown_nodocs(LYCEUM_PACKAGE_DEFS)","category":"page"}]
}
