<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Learning a Control Policy · Lyceum</title><link rel="canonical" href="https://docs.lyceum.ml/dev/examples/NPG/index.html"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Lyceum</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../humanoid/">Creating a MuJoCo Environment</a></li><li class="is-active"><a class="tocitem" href>Learning a Control Policy</a><ul class="internal"><li><a class="tocitem" href="#Overview-1"><span>Overview</span></a></li><li><a class="tocitem" href="#The-Code-1"><span>The Code</span></a></li></ul></li><li><a class="tocitem" href="../visualize/">Using the Visualizer</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Learning a Control Policy</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Learning a Control Policy</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/Lyceum/LyceumDocs.jl/blob/master/docs/src/examples/NPG.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><div class="admonition is-info"><header class="admonition-header">Running examples locally</header><div class="admonition-body"><p>This example and more are also available as Julia scripts and Jupyter notebooks.</p><p>See <a href="https://github.com/Lyceum/LyceumDocs.jl/blob/master/example_howto.md">the how-to page</a> for more information.</p></div></div><h1 id="Learning-a-Control-Policy-1"><a class="docs-heading-anchor" href="#Learning-a-Control-Policy-1">Learning a Control Policy</a><a class="docs-heading-anchor-permalink" href="#Learning-a-Control-Policy-1" title="Permalink"></a></h1><h2 id="Overview-1"><a class="docs-heading-anchor" href="#Overview-1">Overview</a><a class="docs-heading-anchor-permalink" href="#Overview-1" title="Permalink"></a></h2><p>In this example we walk through the process of setting up an experiment that runs <a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf">Natural Policy Gradient</a> (or more recently <a href="https://arxiv.org/pdf/1703.02660.pdf">in this work</a>). This is an on-policy reinforcement learning method that is comparable to TRPO, PPO, and other policy gradient methods. See the documentation for <code>NaturalPolicyGradient</code> for full implementation details.</p><h2 id="The-Code-1"><a class="docs-heading-anchor" href="#The-Code-1">The Code</a><a class="docs-heading-anchor-permalink" href="#The-Code-1" title="Permalink"></a></h2><p>First, let&#39;s go head and grab all the dependencies:</p><pre><code class="language-julia">using LinearAlgebra, Random, Statistics # From Stdlib
using LyceumAI                          # For the NPG controller
using LyceumMuJoCo                      # For the Hopper environment
using LyceumBase.Tools                  # For the ControllerIterator discussed below
using Flux                              # For our neural networks needs
using UniversalLogger                   # For logging experiment data
using Plots                             # For plotting the results</code></pre><p>We first instantiate a <code>HopperV2</code> environment to grab useful environment-specific values, such as the size of the observation and action vectors:</p><pre><code class="language-">env = LyceumMuJoCo.HopperV2();
dobs, dact = length(obsspace(env)), length(actionspace(env));
nothing #hide</code></pre><p>We&#39;ll also seed the per-thread global RNGs:</p><pre><code class="language-julia">seed_threadrngs!(1)</code></pre><pre><code class="language-none">1-element Array{Random.MersenneTwister,1}:
 Random.MersenneTwister(UInt32[0x00000001], Random.DSFMT.DSFMT_state(Int32[1749029653, 1072851681, 1610647787, 1072862326, 1841712345, 1073426746, -198061126, 1073322060, -156153802, 1073567984  …  1977574422, 1073209915, 278919868, 1072835605, 1290372147, 18858467, 1815133874, -1716870370, 382, 0]), [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], UInt128[0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000  …  0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000], 1002, 0)</code></pre><h3 id="Policy-Gradient-Components-1"><a class="docs-heading-anchor" href="#Policy-Gradient-Components-1">Policy Gradient Components</a><a class="docs-heading-anchor-permalink" href="#Policy-Gradient-Components-1" title="Permalink"></a></h3><p>Policy Gradient methods require a policy: a function that takes in the state/observations of the agent, and outputs an action i.e. <code>action = π(obs)</code>. In much of Deep RL, the policy takes the form of a neural network which can be built on top of the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library. We utilize a stochastic policy in this example. Specifically, our policy is represented as a multivariate Gaussian distribution of the form:</p><div>\[\pi(a | s) = \mathcal{N}(\mu_{\theta_1}(\text{obs}), \Sigma_{\theta_2})\]</div><p>where <span>$\mu_{\theta_1}$</span> is a neural network, parameterized by <span>$\theta_1$</span>, that maps an observation <code>obs</code> to a mean action and <span>$\Sigma_{\theta_2}$</span> is a diagonal covariance matrix parameterized by <span>$\theta_2$</span>, the diagonal entries of the matrix. For <span>$\mu_{\theta_1}$</span> we utilize a 2-layer neural network, where each layer has a &quot;width&quot; of 32. We use tanh activations for each hidden layer and initialize the network weights with Glorot Uniform initializations. Rather than tracking <span>$\Sigma_{\theta_2}$</span> directly, we track the log standard deviations, which are easier to learn. We initialize <span>$\log \text{diag}(\Sigma_{\theta_2})$</span> as <code>zeros(dact)</code>, i.e. a <code>Vector</code> of length <code>dact</code>, initialized to 0. Both <span>$\theta_1$</span> and <span>$\theta_2$</span> are learned in this example. Note that <span>$\mu_{\theta_1}$</span> is a <em>state-dependent</em> mean while <span>$\Sigma_{\theta_2}$</span> is a <em>global</em> covariance.</p><pre><code class="language-">const policy = DiagGaussianPolicy(
    multilayer_perceptron(
        dobs,
        32,
        32,
        dact;
        σ = tanh,
        initb = Flux.glorot_uniform,
        initb_final = Flux.glorot_uniform,
        dtype = Float32,
    ),
    zeros(Float32, dact),
);
nothing #hide</code></pre><p>This NPG implementation uses <a href="https://arxiv.org/pdf/1506.02438.pdf">Generalized Advantaged Estimation</a>, which requires an estimate of the value function, <code>value(state)</code>, which we represent using a 2-layer, feedforward neural network where each layer has a width of 128 and uses the ReLU activation function. The model weights are initialized using Glorot Uniform initialization as above.</p><pre><code class="language-">const value = multilayer_perceptron(
    dobs,
    128,
    128,
    1;
    σ = Flux.relu,
    initb = Flux.glorot_uniform,
    initb_final = Flux.glorot_uniform,
    dtype = Float32,
);
nothing #hide</code></pre><p>Next, we set up the optimization pipeline for <code>value</code>. We use a mini-batch size of 64 and the <a href="https://arxiv.org/pdf/1412.6980.pdf">ADAM</a> optimizer. <code>FluxTrainer</code> is an iterator that loops on the model provided, performing a single step of gradient descent at each iteration. The result at each loop is passed to <code>stopcb</code> below, so you can quit after a number of epochs, convergence, or other criteria; here it&#39;s capped at two epochs. See the documentation for <code>FluxTrainer</code> for more information.</p><pre><code class="language-julia">valueloss(bl, X, Y) = Flux.mse(vec(bl(X)), vec(Y))
stopcb(x) = x.nepochs &gt; 2
const valuetrainer = FluxTrainer(
    optimiser = ADAM(1e-3),
    szbatch = 64,
    lossfn = valueloss,
    stopcb = stopcb
);</code></pre><p>The <code>NaturalPolicyGradient</code> iterator is a type that pre-allocates all necesary data structures and performs one gradient update to <code>policy</code> at each iteration. We first pass in a constructor that given <code>n</code> returns <code>n</code> instances of <code>LyceumMuJoCo.HopperV2</code>, all sharing the same <code>jlModel</code>, to allow <code>NaturalPolicyGradient</code> to allocate per-thread environments and enable performant, parallel sampling from <code>policy</code>. We then pass in the <code>policy</code>, <code>value</code>, and <code>valuetrainer</code> instances constructed above and override a few of the default <code>NaturalPolicyGradient</code> parameters: <code>gamma</code>, <code>gaelambda</code>, and <code>norm_step_size</code>. Finally, we set the max trajectory length <code>Hmax</code> and total number of samples per iteration, <code>N</code>. Under the hood, <code>NaturalPolicyGradient</code> will use approximately <code>div(N, Hmax)</code> threads to perform the sampling.</p><pre><code class="language-">const npg = NaturalPolicyGradient(
    n -&gt; tconstruct(LyceumMuJoCo.HopperV2, n),
    policy,
    value,
    valuetrainer;
    gamma = 0.995,
    gaelambda = 0.97,
    norm_step_size = 0.05,
    Hmax = 1000,
    N = 10240,
);
nothing #hide</code></pre><h3 id="Running-Experiments-1"><a class="docs-heading-anchor" href="#Running-Experiments-1">Running Experiments</a><a class="docs-heading-anchor-permalink" href="#Running-Experiments-1" title="Permalink"></a></h3><p>Finally, let&#39;s spin on our iterator 200 times, plotting every 20 iterations. This lets us break out of the loop if certain conditions are met, or re-start training manually if needed. We of course wish to track results, so we create a <code>ULogger</code> and <code>Experiment</code> to which we can save data. We also have useful timing information displayed every 20 iterations to better understand the performance of our algorithm and identify any potential bottlenecks. Rather than iterating on <code>npg</code> at the global scope, we&#39;ll do it inside of a function to avoid the performance issues associated with global variables as discussed in the <a href="https://docs.julialang.org/en/v1/manual/performance-tips/">Julia performance tips</a>. Note, to keep the Markdown version of this tutorial readable, we skip the plots and performance statistics. To enable them, simply call <code>hopper_NPG(npg, true)</code>.</p><pre><code class="language-">function hopper_NPG(npg::NaturalPolicyGradient, plot::Bool)
    exper = Experiment(&quot;/tmp/hopper_example.jlso&quot;, overwrite = true)
    # Walks, talks, and acts like a Julia logger. See the UniversalLogger.jl docs for more info.
    lg = ULogger()
    for (i, state) in enumerate(npg)
        if i &gt; 200
            # serialize some stuff and quit
            exper[:policy] = npg.policy
            exper[:value] = npg.value
            exper[:etype] = LyceumMuJoCo.HopperV2
            exper[:meanstates] = state.meanbatch
            exper[:stocstates] = state.stocbatch
            break
        end

        # log everything in `state` except meanbatch and stocbatch
        push!(lg, :algstate, filter_nt(state, exclude = (:meanbatch, :stocbatch)))

        if plot &amp;&amp; mod(i, 20) == 0
            x = lg[:algstate]
            # The following are helper functions for plotting to the terminal.
            # The first plot displays the `geteval` function for our stochastic
            # and mean policy rollouts.
            display(expplot(
                Line(x[:stocterminal_eval], &quot;StocLastE&quot;),
                Line(x[:meanterminal_eval], &quot;MeanLastE&quot;),
                title = &quot;Evaluation Score, Iter=$i&quot;,
                width = 60,
                height = 8,
            ))
            # While the second one similarly plots `getreward`.
            display(expplot(
                Line(x[:stoctraj_reward], &quot;StocR&quot;),
                Line(x[:meantraj_reward], &quot;MeanR&quot;),
                title = &quot;Reward, Iter=$i&quot;,
                width = 60,
                height = 8,
            ))

            # The following are timing values for various parts of the Natural Policy Gradient
            # algorithm at the last iteration, useful for finding performance bottlenecks
            # in the algorithm.
            println(&quot;elapsed_sampled  = &quot;, state.elapsed_sampled)
            println(&quot;elapsed_gradll   = &quot;, state.elapsed_gradll)
            println(&quot;elapsed_vpg      = &quot;, state.elapsed_vpg)
            println(&quot;elapsed_cg       = &quot;, state.elapsed_cg)
            println(&quot;elapsed_valuefit = &quot;, state.elapsed_valuefit)
        end
    end
    exper, lg
end
exper, lg = hopper_NPG(npg, false);
nothing #hide</code></pre><p>Let&#39;s go ahead and plot the final reward trajectory for our stochastic and mean policies to see how we did:</p><pre><code class="language-">plot(
    [lg[:algstate][:meantraj_reward] lg[:algstate][:stoctraj_reward]],
    labels = [&quot;Mean Policy&quot; &quot;Stochastic Policy&quot;],
    title = &quot;HopperV2 Reward&quot;,
    legend = :bottomright,
)</code></pre><p>We&#39;ll also plot the evaluations:</p><pre><code class="language-">plot(
    [lg[:algstate][:meantraj_eval] lg[:algstate][:stoctraj_eval]],
    labels = [&quot;Mean Policy&quot; &quot;Stochastic Policy&quot;],
    title = &quot;HopperV2 Eval&quot;,
    legend = :bottomright,
)</code></pre><p>Finally, we save the logged results to <code>exper</code> for later review:</p><pre><code class="language-">exper[:logs] = get(lg)
finish!(exper); # flushes everything to disk
nothing #hide</code></pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../humanoid/">« Creating a MuJoCo Environment</a><a class="docs-footer-nextpage" href="../visualize/">Using the Visualizer »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 16 January 2020 20:22">Thursday 16 January 2020</span>. Using Julia version 1.3.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
