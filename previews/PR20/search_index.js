var documenterSearchIndex = {"docs":
[{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"#md # !!! note \"Running examples locally\" #md #     This example and more are also available as Julia scripts and Jupyter notebooks. #md # #md #     See the how-to page for more information. #md #","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"lyceummujoco/environments/#Environments-1","page":"Environments","title":"Environments","text":"","category":"section"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"CurrentModule = LyceumDocs.LyceumMuJoCo","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"LyceumMuJoCo comes with a variety of environments:","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"Lyceum Suite: A collection of Lyceum-custom environments.\nGym: Ports of environments from OpenAI's gym.\ndm_control: Ports of environments from DeepMind's dm_control","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"We highly encourage that users get familiar with the source codes of these environments and modify them as you see fit. We also hope that they serve as inspiration for creating new, interesting environments. As always, if you make something cool we'd gladly welcome a pull request to incorporate it into Lyceum!","category":"page"},{"location":"lyceummujoco/environments/#Lyceum-Suite-1","page":"Environments","title":"Lyceum Suite","text":"","category":"section"},{"location":"lyceummujoco/environments/#PointMass-1","page":"Environments","title":"PointMass","text":"","category":"section"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"<img src=\"pointmass.png\" width=\"25%\"/>","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"PointMass","category":"page"},{"location":"lyceummujoco/environments/#LyceumMuJoCo.PointMass","page":"Environments","title":"LyceumMuJoCo.PointMass","text":"struct PointMass{S<:MJSim, O} <: LyceumMuJoCo.AbstractMuJoCoEnvironment\n\nPointMass is a simple environment useful for trying out and debugging new algorithms. The task is simply to move a 2D point mass to a target position by applying x and y forces to the mass.\n\nSpaces\n\nState: (13, )\nAction: (2, )\nObservation: (6, )\n\n\n\n\n\n","category":"type"},{"location":"lyceummujoco/environments/#ArmHandPickup-1","page":"Environments","title":"ArmHandPickup","text":"","category":"section"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"<img src=\"armhand.png\" width=\"25%\"/>","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"ArmHandPickup","category":"page"},{"location":"lyceummujoco/environments/#LyceumMuJoCo.ArmHandPickup","page":"Environments","title":"LyceumMuJoCo.ArmHandPickup","text":"struct ArmHandPickup{S<:MJSim, O<:Shapes.MultiShape} <: LyceumMuJoCo.AbstractMuJoCoEnvironment\n\nPickup a block using a robot arm modeled after the Modular Prosthetic Limb developed by the Applied Physics Laboratory, The Johns Hopkins University.\n\nSpaces\n\nState: (106, )\nAction: (36, )\nObservation: (19, )\n\n\n\n\n\n","category":"type"},{"location":"lyceummujoco/environments/#Gym-1","page":"Environments","title":"Gym","text":"","category":"section"},{"location":"lyceummujoco/environments/#SwimmerV2-1","page":"Environments","title":"SwimmerV2","text":"","category":"section"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"<img src=\"swimmer.png\" width=\"25%\"/>","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"SwimmerV2","category":"page"},{"location":"lyceummujoco/environments/#LyceumMuJoCo.SwimmerV2","page":"Environments","title":"LyceumMuJoCo.SwimmerV2","text":"mutable struct SwimmerV2{SIM<:MJSim, S<:Shapes.AbstractShape, O<:Shapes.AbstractShape} <: LyceumMuJoCo.AbstractMuJoCoEnvironment\n\nThis task involves a 3-link swimming robot in a viscous fluid, where the goal is to make it swim forward as fast as possible, by actuating the two joints. The origins of task can be traced back to Remi Coulom's thesis: \"Reinforcement Learning Using Neural Networks, with Applications to Motor Control\"\n\nState: (17, )\nAction: (2, )\nObservation: (8, )\n\n\n\n\n\n","category":"type"},{"location":"lyceummujoco/environments/#Walker2DV2-1","page":"Environments","title":"Walker2DV2","text":"","category":"section"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"<img src=\"walker2d.png\" width=\"25%\"/>","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"Walker2DV2","category":"page"},{"location":"lyceummujoco/environments/#LyceumMuJoCo.Walker2DV2","page":"Environments","title":"LyceumMuJoCo.Walker2DV2","text":"mutable struct Walker2DV2{SIM, S, O} <: LyceumMuJoCo.AbstractMuJoCoEnvironment\n\nMake a two-dimensional bipedal robot walk forward as fast as possible. The robot model is based on the work T. Erez, Y. Tassa, and E. Todorov: \"Infinite Horizon Model Predictive Control for Nonlinear Periodic Tasks\", 2011.\n\nState: (29, )\nAction: (6, )\nObservation: (7, )\n\n\n\n\n\n","category":"type"},{"location":"lyceummujoco/environments/#HopperV2-1","page":"Environments","title":"HopperV2","text":"","category":"section"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"<img src=\"hopper.png\" width=\"25%\"/>","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"HopperV2","category":"page"},{"location":"lyceummujoco/environments/#LyceumMuJoCo.HopperV2","page":"Environments","title":"LyceumMuJoCo.HopperV2","text":"mutable struct HopperV2{SIM, S, O} <: LyceumMuJoCo.AbstractMuJoCoEnvironment\n\nMake a two-dimensional, one-legged robot walk forward as fast as possible. The robot model is based on the work T. Erez, Y. Tassa, and E. Todorov: \"Infinite Horizon Model Predictive Control for Nonlinear Periodic Tasks\", 2011.\n\nSpaces\n\nState: (20, )\nAction: (3, )\nObservation: (11, )\n\n\n\n\n\n","category":"type"},{"location":"lyceummujoco/environments/#dm_control-1","page":"Environments","title":"dm_control","text":"","category":"section"},{"location":"lyceummujoco/environments/#CartpoleSwingup-1","page":"Environments","title":"CartpoleSwingup","text":"","category":"section"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"<img src=\"cartpole.png\" width=\"25%\"/>","category":"page"},{"location":"lyceummujoco/environments/#","page":"Environments","title":"Environments","text":"CartpoleSwingup","category":"page"},{"location":"lyceummujoco/environments/#LyceumMuJoCo.CartpoleSwingup","page":"Environments","title":"LyceumMuJoCo.CartpoleSwingup","text":"struct CartpoleSwingup{S<:MJSim, O<:Shapes.MultiShape} <: LyceumMuJoCo.AbstractMuJoCoEnvironment\n\nSwing up and balance an unactuated pole by applying forces to a cart at its base. The physical model conforms to Neuronlike adaptive elements that can solve difficult learning control problems (Barto et al., 1983).\n\nSpaces\n\nState: (7, )\nAction: (1, )\nObservation: (5, )\n\n\n\n\n\n","category":"type"},{"location":"lyceumai/algorithms/naturalpolicygradient/#","page":"Natural Policy Gradient","title":"Natural Policy Gradient","text":"#md # !!! note \"Running examples locally\" #md #     This example and more are also available as Julia scripts and Jupyter notebooks. #md # #md #     See the how-to page for more information. #md #","category":"page"},{"location":"lyceumai/algorithms/naturalpolicygradient/#","page":"Natural Policy Gradient","title":"Natural Policy Gradient","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"lyceumai/algorithms/naturalpolicygradient/#Natural-Policy-Gradient-1","page":"Natural Policy Gradient","title":"Natural Policy Gradient","text":"","category":"section"},{"location":"lyceumai/algorithms/naturalpolicygradient/#","page":"Natural Policy Gradient","title":"Natural Policy Gradient","text":"CurrentModule = LyceumDocs.LyceumAI","category":"page"},{"location":"lyceumai/algorithms/naturalpolicygradient/#","page":"Natural Policy Gradient","title":"Natural Policy Gradient","text":"NaturalPolicyGradient","category":"page"},{"location":"lyceumai/algorithms/naturalpolicygradient/#LyceumAI.NaturalPolicyGradient","page":"Natural Policy Gradient","title":"LyceumAI.NaturalPolicyGradient","text":"NaturalPolicyGradient{DT<:AbstractFloat}(args...; kwargs...) -> NaturalPolicyGradient\nNaturalPolicyGradient(args...; kwargs...) -> NaturalPolicyGradient\n\nConstruct an instance of NaturalPolicyGradient with args and kwargs, where DT <: AbstractFloat is the element type used for pre-allocated buffers, which defaults to Float32.\n\nIn the following explanation of the NaturalPolicyGradient constructor, we use the following notation/definitions:\n\ndim_o = length(obsspace(env))\ndim_a = length(actionspace(env))\n\"terminal\" (e.g. terminal observation) refers to timestep T + 1 for a length T trajectory.\n\nArguments\n\nenv_tconstructor: a function with signature env_tconstructor(n) that returns n   instances of T, where T <: AbstractEnvironment.\npolicy: a function mapping observations to actions, with the following signatures:\npolicy(obs::AbstractVector) –> action::AbstractVector,   where size(obs) == (dim_o, ) and size(action) == (dim_a, ).\npolicy(obs::AbstractMatrix) –> action::AbstractMatrix,   where size(obs) == (dim_o, N) and size(action) == (dim_a, N).\nvalue: a function mapping observations to scalar rewards, with the following signatures:\nvalue(obs::AbstractVector) –> reward::Real, where size(obs) == (dim_o, )\nvalue(obs::AbstractMatrix) –> reward::AbstractVector,   where size(obs) == (dim_o, N) and size(reward) == (N, ).\nvaluefit!: a function with signature valuefit!(value, obs::AbstractMatrix, returns::AbstractVector),   where size(obs) == (dim_o, N) and size(returns) == (N, ), that fits value to   obs and returns.\n\nKeywords\n\nHmax::Integer: Maximum trajectory length for environments rollouts.\nN::Integer: Total number of data samples used for each policy gradient step.\nNmean::Integer: Total number of data samples for the mean policy (without stochastic   noise). Mean rollouts are used for evaluating policy and not used to improve policy   in any form.\nnorm_step_size::Real: Scaling for the applied gradient update after gradient normalization   has occured. This process makes training much more stable to step sizes;   see equation 5 in this paper for more details.\ngamma::Real: Reward discount, applied as gamma^(t - 1) * reward[t].\ngaelambda::Real: Generalized Advantage Estimate parameter, balances bias and variance when   computing advantages. See this paper for details.\nmax_cg_iter::Integer: Maximum number of Conjugate Gradient iterations when estimating   natural_gradient = alpha * inv(FIM) * gradient, where FIM is the Fisher Information Matrix.\ncg_tol::Real: Numerical tolerance for Conjugate Gradient convergence.\nwhiten_advantages::Bool: if true, apply statistical whitening to calculated advantages   (resulting in mean(returns) ≈ 0 && std(returns) ≈ 1).\nbootstrapped_nstep_returns::Bool: if true, bootstrap the returns calculation starting   value(terminal_observation) instead of 0. See \"Reinforcement Learning\" by   Sutton & Barto for further information.\nvalue_feature_op: a function with the below signatures that transforms environment   observations to a set of \"features\" to be consumed by value and valuefit!:\nvalue_feature_op(observations::AbstractVector{<:AbstractMatrix}) --> AbstractMatrix\nvalue_feature_op(terminal_observations::AbstractMatrix, trajlengths::Vector{<:Integer}) --> AbstractMatrix,   where observations is a vector of observations from each trajectory,   terminal_observations has size (dim_o, number_of_trajectories), and trajlengths   contains the lengths of each trajectory   (such that trajlengths[i] == size(observations[i], 2)).\n\nFor some continuous control tasks, one may consider the following notes when applying NaturalPolicyGradient to new tasks and environments:\n\nFor two policies that both learn to complete a task satisfactorially, the larger one  may not perform significantly better. A minimum amount of representational power is  necessary, but larger networks may not offer quantitative benefits. The same goes for  the value function approximator.\nHmax needs to be sufficiently long for the correct behavior to emerge; N needs to be  sufficiently large that the agent samples useful data. They may also be surprisingly  small for simple tasks. These parameters are the main tunables when applying NaturalPolicyGradient.\nOne might consider the norm_step_size and max_cg_iter parameters as the next most  important when initially testing NaturalPolicyGradient on new tasks, assuming Hmax and N are  appropriately chosen for the task. gamma has interaction with Hmax,  while the default value for gaelambda has been empirically found to be stable for a  wide range of tasks.\n\nFor more details, see Algorithm 1 in Towards Generalization and Simplicity in Continuous Control.\n\n\n\n\n\n","category":"type"},{"location":"assets/LyceumExamples/README/#LyceumDocs.jl-Examples-1","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"","category":"section"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"Included in this project are all the examples found at LyceumDocs.jl, each available as either a .jl script or Jupyter notebook. To start, open up a Julia REPL with the project activated by executing the following in the directory containing this README:","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"julia --project=.","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"Now press the ] charcter to enter the Pkg REPL-mode. Your prompt should now look like this:","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"(LyceumExamples) pkg>","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"First, we'll add the LyceumRegistry so the package manager knows where to find the Lyceum packages:","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"(LyceumExamples) pkg> registry add https://github.com/Lyceum/LyceumRegistry.git\n   Cloning registry from \"https://github.com/Lyceum/LyceumRegistry.git\"\n     Added registry `LyceumRegistry` to `~/.julia/registries/LyceumRegistry`\n\n(LyceumExamples) pkg>","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"Next, call instantiate to download the required packages:","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"(LyceumExamples) pkg> instantiate\n  Updating registry at `~/.julia/registries/General`\n  Updating git-repo `https://github.com/JuliaRegistries/General.git`\n  Updating registry at `~/.julia/registries/LyceumRegistry`\n  Updating git-repo `https://github.com/Lyceum/LyceumRegistry.git`\n   Cloning git-repo `https://github.com/Lyceum/Lyceum.jl.git`\n\n   ...","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"You can now press Backspace to exit Pkg REPL-mode, returning you to the regular REPL:","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"julia>","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"To run the Julia scripts, simply include them into your current session:","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"julia> include(\"scripts/path/to/example.jl\")","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"Alternative, you can run the notebooks using IJulia:","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"julia> using IJulia\njulia> notebook(dir=\"notebooks/\"; detached=true)","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"The Jupyter notebook should open in your browser automatically. If not, go to http://localhost:8888/ in your browser of choice. From there you can browse and execute the various notebooks.","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"If you run into any trouble, don't hesitate to open an issue on the LyceumDocs.jl repo.","category":"page"},{"location":"assets/LyceumExamples/README/#","page":"LyceumDocs.jl Examples","title":"LyceumDocs.jl Examples","text":"Enjoy!","category":"page"},{"location":"lyceumai/algorithms/mppi/#","page":"MPPI","title":"MPPI","text":"#md # !!! note \"Running examples locally\" #md #     This example and more are also available as Julia scripts and Jupyter notebooks. #md # #md #     See the how-to page for more information. #md #","category":"page"},{"location":"lyceumai/algorithms/mppi/#","page":"MPPI","title":"MPPI","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"lyceumai/algorithms/mppi/#Model-Predictive-Path-Integral-Control-1","page":"MPPI","title":"Model-Predictive Path Integral Control","text":"","category":"section"},{"location":"lyceumai/algorithms/mppi/#","page":"MPPI","title":"MPPI","text":"Implements Model-Predictive Path Integral Control, a stochastic sampling based model predictive control method. For further information, see the following papers:","category":"page"},{"location":"lyceumai/algorithms/mppi/#","page":"MPPI","title":"MPPI","text":"Information Theoretic MPC for Model-Based Reinforcement Learning\nAggressive Driving with Model Predictive Path Integral Control","category":"page"},{"location":"lyceumai/algorithms/mppi/#","page":"MPPI","title":"MPPI","text":"CurrentModule = LyceumDocs.LyceumAI","category":"page"},{"location":"lyceumai/algorithms/mppi/#","page":"MPPI","title":"MPPI","text":"MPPI\ngetaction!(::AbstractVector, ::Any, ::MPPI)\nreset!(::MPPI)","category":"page"},{"location":"lyceumai/algorithms/mppi/#LyceumAI.MPPI","page":"MPPI","title":"LyceumAI.MPPI","text":"struct MPPI{DT<:AbstractFloat, nu, Covar<:AbstractArray{DT<:AbstractFloat,2}, Value, Env, Init, Obs, State}\n\nMPPI{DT<:AbstractFloat}(args...; kwargs...) -> MPPI\nMPPI(args...; kwargs...) -> MPPI\n\nConstruct an instance of  MPPI with args and kwargs, where DT <: AbstractFloat is the element type used for pre-allocated buffers, which defaults to Float32.\n\nIn the following explanation of the MPPI constructor, we use the following notation:\n\nU::Matrix: the canonical control vector (u_1 u_2 dots u_H), where   size(U) == (length(actionspace(env)), H).\n\nKeywords\n\nenv_tconstructor: a function with signature env_tconstructor(n) that returns n   instances of T, where T <: AbstractEnvironment.\nH::Integer: Length of sampled trajectories.\nK::Integer: Number of trajectories to sample.\ncovar::AbstractMatrix: The covariance matrix for the Normal distribution from which   control pertubations are sampled from.\ngamma::Real: Reward discount, applied as gamma^(t - 1) * reward[t].\nlambda::Real: Temperature parameter for the exponential reweighting of sampled   trajectories. In the limit that lambda approaches 0, U is set to the highest reward   trajectory. Conversely, as lambda approaches infinity, U is computed as the   unweighted-average of the samples trajectories.\nvalue: a function mapping observations to scalar rewards, with the signature   value(obs::AbstractVector) --> reward::Real\ninitfn!: A function with the signature initfn!(U::Matrix) used for   re-initializing U after shifting it. Defaults to setting the last   element of U to 0.\n\n\n\n\n\n","category":"type"},{"location":"lyceumai/algorithms/mppi/#LyceumBase.getaction!-Tuple{AbstractArray{T,1} where T,Any,LyceumAI.MPPI}","page":"MPPI","title":"LyceumBase.getaction!","text":"getaction!(action, state, m; nthreads)\n\n\nStarting from the environment's state, perform one step of the MPPI algorithm and store the resulting action in action. The trajectory sampling portion of MPPI is done in parallel using nthreads threads.\n\n\n\n\n\n","category":"method"},{"location":"lyceumai/algorithms/mppi/#LyceumBase.reset!-Tuple{LyceumAI.MPPI}","page":"MPPI","title":"LyceumBase.reset!","text":"reset!(m::LyceumAI.MPPI) -> LyceumAI.MPPI\n\n\nResets the canonical control vector to zeros.\n\n\n\n\n\n","category":"method"},{"location":"lyceumai/models/policies/#","page":"Policies","title":"Policies","text":"#md # !!! note \"Running examples locally\" #md #     This example and more are also available as Julia scripts and Jupyter notebooks. #md # #md #     See the how-to page for more information. #md #","category":"page"},{"location":"lyceumai/models/policies/#","page":"Policies","title":"Policies","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"lyceumai/models/policies/#Policies-1","page":"Policies","title":"Policies","text":"","category":"section"},{"location":"lyceumai/models/policies/#","page":"Policies","title":"Policies","text":"CurrentModule = LyceumDocs.LyceumAI","category":"page"},{"location":"lyceumai/models/policies/#DiagGaussianPolicy-1","page":"Policies","title":"DiagGaussianPolicy","text":"","category":"section"},{"location":"lyceumai/models/policies/#","page":"Policies","title":"Policies","text":"DiagGaussianPolicy\nDiagGaussianPolicy(::Any, ::AbstractVector)\nsample!(::AbstractRNG, ::AbsVec, ::DiagGaussianPolicy, ::AbsVec)\ngetaction!(::AbsVec, ::DiagGaussianPolicy, ::AbsVec)\nloglikelihood","category":"page"},{"location":"lyceumai/models/policies/#LyceumAI.DiagGaussianPolicy","page":"Policies","title":"LyceumAI.DiagGaussianPolicy","text":"struct DiagGaussianPolicy{Mean, Logstd<:(AbstractArray{T,1} where T)}\n\nDiagGaussianPolicy policy represents a stochastic control policy, represented as a multivariate Gaussian distribution of the form:\n\npi_theta(a  o) = mathcalN(mu_theta_1(o) Sigma_theta_2)\n\nwhere mu_theta_1 is a neural network, parameterized by theta_1, that maps an observation to a mean action and Sigma_theta_2 is a diagonal covariance matrix parameterized by theta_2, the diagonal entries of the matrix. Rather than tracking Sigma_theta_2 directly, we track the log standard deviations, which are easier to learn. Note that mu_theta_1 is a state-dependent mean while Sigma_theta_2 is a global covariance.\n\n\n\n\n\n","category":"type"},{"location":"lyceumai/models/policies/#LyceumAI.DiagGaussianPolicy-Tuple{Any,AbstractArray{T,1} where T}","page":"Policies","title":"LyceumAI.DiagGaussianPolicy","text":"DiagGaussianPolicy(meanNN, logstd; fixedlogstd)\n\n\nConstruct a DiagGaussianPolicy with a state-dependent mean meanNN and initial log-standard deviation logstd. If fixedlogstd is true, logstd will be treated as a constant. meanNN should be object that is compatible with Flux.jl and have the following signatures:\n\nmeanNN(obs::AbstractVector) –> action::AbstractVector\nmeanNN(obs::AbstractMatrix) –> action::AbstractMatrix\n\n\n\n\n\n","category":"method"},{"location":"lyceumai/models/policies/#LyceumBase.Tools.sample!-Tuple{Random.AbstractRNG,AbstractArray{T,1} where T,LyceumAI.DiagGaussianPolicy,AbstractArray{T,1} where T}","page":"Policies","title":"LyceumBase.Tools.sample!","text":"sample!([rng = GLOBAL_RNG, ]action, policy, feature)\n\nTreating policy as a stochastic policy, sample an action from policy, conditioned on feature, and store it in action.\n\n\n\n\n\n","category":"method"},{"location":"lyceumai/models/policies/#LyceumBase.getaction!-Tuple{AbstractArray{T,1} where T,LyceumAI.DiagGaussianPolicy,AbstractArray{T,1} where T}","page":"Policies","title":"LyceumBase.getaction!","text":"getaction!(action, policy, feature)\n\n\nTreating policy as a deterministic policy, compute the mean action of policy, conditioned on feature, and store it in action.\n\n\n\n\n\n","category":"method"},{"location":"lyceumai/models/policies/#LyceumAI.loglikelihood","page":"Policies","title":"LyceumAI.loglikelihood","text":"loglikelihood(policy, action, feature)\n\n\nReturn loglikelihood of action conditioned on feature for policy.\n\n\n\n\n\nloglikelihood(policy, actions, features)\n\n\nTreating each column of actions and features as a single action/feature, return a vector of the loglikelihoods of actions conditioned on features for policy.\n\n\n\n\n\n","category":"function"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"#md # !!! note \"Running examples locally\" #md #     This example and more are also available as Julia scripts and Jupyter notebooks. #md # #md #     See the how-to page for more information. #md #","category":"page"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"lyceummujoco/lyceummujoco/#LyceumMuJoCo-1","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"","category":"section"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"CurrentModule = LyceumDocs.LyceumMuJoCo","category":"page"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"LyceumMuJoCo uses MuJoCo.jl to provide the following:","category":"page"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"MuJoCo-based environments that implement the AbstractEnvironment interface.\nThe MJSim type and related utilities for combining a jlModel and jlData from   MuJoCo.jl to provide a full simulation.","category":"page"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"Note that to use MuJoCo, you'll need a valid license which you can obtain from here. Up to three thirty-day trials can be obtained for free from MuJoCo's webiste, while students are eligible for a free personal license. Once you have obtained the license file, set the environment variable MUJOCO_KEY_PATH to point to its location. On Linux machines this would look like:","category":"page"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"$ export MUJOCO_KEY_PATH=/path/to/mjkey.txt","category":"page"},{"location":"lyceummujoco/lyceummujoco/#AbstractMuJoCoEnvironment-1","page":"LyceumMuJoCo","title":"AbstractMuJoCoEnvironment","text":"","category":"section"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"To create a new MuJoCo-based environment, you will need to:","category":"page"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"Define a type Env that subtypes AbstractMuJoCoEnvironment <: LyceumBase.AbstractEnvironment.\nImplement the AbstractEnvironment interface.\nAdditionally, implement the method getsim(env::Env) --> MJSim that  returns the underlying MJSim which is used to provide defaults and other features.","category":"page"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"AbstractMuJoCoEnvironment\ngetsim","category":"page"},{"location":"lyceummujoco/lyceummujoco/#LyceumMuJoCo.AbstractMuJoCoEnvironment","page":"LyceumMuJoCo","title":"LyceumMuJoCo.AbstractMuJoCoEnvironment","text":"abstract type AbstractMuJoCoEnvironment <: LyceumBase.AbstractEnvironment\n\nThe supertype for all MuJoCo-based environments. Subtypes of AbstractMuJoCoEnvironment provide default functions for state, action, and observation (e.g. setstate!). For more information, see the documentation for MJSim.\n\n\n\n\n\n","category":"type"},{"location":"lyceummujoco/lyceummujoco/#LyceumMuJoCo.getsim","page":"LyceumMuJoCo","title":"LyceumMuJoCo.getsim","text":"getsim(env::LyceumMuJoCo.AbstractMuJoCoEnvironment) -> MJSim\n\n\nReturn env's underlying MJSim that defines the MuJoCo physics simulation. This is used for providing default state, action, and observation functions as well as the visualizer provided by LyceumMuJoCoViz.\n\n\n\n\n\n","category":"function"},{"location":"lyceummujoco/lyceummujoco/#MJSim-1","page":"LyceumMuJoCo","title":"MJSim","text":"","category":"section"},{"location":"lyceummujoco/lyceummujoco/#","page":"LyceumMuJoCo","title":"LyceumMuJoCo","text":"MJSim\nsetstate!(::MJSim, ::RealVec)\ngetstate!(::RealVec, ::MJSim)\nobsspace(::MJSim)\ngetobs!(::RealVec, ::MJSim)\ngetobs(::MJSim)\nzeroctrl!(::MJSim)\nzerofullctrl!(::MJSim)\nforward!(::MJSim)\ntimestep(::MJSim)\ntime(::MJSim)","category":"page"},{"location":"lyceummujoco/lyceummujoco/#LyceumMuJoCo.MJSim","page":"LyceumMuJoCo","title":"LyceumMuJoCo.MJSim","text":"MJSim\n\nThe MJSim type couples a jlModel and jlData from MuJoCo.jl to provide a full simulation.\n\nThe following are the official/internal/minimum set of fields from jlData for state, observation, and action in MuJoCo:\n\nState: (time, qpos, qvel, act, mocap_pos, mocap_quat, userdata, qacc_warmstart)\nObservation: sensordata\nAction: (ctrl, qfrc_applied, xfrc_applied)\n\nMJSim follows this definition except for actions (e.g. setaction!), which is composed of justctrl` by default.\n\nFor more information, see the \"State and control\" section of the MuJoCo Documentation\n\nFields\n\nm::jlModel: contains the model description and is expected to remain constant.\nd::jlData: contains all dynamic variables and intermediate results.\nmn::Tuple: named-access version of MJSim.m provided by AxisArrays.jl.\ndn::Tuple: named-access version of MJSim.d provided by AxisArrays.jl.\ninitstate::Vector{mjtNum}: The initial state vector at the time when this MJSim was constructed.\nskip::Int: the number of times the simulation is integrated, yielding an effective simulation timestep of skip * m.opt.timestep.\n\n\n\n\n\n","category":"type"},{"location":"lyceummujoco/lyceummujoco/#LyceumBase.setstate!-Tuple{MJSim,AbstractArray{#s12,1} where #s12<:Real}","page":"LyceumMuJoCo","title":"LyceumBase.setstate!","text":"setstate!(sim, state)\n\n\nCopy the components of state to their respective fields in sim.d, namely:\n\n(time, qpos, qvel, act, mocap_pos, mocap_quat, userdata, qacc_warmstart)\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#LyceumBase.getstate!-Tuple{AbstractArray{#s12,1} where #s12<:Real,MJSim}","page":"LyceumMuJoCo","title":"LyceumBase.getstate!","text":"getstate!(state, sim)\n\n\nCopy the following state fields from sim.d into state:\n\n(time, qpos, qvel, act, mocap_pos, mocap_quat, userdata, qacc_warmstart)\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#LyceumBase.obsspace-Tuple{MJSim}","page":"LyceumMuJoCo","title":"LyceumBase.obsspace","text":"obsspace(sim::MJSim) -> Any\n\n\nReturn a description of sim's observation space.\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#LyceumBase.getobs!-Tuple{AbstractArray{#s12,1} where #s12<:Real,MJSim}","page":"LyceumMuJoCo","title":"LyceumBase.getobs!","text":"getobs!(obs, sim)\n\n\nCopy sim.d.sensordata into obs.\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#LyceumBase.getobs-Tuple{MJSim}","page":"LyceumMuJoCo","title":"LyceumBase.getobs","text":"getobs(sim::MJSim) -> Any\n\n\nReturn a copy of sim.d.sensordata.\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#LyceumMuJoCo.zeroctrl!-Tuple{MJSim}","page":"LyceumMuJoCo","title":"LyceumMuJoCo.zeroctrl!","text":"zeroctrl!(sim::MJSim) -> MJSim\n\n\nZero out sim.d.ctrl and compute new forward dynamics.\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#LyceumMuJoCo.zerofullctrl!-Tuple{MJSim}","page":"LyceumMuJoCo","title":"LyceumMuJoCo.zerofullctrl!","text":"zerofullctrl!(sim::MJSim) -> MJSim\n\n\nZero out all of the fields in sim.d that contribute to forward dynamics calculations, namely ctrl, qfrc_applied, and xfrc_applied, and compute the forward dynamics.\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#LyceumMuJoCo.forward!-Tuple{MJSim}","page":"LyceumMuJoCo","title":"LyceumMuJoCo.forward!","text":"forward!(sim::MJSim) -> MJSim\n\n\nCompute the forward dynamics of the simulation and store them in sim.d. Equivalent to mj_forward(sim.m, sim.d).\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#LyceumBase.timestep-Tuple{MJSim}","page":"LyceumMuJoCo","title":"LyceumBase.timestep","text":"timestep(sim::MJSim) -> Float64\n\n\nReturn the effective timestep of sim. Equivalent to sim.skip * sim.m.opt.timestep.\n\n\n\n\n\n","category":"method"},{"location":"lyceummujoco/lyceummujoco/#Base.Libc.time-Tuple{MJSim}","page":"LyceumMuJoCo","title":"Base.Libc.time","text":"time(sim::MJSim) -> Float64\n\n\nReturn the current simulation time, in seconds, of sim. Equivalent to sim.d.time.\n\n\n\n\n\n","category":"method"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"#md # !!! note \"Running examples locally\" #md #     This example and more are also available as Julia scripts and Jupyter notebooks. #md # #md #     See the how-to page for more information. #md #","category":"page"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"lyceumbase/abstractenvironment/#AbstractEnvironment-1","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"CurrentModule = LyceumDocs.LyceumBase","category":"page"},{"location":"lyceumbase/abstractenvironment/#Overview-1","page":"AbstractEnvironment","title":"Overview","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"What follows is the AbstractEnvironment interface in its entirety. For users implementing new environments, only a subset of the methods discussed below are required. The remaining methods are built off of that subset and should not be implemented directly. Some of the required methods may have defaults.","category":"page"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"Required Methods","category":"page"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"State\nstatespace(env)\ngetstate!(state, env)\nsetstate!(env, state)\nObservation\nobsspace(env)\ngetobs!(obs, env)\nAction\nactionspace(env)\ngetaction!(action, env)\nsetaction!(env, action)\nReward\nrewardspace(env)\ngetreward(env)\nEvaluation\nevalspace(env)\ngeteval(env)\nSimulation\nreset!(env)\nrandreset!(env)\nstep!(env)\nisdone(env)\ntimestep(env)\nBase.time(env)","category":"page"},{"location":"lyceumbase/abstractenvironment/#API-1","page":"AbstractEnvironment","title":"API","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"AbstractEnvironment","category":"page"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.AbstractEnvironment","page":"AbstractEnvironment","title":"LyceumBase.AbstractEnvironment","text":"AbstractEnvironment\n\nSupertype for all environments.\n\n\n\n\n\n","category":"type"},{"location":"lyceumbase/abstractenvironment/#State-1","page":"AbstractEnvironment","title":"State","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"statespace\ngetstate!\nsetstate!\ngetstate","category":"page"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.statespace","page":"AbstractEnvironment","title":"LyceumBase.statespace","text":"statespace(sim::MJSim) -> Any\n\n\nReturn a description of sim's statespace.\n\n\n\n\n\nstatespace(env::AbstractEnvironment) --> Shapes.AbstractShape\n\nReturns a subtype of Shapes.AbstractShape describing the state space of env.\n\nSee also: getstate!, setstate!, getstate.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.getstate!","page":"AbstractEnvironment","title":"LyceumBase.getstate!","text":"getstate!(state, sim)\n\n\nCopy the following state fields from sim.d into state:\n\n(time, qpos, qvel, act, mocap_pos, mocap_quat, userdata, qacc_warmstart)\n\n\n\n\n\ngetstate!(state, env::AbstractEnvironment)\n\nStore the current state of env in state, where state conforms to the state space returned by statespace(env).\n\nSee also: statespace, setstate!, getstate.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.setstate!","page":"AbstractEnvironment","title":"LyceumBase.setstate!","text":"setstate!(sim, state)\n\n\nCopy the components of state to their respective fields in sim.d, namely:\n\n(time, qpos, qvel, act, mocap_pos, mocap_quat, userdata, qacc_warmstart)\n\n\n\n\n\nsetstate!(env::AbstractEnvironment, state)\n\nSet the state of env to state, where state conforms to the state space returned by statespace(env).\n\nSee also: statespace, getstate!, getstate.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes must guarantee that calls to other \"getter\" functions (e.g. getreward) after a call to setstate! reflect the new, passed-in state.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.getstate","page":"AbstractEnvironment","title":"LyceumBase.getstate","text":"getstate(sim)\n\n\nReturn a flattened vector of the following state fields from sim.d:\n\n(time, qpos, qvel, act, mocap_pos, mocap_quat, userdata, qacc_warmstart)\n\n\n\n\n\ngetstate(env::AbstractEnvironment)\n\nGet the current state of env. The returned value will be an object conforming to the state space returned by statespace(env).\n\nSee also: statespace, getstate!, setstate!.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should implement statespace and getstate!, which are used internally by getstate.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#Observation-1","page":"AbstractEnvironment","title":"Observation","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"obsspace\ngetobs!\ngetobs","category":"page"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.obsspace","page":"AbstractEnvironment","title":"LyceumBase.obsspace","text":"obsspace(sim::MJSim) -> Any\n\n\nReturn a description of sim's observation space.\n\n\n\n\n\nobsspace(env::AbstractEnvironment) --> Shapes.AbstractShape\n\nReturns a subtype of Shapes.AbstractShape describing the observation space of env.\n\nSee also: getobs!, getobs.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.getobs!","page":"AbstractEnvironment","title":"LyceumBase.getobs!","text":"getobs!(obs, sim)\n\n\nCopy sim.d.sensordata into obs.\n\n\n\n\n\ngetobs!(obs, env::AbstractEnvironment)\n\nStore the current observation of env in obs, where obs conforms to the observation space returned by obsspace(env).\n\nSee also: obsspace, getobs.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.getobs","page":"AbstractEnvironment","title":"LyceumBase.getobs","text":"getobs(sim::MJSim) -> Any\n\n\nReturn a copy of sim.d.sensordata.\n\n\n\n\n\ngetobs(env::AbstractEnvironment)\n\nGet the current observation of env. The returned value will be an object conforming to the observation space returned by obsspace(env).\n\nSee also: obsspace, getobs!.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should implement obsspace and getobs!, which are used internally by getobs.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#Action-1","page":"AbstractEnvironment","title":"Action","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"actionspace\ngetaction!\nsetaction!\ngetaction","category":"page"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.actionspace","page":"AbstractEnvironment","title":"LyceumBase.actionspace","text":"actionspace(sim::MJSim) -> Any\n\n\nReturn a description of sim's action space.\n\n\n\n\n\nactionspace(env::AbstractEnvironment) --> Shapes.AbstractShape\n\nReturns a subtype of Shapes.AbstractShape describing the action space of env.\n\nSee also: getaction!, setaction!, getaction.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.getaction!","page":"AbstractEnvironment","title":"LyceumBase.getaction!","text":"getaction!(action::AbstractArray{#s12,1} where #s12<:Real, sim::MJSim) -> Any\n\n\nCopy sim.d.ctrl into action.\n\n\n\n\n\ngetaction!(action, policy, feature)\n\n\nTreating policy as a deterministic policy, compute the mean action of policy, conditioned on feature, and store it in action.\n\n\n\n\n\ngetaction!(action, state, m; nthreads)\n\n\nStarting from the environment's state, perform one step of the MPPI algorithm and store the resulting action in action. The trajectory sampling portion of MPPI is done in parallel using nthreads threads.\n\n\n\n\n\ngetaction!(action, env::AbstractEnvironment)\n\nStore the current action of env in action, where action conforms to the action space returned by actionspace(env).\n\nSee also: actionspace, setaction!, getaction.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.setaction!","page":"AbstractEnvironment","title":"LyceumBase.setaction!","text":"setaction!(sim::MJSim, action::AbstractArray{#s12,1} where #s12<:Real) -> MJSim\n\n\nCopy action into sim.d.ctrl into action and compute the new forward dynamics.\n\n\n\n\n\nsetaction!(env::AbstractEnvironment, action)\n\nSet the action of env to action, where action conforms to the action space returned by actionspace(env).\n\nSee also: actionspace, getaction!, getaction.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes must guarantee that calls to other \"getter\" functions (e.g. getreward) after a call to setaction! reflect the new, passed-in action.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.getaction","page":"AbstractEnvironment","title":"LyceumBase.getaction","text":"getaction(sim::MJSim, action::AbstractArray{#s12,1} where #s12<:Real) -> Any\n\n\nReturn a copy of sim.d.ctrl.\n\n\n\n\n\ngetaction(env::AbstractEnvironment)\n\nGet the current action of env. The returned value will be an object conforming to the action space returned by actionspace(env).\n\nSee also: actionspace, getaction!, setaction!.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should implement actionspace and getaction!, which are used internally by getaction.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#Reward-1","page":"AbstractEnvironment","title":"Reward","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"rewardspace\ngetreward","category":"page"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.rewardspace","page":"AbstractEnvironment","title":"LyceumBase.rewardspace","text":"rewardspace(env::AbstractEnvironment) --> Shapes.AbstractShape\n\nReturns a subtype of Shapes.AbstractShape describing the reward space of env. Defaults to Shapes.ScalarShape{Float64}().\n\nSee also: getreward.\n\nnote: Note\nCurrently, only scalar spaces are supported (e.g. Shapes.ScalarShape).\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.getreward","page":"AbstractEnvironment","title":"LyceumBase.getreward","text":"getreward(state, action, observation, env::AbstractEnvironment)\n\nGet the current reward of env as a function of state, action, and observation. The returned value will be an object conforming to the reward space returned by rewardspace(env).\n\nSee also: rewardspace.\n\nnote: Note\nCurrently, only scalar rewards are supported, so there is no in-place getreward!.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should be careful to ensure that the result of getreward is purely a function of state/action/observation and not any internal, dynamic state contained in env.\n\n\n\n\n\ngetreward(env::AbstractEnvironment)\n\nGet the current reward of env.\n\nInternally calls getreward(getstate(env), getaction(env), getobs(env), env).\n\nSee also: rewardspace.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should implement getreward(state, action, observation, env).\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#Evaluation-1","page":"AbstractEnvironment","title":"Evaluation","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"evalspace\ngeteval","category":"page"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.evalspace","page":"AbstractEnvironment","title":"LyceumBase.evalspace","text":"evalspace(env::AbstractEnvironment) --> Shapes.AbstractShape\n\nReturns a subtype of Shapes.AbstractShape describing the evaluation space of env. Defaults to Shapes.ScalarShape{Float64}().\n\nSee also: geteval.\n\nnote: Note\nCurrently, only scalar evaluation spaces are supported (e.g. Shapes.ScalarShape).\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.geteval","page":"AbstractEnvironment","title":"LyceumBase.geteval","text":"geteval(state, action, observation, env::AbstractEnvironment)\n\nGet the current evaluation metric of env as a function of state, action, and observation. The returned value will be an object conforming to the evaluation space returned by evalspace(env).\n\nOften times reward functions are heavily \"shaped\" and hard to interpret. For example, the reward function for bipedal walking may include root pose, ZMP terms, control costs, etc., while success can instead be simply evaluated by distance of the root along an axis. The evaluation metric serves to fill this gap.\n\nThe default behavior is to return getreward(state, action, observation, env::AbstractEnvironment).\n\nSee also: evalspace.\n\nnote: Note\nCurrently, only scalar evaluation metrics are supported, so there is no in-place geteval!.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should be careful to ensure that the result of geteval is purely a function of state/action/observation and not any internal, dynamic state contained in env.\n\n\n\n\n\ngeteval(env::AbstractEnvironment)\n\nGet the current evaluation metric of env.\n\nInternally calls geteval(getstate(env), getaction(env), getobs(env), env).\n\nSee also: evalspace.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should implement geteval(state, action, obs, env).\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#Simulation-1","page":"AbstractEnvironment","title":"Simulation","text":"","category":"section"},{"location":"lyceumbase/abstractenvironment/#","page":"AbstractEnvironment","title":"AbstractEnvironment","text":"reset!\nrandreset!\nstep!\nisdone\ntimestep\nBase.time","category":"page"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.reset!","page":"AbstractEnvironment","title":"LyceumBase.reset!","text":"reset!(m::LyceumAI.MPPI) -> LyceumAI.MPPI\n\n\nResets the canonical control vector to zeros.\n\n\n\n\n\nreset!(env::AbstractEnvironment)\n\nReset env to a fixed, initial state with zero/passive controls.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.randreset!","page":"AbstractEnvironment","title":"LyceumBase.randreset!","text":"randreset!([rng::Random.AbstractRNG, ], env::AbstractEnvironment)\n\nReset env to a random state with zero/passive controls.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should implement randreset!(rng, env).\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.step!","page":"AbstractEnvironment","title":"LyceumBase.step!","text":"step!(sim::MJSim) -> MJSim\nstep!(sim::MJSim, skip::Integer) -> MJSim\n\n\nStep the simulation by skip steps, where skip defaults to sim.skip.\n\nState-dependent controls (e.g. the ctrl, xfrc_applied, qfrc_applied fields of sim.d) should be set before calling step!.\n\n\n\n\n\nstep!(env::AbstractEnvironment)\n\nAdvance env forward by one timestep.\n\nSee also: timestep.\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.isdone","page":"AbstractEnvironment","title":"LyceumBase.isdone","text":"isdone(state, action, observation, env::AbstractEnvironment) --> Bool\n\nReturns true if state, action, and observation meet an early termination condition for env. Defaults to false.\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should be careful to ensure that the result of isdone is purely a function of state/action/observation and not any internal, dynamic state contained in env.\n\n\n\n\n\nisdone(env::AbstractEnvironment)\n\nReturns true if env has met an early termination condition.\n\nInternally calls isdone(getstate(env), getaction(env), getobs(env), env).\n\nnote: Note\nImplementers of custom AbstractEnvironment subtypes should implement isdone(state, action, obs, env).\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#LyceumBase.timestep","page":"AbstractEnvironment","title":"LyceumBase.timestep","text":"timestep(sim::MJSim) -> Float64\n\n\nReturn the effective timestep of sim. Equivalent to sim.skip * sim.m.opt.timestep.\n\n\n\n\n\ntimestep(env::AbstractEnvironment)\n\nReturn the internal simulation timestep, in seconds, of env.\n\nSee also: Base.time.\n\nExamples\n\nenv = FooEnv()\nreset!(env)\nt1 = time(env)\nstep!(env)\nt2 = time(env)\n@assert timestep(env) == (t2 - t1)\n\n\n\n\n\n","category":"function"},{"location":"lyceumbase/abstractenvironment/#Base.Libc.time","page":"AbstractEnvironment","title":"Base.Libc.time","text":"time(sim::MJSim) -> Float64\n\n\nReturn the current simulation time, in seconds, of sim. Equivalent to sim.d.time.\n\n\n\n\n\nBase.time(env::AbstractEnvironment)\n\nReturns the current simulation time, in seconds, of env. By convention, time(env) should return zero after a call to reset!(env) or randreset!(env).\n\nSee also: timestep.\n\n\n\n\n\n","category":"function"},{"location":"lyceummujocoviz/lyceummujocoviz/#","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"#md # !!! note \"Running examples locally\" #md #     This example and more are also available as Julia scripts and Jupyter notebooks. #md # #md #     See the how-to page for more information. #md #","category":"page"},{"location":"lyceummujocoviz/lyceummujocoviz/#","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"lyceummujocoviz/lyceummujocoviz/#LyceumMuJoCoViz-1","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"","category":"section"},{"location":"lyceummujocoviz/lyceummujocoviz/#","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"CurrentModule = LyceumDocs.LyceumMuJoCoViz","category":"page"},{"location":"lyceummujocoviz/lyceummujocoviz/#Overview-1","page":"LyceumMuJoCoViz","title":"Overview","text":"","category":"section"},{"location":"lyceummujocoviz/lyceummujocoviz/#","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"LyceumMuJoCoViz is an OpenGL-based interactive visualizer for MuJoCo-based models and environments. It allows for visualizing passive dynamics, playing back recorded trajectories, and interacting with a policy or controller in real time. Several additional features are provided:","category":"page"},{"location":"lyceummujocoviz/lyceummujocoviz/#","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"Interact with the simulation using the mouse and keyboard.\n\"Burst mode\", the ability to render multiple snapshots along an entire trajectory at once.\nRun the simulation faster or slower than real time, or in reverse.\nRecord a high-resolution video of the rendered scene.\nStep through time, one or several timesteps at a time.","category":"page"},{"location":"lyceummujocoviz/lyceummujocoviz/#","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"All of these commands are controlled via keyboard shortcuts which are displayed in a help window when the visualizer is launched:","category":"page"},{"location":"lyceummujocoviz/lyceummujocoviz/#","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"<img class=\"center\" src=\"../examples/visualize.png\" width=\"75%\"/>","category":"page"},{"location":"lyceummujocoviz/lyceummujocoviz/#API-1","page":"LyceumMuJoCoViz","title":"API","text":"","category":"section"},{"location":"lyceummujocoviz/lyceummujocoviz/#","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz","text":"visualize","category":"page"},{"location":"lyceummujocoviz/lyceummujocoviz/#LyceumMuJoCoViz.visualize","page":"LyceumMuJoCoViz","title":"LyceumMuJoCoViz.visualize","text":"visualize(model::Union{LyceumMuJoCo.AbstractMuJoCoEnvironment, MJSim}; trajectories, controller)\n\n\nStarts an interactive visualization of model, which can be either a valid subtype of AbstractMuJoCoEnvironment or just a MJSim simulation. The visualizer has several \"modes\" that allow you to visualize passive dynamics, play back recorded trajectories, and run a controller interactively. The passive dynamics mode depends only on model and is always available, while the other modes are specified by the keyword arguments below.\n\nFor more information, see the on-screen help menu.\n\nKeywords\n\ntrajectories::AbstractVector{<:AbstractMatrix}: a vector of trajectories, where each   trajectory is an AbstractMatrix of states with size (length(statespace(model)), T) and   T is the length of the trajectory. Note that each trajectory can have different length.\ncontroller: a callback function with the signature controller(model), called at each   timestep, that that applys a control input to the system.\n\nExamples\n\nusing LyceumMuJoCo, LyceumMuJoCoViz\nenv = LyceumMuJoCo.HopperV2()\nT = 100\nstates = Array(undef, statespace(env), T)\nfor t = 1:T\n    step!(env)\n    states[:, t] .= getstate(env)\nend\nvisualize(\n    env,\n    trajectories=[states],\n    controller = env -> setaction!(env, rand(actionspace(env)))\n)\n\n\n\n\n\n","category":"function"},{"location":"#","page":"Home","title":"Home","text":"#md # !!! note \"Running examples locally\" #md #     This example and more are also available as Julia scripts and Jupyter notebooks. #md # #md #     See the how-to page for more information. #md #","category":"page"},{"location":"#","page":"Home","title":"Home","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"#Home-1","page":"Home","title":"Home","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Welcome to Lyceum, a framework for developing reinforcement learning, trajectory optimization, and other algorithms for continuous control problems in Julia. The primary goal of Lyceum is to increase research throughput and creativity by leveraging the flexible, performant nature of Julia and its cutting-edge ecosystem.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"We hope the community can build on these tools to produce more creative (and performant!) methods.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The Lyceum ecosystem is organized into several core packages:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"LyceumBase, a lightweight package consisting of   common interface definitions and utilities used throughout Lyceum, such as the   AbstractEnvironment type that provides a (PO)MDP-like environment abstraction for   robotic control.\nLyceumAI, a collection of trajectory optimization,   reinforcement learning, and other algorithms for robotic control.\nLyceumMuJoCo, a variety of environments   implementing the AbstractEnvironment interface built on the MuJoCo physics simulator.\nLyceumMuJoCoViz, a feature-rich   interactive visualizer for LyceumMuJoCo.\nShapes, a high-performance library for viewing   flat (e.g. vector) data as structured data.\nUniversalLogger, a small package   that implements Julia's logging interface and provides a general key-value store for   logging experimental data.\nMuJoCo, a low-level wrapper for the MuJoCo physics   library.\nLyceum, a meta-package combining all of the above.","category":"page"},{"location":"#Initial-Setup-1","page":"Home","title":"Initial Setup","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Julia can be downloaded for Windows, Mac, and Linux. We recommend the newest version. Lyceum tests against all three platforms, but the authors primarily use Linux so your mileage may vary.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"As Lyceum is still under heavy development, all Lyceum packages are currently registered in the LyceumRegistry. Note that in the future, Lyceum will migrate to Julia's General registry. For now, however, we can add LyceumRegistry by first entering the ] key into the REPL to enter \"Pkg Mode\":","category":"page"},{"location":"#","page":"Home","title":"Home","text":"julia> ]\n(v1.3) pkg> registry add https://github.com/Lyceum/LyceumRegistry\n   Cloning registry from \"https://github.com/Lyceum/LyceumRegistry\"\n     Added registry `LyceumRegistry` to `~/.julia/registries/LyceumRegistry`","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Now you can add the Lyceum packages to your environment:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"(v1.3) pkg> add LyceumBase","category":"page"},{"location":"#","page":"Home","title":"Home","text":"For more information on registrys and packaging, checkout the Julia Pkg.jl docs.","category":"page"},{"location":"#Supporting-and-Citing-1","page":"Home","title":"Supporting and Citing","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"The Lyceum ecosystem was developed as a part of academic research. If you found Lyceum helpful and would like to support further development, please star the repository(s) as such metrics may help secure further funding in the future. If you use Lyceum as part of your research, teaching, or other engagements, we would be grateful if you could cite our work:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Lyceum: An efficient and scalable ecosystem for robot learning","category":"page"},{"location":"#","page":"Home","title":"Home","text":"@misc{summers2020lyceum,\n    title={Lyceum: An efficient and scalable ecosystem for robot learning},\n    author={Colin Summers and Kendall Lowrey and Aravind Rajeswaran and Siddhartha Srinivasa and Emanuel Todorov},\n    year={2020},\n    eprint={2001.07343},\n    archivePrefix={arXiv},\n    primaryClass={cs.RO}\n}","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"#md # !!! note \"Running examples locally\" #md #     This example and more are also available as Julia scripts and Jupyter notebooks. #md # #md #     See the how-to page for more information. #md #","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"EditURL = \"@__FILE_URL__\"","category":"page"},{"location":"examples/example_howto/#Running-Examples-Locally-1","page":"Running Examples Locally","title":"Running Examples Locally","text":"","category":"section"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"All examples can be found as Julia scripts and Jupyter notebooks in a self-contained Julia project which is available here: examples.tar.gz.","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"Once downloaded, extract the archive with your tool of choice. On Linux machines, you can run:","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"tar xzf examples.tar.gz","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"which will produce a folder in the same directory named \"LyceumExamples\". Inside, you'll find a README.md, reproduced below, with further instructions.","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"","category":"page"},{"location":"examples/example_howto/#LyceumDocs.jl-Examples-1","page":"Running Examples Locally","title":"LyceumDocs.jl Examples","text":"","category":"section"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"Included in this project are all the examples found at LyceumDocs.jl, each available as either a .jl script or Jupyter notebook. To start, open up a Julia REPL with the project activated by executing the following in the directory containing this README:","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"julia --project=.","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"Now press the ] charcter to enter the Pkg REPL-mode. Your prompt should now look like this:","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"(LyceumExamples) pkg>","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"First, we'll add the LyceumRegistry so the package manager knows where to find the Lyceum packages:","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"(LyceumExamples) pkg> registry add https://github.com/Lyceum/LyceumRegistry.git\n   Cloning registry from \"https://github.com/Lyceum/LyceumRegistry.git\"\n     Added registry `LyceumRegistry` to `~/.julia/registries/LyceumRegistry`\n\n(LyceumExamples) pkg>","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"Next, call instantiate to download the required packages:","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"(LyceumExamples) pkg> instantiate\n  Updating registry at `~/.julia/registries/General`\n  Updating git-repo `https://github.com/JuliaRegistries/General.git`\n  Updating registry at `~/.julia/registries/LyceumRegistry`\n  Updating git-repo `https://github.com/Lyceum/LyceumRegistry.git`\n   Cloning git-repo `https://github.com/Lyceum/Lyceum.jl.git`\n\n   ...","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"You can now press Backspace to exit Pkg REPL-mode, returning you to the regular REPL:","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"julia>","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"To run the Julia scripts, simply include them into your current session:","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"julia> include(\"scripts/path/to/example.jl\")","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"Alternative, you can run the notebooks using IJulia:","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"julia> using IJulia\njulia> notebook(dir=\"notebooks/\"; detached=true)","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"The Jupyter notebook should open in your browser automatically. If not, go to http://localhost:8888/ in your browser of choice. From there you can browse and execute the various notebooks.","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"If you run into any trouble, don't hesitate to open an issue on the LyceumDocs.jl repo.","category":"page"},{"location":"examples/example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"Enjoy!","category":"page"}]
}
