{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "In this example we walk through the process of setting up an experiment\n",
    "that runs [Natural Policy Gradient](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf),\n",
    "or more recently [in this work](https://arxiv.org/abs/1703.02660).\n",
    "This is an on-policy reinforcement learning method that is comparable to TRPO,\n",
    "PPO, and other policy gradient methods."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "First, let's go head and grab all the dependencies"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using LinearAlgebra, Random, Statistics # From Stdlib\n",
    "using LyceumAI         # For the NPG controller\n",
    "using LyceumMuJoCo     # For the Hopper environment\n",
    "using LyceumBase.Tools # For the ControllerIterator discussed below\n",
    "using Flux             # For our Neural Network Needs\n",
    "using Flux: glorot_uniform\n",
    "using UniversalLogger\n",
    "using Plots"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "We first configure and instantiate of our `Hopper` environment to grab useful\n",
    "environment specific values such as the size of the observation and action vectors."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "env = LyceumMuJoCo.HopperV2();\n",
    "dobs, dact = length(obsspace(env)), length(actionspace(env));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Policy Gradient methods require a policy: a function that takes in the state/observations\n",
    "of the agent, and output an action i.e. `action = π(obs)`.\n",
    "In much of Deep RL, the policy takes the form of a neural network which can be built\n",
    "on top of the [Flux.jl](https://github.com/FluxML/Flux.jl) library.\n",
    "The network below is two layers, mapping from our observation space to 32 hidden units,\n",
    "to the second 32 hidden units layer, before emitting a vector of actions. The activations\n",
    "are all tanh functions, and we initialize the network with Glorot Uniform initializations.\n",
    "The policy is more than just a feed forward nerual network, however. It's treated as a\n",
    "stochastic variable, and thus we track the log of the standard deviation of noise to apply\n",
    "to the action sampling; this is final zero vector of size 'dact'."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "policy = DiagGaussianPolicy(\n",
    "    multilayer_perceptron(\n",
    "        dobs,\n",
    "        32,\n",
    "        32,\n",
    "        dact;\n",
    "        σ = tanh,\n",
    "        initb = glorot_uniform,\n",
    "        initb_final = glorot_uniform,\n",
    "    ),\n",
    "    zeros(dact),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Convert the weights of our policy to `Float32` rather than the default\n",
    "Float64 for performance."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "policy = Flux.paramtype(Float32, policy);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "This NPG implementation uses [Generalized Advantaged Estimation](https://arxiv.org/pdf/1506.02438.pdf),\n",
    "which requires an estimate of the value function `V(state)`, which we represent using\n",
    "a 2-layer, feedforward neural network."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "value = multilayer_perceptron(\n",
    "    dobs,\n",
    "    128,\n",
    "    128,\n",
    "    1;\n",
    "    σ = Flux.relu,\n",
    "    initb = glorot_uniform,\n",
    "    initb_final = glorot_uniform,\n",
    ")\n",
    "value = Flux.paramtype(Float32, value);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "FluxTrainer is an iterator that loops on the Flux object provided.\n",
    "The result at each loop is passed to stopcb below, so you can quit after\n",
    "a number of epochs, convergence, or other criteria; here it's capped at two epochs\n",
    "as a lambda function."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "valueloss(bl, X, Y) = Flux.mse(vec(bl(X)), vec(Y))\n",
    "valuetrainer = FluxTrainer(\n",
    "    optimiser = ADAM(1e-3),\n",
    "    szbatch = 64,\n",
    "    lossfn = valueloss,\n",
    "    stopcb = s -> s.nepochs > 2,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The `NaturalPolicyGradient` iterator is a type that pre-allocates all necesary\n",
    "data structures and performs one gradient update to the policy at each iteration.\n",
    "We first pass in a constructor that given `n` returns `n` instances of `LyceumMuJoCo.HopperV2`,\n",
    "all sharing the same `jlModel`, to allow for performant sampling from our policy.\n",
    "We then pass in the `policy`, `value`, and `valuetrainer` instances constructed above\n",
    "and override a few of the default `NaturalPolicyGradient` parameters: `gamma`, `gaelambda`,\n",
    "and `norm_step_size`. Finally, we set the max trajectory length `Hmax` and total number of\n",
    "samples per iteration, `N`. Under the hood, `NaturalPolicyGradient` will use approximately\n",
    "`div(N, Hmax)` threads to perform the sampling."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "npg = NaturalPolicyGradient(\n",
    "    n -> tconstruct(LyceumMuJoCo.HopperV2, n),\n",
    "    policy,\n",
    "    value,\n",
    "    valuetrainer;\n",
    "    gamma = 0.995,\n",
    "    gaelambda = 0.97,\n",
    "    norm_step_size = 0.05,\n",
    "    Hmax = 1000,\n",
    "    N = 10000,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Finally, let's spin on our iterator 200 times, plotting every 20 iterations.\n",
    "This lets us break out of the loop if certain conditions are met, or re-start training\n",
    "manually if needed. We of course wish to track results, so we create an `Experiment` to\n",
    "which we can save data. We also have useful timing information displayed every\n",
    "20 iterations to understand CPU performance."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "exper = Experiment(\"/tmp/hopper_example.jlso\", overwrite = true)\n",
    "lg = ULogger() # walks, talks, and acts like a Julia logger. See the docs on UniversalLogger for more info.\n",
    "for (i, state) in enumerate(npg)\n",
    "    if i > 200\n",
    "        # serialize some stuff and quit\n",
    "        exper[:policy] = npg.policy\n",
    "        exper[:value] = npg.value\n",
    "        exper[:etype] = LyceumMuJoCo.HopperV2\n",
    "        exper[:meanstates] = state.meanbatch\n",
    "        exper[:stocstates] = state.stocbatch\n",
    "        break\n",
    "    end\n",
    "\n",
    "    # log everything in `state` except meanbatch and stocbatch\n",
    "    push!(lg, :algstate, filter_nt(state, exclude = (:meanbatch, :stocbatch)))\n",
    "\n",
    "    if mod(i, 20) == 0\n",
    "        x = lg[:algstate]\n",
    "        # The following are helper functions for plotting to the terminal.\n",
    "        # The first plot displays the eval function for our stochastic\n",
    "        # and mean policy rollouts.\n",
    "        display(expplot(\n",
    "            Line(x[:stocterminal_eval], \"StocLastE\"),\n",
    "            Line(x[:meanterminal_eval], \"MeanLastE\"),\n",
    "            title = \"Evaluation Score, Iter=$i\",\n",
    "            width = 60,\n",
    "            height = 8,\n",
    "        ))\n",
    "        # While the second one similarly plots the reward.\n",
    "        display(expplot(\n",
    "            Line(x[:stoctraj_reward], \"StocR\"),\n",
    "            Line(x[:meantraj_reward], \"MeanR\"),\n",
    "            title = \"Reward, Iter=$i\",\n",
    "            width = 60,\n",
    "            height = 8,\n",
    "        ))\n",
    "\n",
    "        # The following are timing values for various parts of the Natural Policy Gradient\n",
    "        # algorithm at the last iteration, useful for finding performance bottlenecks in the algorithm.\n",
    "        println(\"elapsed_sampled  = \", state.elapsed_sampled)\n",
    "        println(\"elapsed_gradll   = \", state.elapsed_gradll)\n",
    "        println(\"elapsed_vpg      = \", state.elapsed_vpg)\n",
    "        println(\"elapsed_cg       = \", state.elapsed_cg)\n",
    "        println(\"elapsed_valuefit = \", state.elapsed_valuefit)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Let's go ahead and plot the final reward trajectory for our stochastic and mean policies\n",
    "to see how we did."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot!(\n",
    "    plot(lg[:algstate][:meantraj_reward], label = \"Mean Policy\", title = \"HopperV2 Reward\"),\n",
    "    lg[:algstate][:stoctraj_reward],\n",
    "    label = \"Stochastic Policy\",\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "and save the logged results to the Experiment"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "for (k, v) in get(lg)\n",
    "    exper[k] = v\n",
    "end\n",
    "finish!(exper); # flushes everything to disk"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  },
  "kernelspec": {
   "name": "julia-1.3",
   "display_name": "Julia 1.3.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
