{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "In this example we walk through the process of setting up an experiment\n",
    "that runs [Natural Policy Gradient](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf),\n",
    "or more recently [in this work](https://arxiv.org/abs/1703.02660).\n",
    "This is an on-policy reinforcement learning method that is comparable to TRPO,\n",
    "PPO, and other policy gradient methods."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "First, let's go head and grab all the dependencies"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using LinearAlgebra, Random, Statistics # From Stdlib\n",
    "using LyceumAI         # For the NPG controller\n",
    "using LyceumMuJoCo     # For the Hopper environment\n",
    "using LyceumBase.Tools # For the ControllerIterator discussed below\n",
    "using Flux             # For our Neural Network Needs\n",
    "using Flux: glorot_uniform\n",
    "using UniversalLogger\n",
    "using Plots"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "We first configure and instantiate of our `Hopper` environment to grab useful\n",
    "environment specific values such as the size of the observation and action vectors."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "env = LyceumMuJoCo.HopperV2();\n",
    "dobs, dact = length(obsspace(env)), length(actionspace(env));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Policy Gradient methods require a policy: a function that takes in the state/observations\n",
    "of the agent, and output an action. a = π(obs).\n",
    "In much of Deep RL, the policy takes the form of a neural network, which we instantiate\n",
    "on top of the [Flux.jl](https://github.com/FluxML/Flux.jl) library.\n",
    "The network below is two layers, mapping from our observation space to 32 hidden units,\n",
    "to the second 32 hidden units layer, before emitting a vector of actions. The activations\n",
    "are all tanh functions, and we initialize the network with Glorot Uniform initializations.\n",
    "The policy is more than just a feed forward nerual network, however. It's treated as a\n",
    "stochastic variable, and thus we track the log of the standard deviation of noise to apply\n",
    "to the action sampling; this is final zero vector of size 'dact'."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "policy = DiagGaussianPolicy(multilayer_perceptron(dobs, 32, 32, dact;\n",
    "                                                  σ=tanh,\n",
    "                                                  initb=glorot_uniform,\n",
    "                                                  initb_final=glorot_uniform),\n",
    "                            zeros(dact))\n",
    "policy = Flux.paramtype(Float32, policy); # We make sure the Policy is a consistent type"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "This NPG implementation uses Generalized Advantaged Estimation, where we subract the estimate\n",
    "of the current policy's performance from an estimate of the value function on the same inputs.\n",
    "The calculation of the advangate is more stable in for gradient descent. We represent the value\n",
    "function as a neural network as well."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "value = multilayer_perceptron(dobs, 128, 128, 1;\n",
    "                              σ=Flux.relu,\n",
    "                              initb=glorot_uniform,\n",
    "                              initb_final=glorot_uniform)\n",
    "value = Flux.paramtype(Float32, value); # Again, consistent type; imporant for performance"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "FluxTrainer is an iterator that loops on the Flux object provided.\n",
    "The result at each loop is passed to stopcb below, so you can quit after\n",
    "a number of epochs, convergence, or other criteria; here it's capped at two epochs\n",
    "as a lambda function."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "valueloss(bl, X, Y) = Flux.mse(vec(bl(X)), vec(Y))\n",
    "valuetrainer = FluxTrainer(optimiser = ADAM(1e-3),\n",
    "                           szbatch = 64,\n",
    "                           lossfn = valueloss,\n",
    "                           stopcb = s -> s.nepochs > 2);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The `NaturalPolicyGradient` iterator is a struct that contains relevant data objects\n",
    "for learning a policy. We first pass in a constructor that, given an input Int\n",
    "(in this case the thread ID), will construct an env; this is for thread-safe multi-threading.\n",
    "The policy and value objects are passed as well as a number of parameters; shown here are\n",
    "generally good defaults but could change depending on the environment and problem.\n",
    "Finally, the max trajectory length is set as 'Hmax', and the total\n",
    "number of samples 'N', specified.\n",
    "Multi-threading happens to collect the 'N' samples using as many threads as possible, up to\n",
    "a trajectory length of Hmax."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "npg = NaturalPolicyGradient((i)->tconstruct(LyceumMuJoCo.HopperV2, i),\n",
    "                            policy,\n",
    "                            value,\n",
    "                            valuetrainer;\n",
    "                            gamma = 0.995,\n",
    "                            gaelambda = 0.97,\n",
    "                            norm_step_size = 0.05,\n",
    "                            Hmax=1000,\n",
    "                            N=10000);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Finally, let's spin on our iterator 200 times, plotting every 20 iterations.\n",
    "This lets us break out of the loop if certain conditions are met, or re-start training\n",
    "manually if needed. We of course wish to track results, so we create an Experiment to\n",
    "which we can save data. We also\n",
    "have useful timing information displayed every 20 iterations to understand CPU performance."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "exper = Experiment(\"/tmp/hopper_example.jlso\", overwrite=true)\n",
    "lg = ULogger() # walks, talks, and acts like a Julia logger\n",
    "for (i, state) in enumerate(npg)\n",
    "    if i > 200\n",
    "        # serialize some stuff and quit\n",
    "        exper[:policy]     = npg.policy\n",
    "        exper[:value]      = npg.value\n",
    "        exper[:etype]      = LyceumMuJoCo.HopperV2\n",
    "        exper[:meanstates] = state.meanbatch\n",
    "        exper[:stocstates] = state.stocbatch\n",
    "        break\n",
    "    end\n",
    "\n",
    "    # log everything in `state` except meanbatch and stocbatch\n",
    "    push!(lg, :algstate, filter_nt(state, exclude=(:meanbatch, :stocbatch)))\n",
    "\n",
    "    if mod(i, 20) == 0\n",
    "        x = lg[:algstate]\n",
    "        # The following are helper functions for plotting to the terminal.\n",
    "        # The first plot renders the 'Eval' function associated with the env.\n",
    "        display(expplot(Line(x[:stocterminal_eval], \"StocLastE\"),\n",
    "                        Line(x[:meanterminal_eval], \"MeanLastE\"),\n",
    "                        title=\"Evaluation Score, Iter=$i\", width=60, height=8\n",
    "                       ));\n",
    "\n",
    "        display(expplot(Line(x[:stoctraj_reward], \"StocR\"),\n",
    "                        Line(x[:meantraj_reward], \"MeanR\"),\n",
    "                        title=\"Reward, Iter=$i\", width=60, height=8\n",
    "                       ));\n",
    "\n",
    "        # The following is timing values for each component of the last iteration.\n",
    "        # It's useful to see where the compute is going.\n",
    "        println(\"elapsed_sampled  = \", state.elapsed_sampled); #md\n",
    "        println(\"elapsed_gradll   = \", state.elapsed_gradll);  #md\n",
    "        println(\"elapsed_vpg      = \", state.elapsed_vpg);     #md\n",
    "        println(\"elapsed_cg       = \", state.elapsed_cg);      #md\n",
    "        println(\"elapsed_valuefit = \", state.elapsed_valuefit);#md\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Let's go ahead and plot the final reward trajectory and see how we did. The two\n",
    "lines is a property of a Policy Gradient method: there is a stochastic policy that takes\n",
    "the actions of the policy and adds noise to explore for better behavior."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot!(plot(lg[:algstate][:meantraj_reward], label=\"Mean Policy\", title=\"HopperV2 Reward\"),\n",
    "      lg[:algstate][:stoctraj_reward], label=\"Stochastic Policy\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "and save the logged results to the experiment's JLSO"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "for (k, v) in get(lg)\n",
    "    exper[k] = v\n",
    "end\n",
    "finish!(exper);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  },
  "kernelspec": {
   "name": "julia-1.3",
   "display_name": "Julia 1.3.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
