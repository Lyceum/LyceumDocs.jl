var documenterSearchIndex = {"docs":
[{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"EditURL = \"https://github.com/Lyceum/LyceumDocs.jl/blob/master/docs/src/examples/NPG.jl\"","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"note: Running examples locally\nThis example and more are also available as Julia scripts and Jupyter notebooks.See the how-to page for more information.","category":"page"},{"location":"examples/NPG/#Learning-a-control-policy-1","page":"Learning a control policy","title":"Learning a control policy","text":"","category":"section"},{"location":"examples/NPG/#Policy-Gradient-Example-1","page":"Learning a control policy","title":"Policy Gradient Example","text":"","category":"section"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"In this example we walk through the process of setting up an experiment that runs Natural Policy Gradient, or more recently in this work. This is an on-policy reinforcement learning method that is comparable to TRPO, PPO, and other policy gradient methods.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"First, let's go head and grab all the dependencies","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"using LinearAlgebra, Random, Statistics # From Stdlib\nusing LyceumAI         # For the NPG controller\nusing LyceumMuJoCo     # For the Hopper environment\nusing LyceumBase.Tools # For the ControllerIterator discussed below\nusing Flux             # For our Neural Network Needs\nusing Flux: glorot_uniform\nusing UniversalLogger\nusing Plots","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"We first configure and instantiate of our Hopper environment to grab useful environment specific values such as the size of the observation and action vectors.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"env = LyceumMuJoCo.HopperV2();\ndobs, dact = length(obsspace(env)), length(actionspace(env));\nnothing #hide","category":"page"},{"location":"examples/NPG/#Policy-Gradient-Components-1","page":"Learning a control policy","title":"Policy Gradient Components","text":"","category":"section"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"Policy Gradient methods require a policy: a function that takes in the state/observations of the agent, and output an action i.e. action = π(obs). In much of Deep RL, the policy takes the form of a neural network which can be built on top of the Flux.jl library. The network below is two layers, mapping from our observation space to 32 hidden units, to the second 32 hidden units layer, before emitting a vector of actions. The activations are all tanh functions, and we initialize the network with Glorot Uniform initializations. The policy is more than just a feed forward nerual network, however. It's treated as a stochastic variable, and thus we track the log of the standard deviation of noise to apply to the action sampling; this is final zero vector of size 'dact'.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"policy = DiagGaussianPolicy(\n    multilayer_perceptron(\n        dobs,\n        32,\n        32,\n        dact;\n        σ = tanh,\n        initb = glorot_uniform,\n        initb_final = glorot_uniform,\n    ),\n    zeros(dact),\n)","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"Convert the weights of our policy to Float32 rather than the default Float64 for performance.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"policy = Flux.paramtype(Float32, policy);\nnothing #hide","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"This NPG implementation uses Generalized Advantaged Estimation, which requires an estimate of the value function V(state), which we represent using a 2-layer, feedforward neural network.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"value = multilayer_perceptron(\n    dobs,\n    128,\n    128,\n    1;\n    σ = Flux.relu,\n    initb = glorot_uniform,\n    initb_final = glorot_uniform,\n)\nvalue = Flux.paramtype(Float32, value);\nnothing #hide","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"FluxTrainer is an iterator that loops on the Flux object provided. The result at each loop is passed to stopcb below, so you can quit after a number of epochs, convergence, or other criteria; here it's capped at two epochs as a lambda function.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"valueloss(bl, X, Y) = Flux.mse(vec(bl(X)), vec(Y))\nvaluetrainer = FluxTrainer(\n    optimiser = ADAM(1e-3),\n    szbatch = 64,\n    lossfn = valueloss,\n    stopcb = s -> s.nepochs > 2,\n);\nnothing #hide","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"The NaturalPolicyGradient iterator is a type that pre-allocates all necesary data structures and performs one gradient update to the policy at each iteration. We first pass in a constructor that given n returns n instances of LyceumMuJoCo.HopperV2, all sharing the same jlModel, to allow for performant sampling from our policy. We then pass in the policy, value, and valuetrainer instances constructed above and override a few of the default NaturalPolicyGradient parameters: gamma, gaelambda, and norm_step_size. Finally, we set the max trajectory length Hmax and total number of samples per iteration, N. Under the hood, NaturalPolicyGradient will use approximately div(N, Hmax) threads to perform the sampling.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"npg = NaturalPolicyGradient(\n    n -> tconstruct(LyceumMuJoCo.HopperV2, n),\n    policy,\n    value,\n    valuetrainer;\n    gamma = 0.995,\n    gaelambda = 0.97,\n    norm_step_size = 0.05,\n    Hmax = 1000,\n    N = 10000,\n);\nnothing #hide","category":"page"},{"location":"examples/NPG/#Running-Experiments-1","page":"Learning a control policy","title":"Running Experiments","text":"","category":"section"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"Finally, let's spin on our iterator 200 times, plotting every 20 iterations. This lets us break out of the loop if certain conditions are met, or re-start training manually if needed. We of course wish to track results, so we create an Experiment to which we can save data. We also have useful timing information displayed every 20 iterations to understand CPU performance.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"exper = Experiment(\"/tmp/hopper_example.jlso\", overwrite = true)\nlg = ULogger() # walks, talks, and acts like a Julia logger. See the docs on UniversalLogger for more info.\nfor (i, state) in enumerate(npg)\n    if i > 200\n        # serialize some stuff and quit\n        exper[:policy] = npg.policy\n        exper[:value] = npg.value\n        exper[:etype] = LyceumMuJoCo.HopperV2\n        exper[:meanstates] = state.meanbatch\n        exper[:stocstates] = state.stocbatch\n        break\n    end\n\n    # log everything in `state` except meanbatch and stocbatch\n    push!(lg, :algstate, filter_nt(state, exclude = (:meanbatch, :stocbatch)))\n\n    if mod(i, 20) == 0\n        x = lg[:algstate]\n        # The following are helper functions for plotting to the terminal.\n        # The first plot displays the eval function for our stochastic\n        # and mean policy rollouts.\n        display(expplot(\n            Line(x[:stocterminal_eval], \"StocLastE\"),\n            Line(x[:meanterminal_eval], \"MeanLastE\"),\n            title = \"Evaluation Score, Iter=$i\",\n            width = 60,\n            height = 8,\n        ))\n        # While the second one similarly plots the reward.\n        display(expplot(\n            Line(x[:stoctraj_reward], \"StocR\"),\n            Line(x[:meantraj_reward], \"MeanR\"),\n            title = \"Reward, Iter=$i\",\n            width = 60,\n            height = 8,\n        ))\n\n        # The following are timing values for various parts of the Natural Policy Gradient\n        # algorithm at the last iteration, useful for finding performance bottlenecks in the algorithm.\n        println(\"elapsed_sampled  = \", state.elapsed_sampled)\n        println(\"elapsed_gradll   = \", state.elapsed_gradll)\n        println(\"elapsed_vpg      = \", state.elapsed_vpg)\n        println(\"elapsed_cg       = \", state.elapsed_cg)\n        println(\"elapsed_valuefit = \", state.elapsed_valuefit)\n    end\nend","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"Let's go ahead and plot the final reward trajectory for our stochastic and mean policies to see how we did.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"plot!(\n    plot(lg[:algstate][:meantraj_reward], label = \"Mean Policy\", title = \"HopperV2 Reward\"),\n    lg[:algstate][:stoctraj_reward],\n    label = \"Stochastic Policy\",\n)","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"and save the logged results to the Experiment","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"for (k, v) in get(lg)\n    exper[k] = v\nend\nfinish!(exper); # flushes everything to disk\nnothing #hide","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"EditURL = \"https://github.com/Lyceum/LyceumDocs.jl/blob/master/docs/src/examples/visualize.jl\"","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"note: Running examples locally\nThis example and more are also available as Julia scripts and Jupyter notebooks.See the how-to page for more information.","category":"page"},{"location":"examples/visualize/#Visualizing-Results-1","page":"Visualizing Results","title":"Visualizing Results","text":"","category":"section"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"using LyceumBase, LyceumAI, MuJoCo, Shapes, FastClosures\nusing LyceumMuJoCo    # We need to bring the Pointmass, Hopper Envs into the workspace\nusing LyceumMuJoCoViz # This provides our visualization needs.\nusing JLSO\nusing FastClosures","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"The following functions present two modes of visualizing results of NPG or MPPI packaged in LyceumAI. They both work by passing into the 'visualize' function a control function and/or trajectory data to be rendered. These inputs select from available visualization modes TODO list the visualiza3tion modes here. that operate on either data passed in (the trajectory option) or will simulate the forward dynamics of the environment, and call the pass in control function appropriately. This allows for interactive policy evaluation or full model predictive control with MPPI.","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"function viz_mppi(mppi::MPPI, env::AbstractMuJoCoEnvironment)\n    a = allocate(actionspace(env))\n    o = allocate(obsspace(env))\n    s = allocate(statespace(env))\n\n    ctrlfn =\n        @closure env -> (getstate!(s, env); getaction!(a, s, o, mppi); setaction!(env, a))\n\n    # The above line is functionally the same as:\n    # ctrlfn(env) = begin\n    #     getstate!(s, env)\n    #     getaction!(a, s, o, mppi)\n    #     setaction!(env, a)\n    # end\n    visualize(env, controller = ctrlfn)\nend\n\nfunction viz_policy(\n    path::AbstractString,\n    etype::Union{Nothing,Type{<:AbstractMuJoCoEnvironment}} = nothing,\n)\n    x = JLSO.load(path)\n    etype = isnothing(etype) ? x[\"etype\"] : etype\n    env = etype() # Load the environment based on what was in the JLSO file.\n\n    # Check if the JLSO has a policy saved, if so, load it and prepare to render\n    # the stochastic rollouts of the policy, and form a control function that is the policy's\n    # mean outputs w.r.t. observations.\n    # Otherwise, just render states, such as the output of MPPI.\n    pol = haskey(x, \"policy\") ? x[\"policy\"] : nothing\n\n    if pol == nothing\n        visualize(env; trajectories = [x[\"states\"]])\n    else\n        a = allocate(actionspace(env))\n        o = allocate(obsspace(env))\n        ctrlfn = @closure (env) -> (getobs!(o, env); a .= pol(o); setaction!(env, a))\n\n        states = x[\"stocstates\"].states\n        visualize(env, controller = ctrlfn, trajectories = states)\n    end\nend","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"Assuming we have the saved jlso files from the previous examples, we can call the above functions as such:","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"TODO the following lines are bullshit; don't run on build, add to md scripts and notebook","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"viz_policy(\"/tmp/hopper_example.jlso\", LyceumMuJoCo.HopperV2)\nviz_policy(\"/tmp/opt_humanoid.jlso\", Humanoid)\nviz_mppi(mppi, LyceumMuJoCo.PointMass())","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"This page was generated using Literate.jl.","category":"page"},{"location":"example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"EditURL = \"@__REPO_ROOT_URL__/docs/src/example_howto.md\"","category":"page"},{"location":"example_howto/#Running-Examples-Locally-1","page":"Running Examples Locally","title":"Running Examples Locally","text":"","category":"section"},{"location":"example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"All examples can be found as Julia scripts and Jupyter notebooks in a self-contained Julia project which is available here: examples.tar.gz.","category":"page"},{"location":"example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"Once downloaded, extract the archive with your tool of choice. On Linux machines, you can run:","category":"page"},{"location":"example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"tar xzf examples.tar.gz","category":"page"},{"location":"example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"which will produce a folder in the same directory named \"LyceumExamples\". Inside, you'll find a README.md, reproduced below, with further instructions.","category":"page"},{"location":"example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"","category":"page"},{"location":"example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"@EXAMPLES_README","category":"page"},{"location":"#","page":"Home","title":"Home","text":"EditURL = \"@__REPO_ROOT_URL__/docs/src/index.md\"","category":"page"},{"location":"#Home-1","page":"Home","title":"Home","text":"","category":"section"},{"location":"#Lyceum-1","page":"Home","title":"Lyceum","text":"","category":"section"},{"location":"#Package-Statuses-1","page":"Home","title":"Package Statuses","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"import LyceumDocs: package_table_markdown_nodocs, LYCEUM_PACKAGE_DEFS\npackage_table_markdown_nodocs(LYCEUM_PACKAGE_DEFS)","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"EditURL = \"https://github.com/Lyceum/LyceumDocs.jl/blob/master/docs/src/examples/MPPI.jl\"","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"note: Running examples locally\nThis example and more are also available as Julia scripts and Jupyter notebooks.See the how-to page for more information.","category":"page"},{"location":"examples/MPPI/#Running-a-simple-controller-1","page":"Running a simple controller","title":"Running a simple controller","text":"","category":"section"},{"location":"examples/MPPI/#Introduction-1","page":"Running a simple controller","title":"Introduction","text":"","category":"section"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"In this example we walk through the process of setting up an experiment that runs Model-Predictive Path Integral Control, or \"MPPI\", a Model-Predictive Control method, on a simple PointMass environment.","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"First, let's go head and grab all the dependencies","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"using LinearAlgebra, Random, Statistics # From Stdlib\nusing LyceumAI # For the MPPI controller\nusing LyceumMuJoCo # For the PointMass environment\nusing LyceumBase.Tools # For the ControllerIterator discussed below\nusing Plots","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"TODO LINK REF Next, we'll define an Experiment which we'll use to log the trajectory executed by our controller and save the results to \"/tmp/pointmass.jlso\".","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"exper = Experiment(\"/tmp/pointmass.jlso\", overwrite = true);\nnothing #hide","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"Then we configure and instantiate of our PointMass environment and MPPI controller. See the documention for MPPI to learn more about its parameters. TODO LINK REF","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"env = LyceumMuJoCo.PointMass();\nmppi = MPPI(\n    env_tconstructor = i -> tconstruct(LyceumMuJoCo.PointMass, i),\n    covar0 = Diagonal(0.1^2 * I, size(actionspace(env), 1)),\n    lambda = 0.01,\n    K = 32,\n    H = 10,\n    gamma = 0.99,\n);\nnothing #hide","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"Finally, let's rollout our controller for 1000 timesteps. As discused in the algorithms section TODO LINK REF, AbstractController's are by themselves not iterable, so we wrap them in a ControllerIterator which will apply the controls generated by MPPI to the environment at each timestep. We'll also plot the progress so far to the terminal every 100 timesteps. For clarity, we do not reproduce these plots here, but you'll see them when you run this example locally!","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"iter = ControllerIterator(mppi, env; T = 300, plotiter = 50);\nfor (t, traj) in iter\nend","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"Let's go ahead and plot the final reward trajectory and see how we did:","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"plot(iter.trajectory.rewards)","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"and save the results:","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"for (k, v) in pairs(iter.trajectory)\n    exper[k] = v\nend\nfinish!(exper);\nnothing #hide","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"EditURL = \"https://github.com/Lyceum/LyceumDocs.jl/blob/master/docs/src/examples/humanoid.jl\"","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"note: Running examples locally\nThis example and more are also available as Julia scripts and Jupyter notebooks.See the how-to page for more information.","category":"page"},{"location":"examples/humanoid/#Creating-a-MuJoCo-Environment-1","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"Using Mujoco, we can create an environment that mostly relies on the defaults of LyceumBase and LyceumMuJoCo to propagate state, action, and observation data. We will have to include a reward and evaluation function, of course, and also show how the functions can be customized for different tasks.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"First we grab our dependencies of the Lyceum ecosystem and other helpful packages","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"using LinearAlgebra, Random, Statistics, UnicodePlots, JLSO\nusing LyceumBase, LyceumAI, LyceumMuJoCo, MuJoCo, UniversalLogger, Shapes\nusing LyceumBase.Tools\nusing Shapes: AbstractVectorShape\nimport LyceumBase: tconstruct\nusing Plots","category":"page"},{"location":"examples/humanoid/#Humanoid-Type-1","page":"Creating a MuJoCo Environment","title":"Humanoid Type","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"This struct is our primary entry into the environment API. As you can see, this simple example only wraps around the underlying simulator. The functions of the LyceumBase API will dispatch on this struct. When an algorithm calls a function such as getobs!, Julia will select from all functions with that name depending on the type of environment that is passed in.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"struct Humanoid{S} <: AbstractMuJoCoEnvironment\n    sim::S\nend","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"Humanoid (and all subtypes of AbstractEnvironment) are designed to be used in a single threaded context. To use Humanoid, one would simply create Threads.nthreads() instances of Humanoid. As Humanoid only ever uses jlModel in a read-only fashion, we can make a performance optimization by sharing a single instance of jlModel across each thread, resulting in improved cache efficiency. LyceumMuJoCo.tconstruct helps us to do just that by providing a common interface for defining \"thread-aware\" constructors. Below, we make a call to tconstruct(MJSim, n, modelpath, skip = 2) which will construct n instances of MJSim constructed from modelpath and with a skip of 2, all sharing the exact same jlModel instance.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"Humanoid() = first(tconstruct(Humanoid, 1))\nfunction LyceumMuJoCo.tconstruct(::Type{Humanoid}, n::Integer)\n    modelpath = joinpath(@__DIR__, \"humanoid.xml\")\n    Tuple(Humanoid(s) for s in tconstruct(MJSim, n, modelpath, skip = 2))\nend;\nnothing #hide","category":"page"},{"location":"examples/humanoid/#Customizing-1","page":"Creating a MuJoCo Environment","title":"Customizing","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"The following are helpers for the tasks we'd like to consider. We want the humanoid to stand up, thus we need to grab the model's height, as well as record a laying down position that we can use to set the state to. By exploring the model in the REPL or xml we can see that qpos[3] is the index for the z-axis (height) of the root joint. The LAYING_QPOS data was collected externally by posing the model into a supine pose; one can either use LyceumMuJoCoViz.jl or simulate.cpp included with a MuJoCo release to do this.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"_getheight(shapedstate::ShapedView, ::Humanoid) = shapedstate.qpos[3]\nconst LAYING_QPOS = [\n    -0.164158,\n    0.0265899,\n    0.101116,\n    0.684044,\n    -0.160277,\n    -0.70823,\n    -0.0693176,\n    -0.1321,\n    0.0203937,\n    0.298099,\n    0.0873523,\n    0.00634907,\n    0.117343,\n    -0.0320319,\n    -0.619764,\n    0.0204114,\n    -0.157038,\n    0.0512385,\n    0.115817,\n    -0.0320437,\n    -0.617078,\n    -0.00153819,\n    0.13926,\n    -1.01785,\n    -1.57189,\n    -0.0914509,\n    0.708539,\n    -1.57187,\n];\nnothing #hide","category":"page"},{"location":"examples/humanoid/#Lyceum-API-Simple-Setup-1","page":"Creating a MuJoCo Environment","title":"Lyceum API Simple Setup","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"LyceumMuJoCo requires access to the underlying MJSim simulator, thus any LyceumMuJoCo environments need to point to the correct field in the env struct that is the simulator; in our case there's only one field.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"LyceumMuJoCo.getsim(env::Humanoid) = env.sim","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"Normally we could rely on MuJoCo to reset the model to the default configuration when the model XML is loaded; in humanoid.xml's case, it is in a vertical position. To reset the model to our laying down or supine pose, we can copy in the data from the const array above to d.qpos. Calling forward! here is the same as mj_forward(m,d), for a pure MuJoCo reference.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"function LyceumMuJoCo.reset!(env::Humanoid)\n    reset!(env.sim)\n    env.sim.d.qpos .= LAYING_QPOS\n    forward!(env.sim)\n    env\nend","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"This reward function uses the _getheight helper function above to get the model's height when the function is called. We also specify a target height of 1.25, and penalize the agent for deviating from the target height. There is also a small penalty for using large control activations; if the coefficient is made larger, the agent may not move at all!","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"function LyceumMuJoCo.getreward(state, action, obs, env::Humanoid)\n    height = _getheight(statespace(env)(state), env)\n    target = 1.25\n\n    reward = 1.0\n    if height < target\n        reward -= 2.0 * abs(target - height)\n    end\n\n    reward -= 1e-3 * norm(action)^2\n\n    return reward\nend","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"Finally, we can specify an evaluation function. The difference between the eval and reward functions are that we can track a useful value, such as height with geteval, but an algorithm like MPPI or NPG may need a shaped function to guide any optimization. Plotting this eval function will show the agent's height over time: this is very useful for reviewing actual desired behavior, regardless of the reward achieved, as it can be used to diagnose reward specification problems.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"function LyceumMuJoCo.geteval(state, action, obs, env::Humanoid)\n    return _getheight(statespace(env)(state), env)\nend","category":"page"},{"location":"examples/humanoid/#Running-Experiments-1","page":"Creating a MuJoCo Environment","title":"Running Experiments","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"As discussed in the Julia performance tips, globals can hinder performance. To avoid this, we construct the MPPI and ControllerIterator instances within a function. This also lets us easily run our experiment with different choices of parameters (e.g. H). Like most algorithms in LyceumAI, MPPI accepts a thread-aware environment constructor as well as any algorithm parameters. MPPI itself is not iterable, so we wrap it in ControllerIterator which simple calls getaction!(action, state, obs, mppi::MPPI) for T timesteps, while simultaneously plotting and logging the trajectory rollout.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"function hmMPPI(etype = Humanoid; T = 200, H = 64, K = 64)\n    env = etype()\n\n    # The following parameters work well for this get-up tasks, and make work for\n    # other similar tasks, but are not invariant to the model.\n    mppi = MPPI(\n        env_tconstructor = i -> tconstruct(etype, i),\n        covar0 = Diagonal(0.05^2 * I, size(actionspace(env), 1)),\n        lambda = 0.4,\n        H = H,\n        K = K,\n        gamma = 1.0,\n    )\n\n    iter = ControllerIterator(mppi, env; T = T, plotiter = div(T, 10))\n    # We can time the following loop; if it ends up less than the time the\n    # MuJoCo models integrated forward in, then one could conceivably run this\n    # MPPI MPC controller interactively...\n    @time for (t, traj) in iter","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"If desired, one can inspect traj, env, or mppi at each timestep.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"    end","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"Save our experiment results to a file for later review.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"    savepath = \"/tmp/opt_humanoid.jlso\"\n    exper = Experiment(savepath, overwrite = true)\n    exper[:etype] = etype\n\n    for (k, v) in pairs(iter.trajectory)\n        exper[k] = v\n    end\n    finish!(exper)\n\n    return mppi, iter\nend\n\nm, d = hmMPPI()","category":"page"},{"location":"examples/humanoid/#Checking-Results-1","page":"Creating a MuJoCo Environment","title":"Checking Results","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"The MPPI algorithm, and any that you develop, can and should use plotting tools to track progress as they go. If one wanted to review the results after training, or prepare plots for presentations, one can load the data from disk instead.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"x = JLSO.load(\"/tmp/opt_humanoid.jlso\")\nplot!(\n    plot(d.trajectory.rewards, label = \"Inst. Reward\", title = \"Humanoid Standup\"),\n    d.trajectory.evaluations,\n    label = \"Evaluation\",\n)","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"This page was generated using Literate.jl.","category":"page"}]
}
