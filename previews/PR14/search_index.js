var documenterSearchIndex = {"docs":
[{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"EditURL = \"https://github.com/Lyceum/LyceumDocs.jl/blob/master/docs/src/examples/NPG.jl\"","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"note: Running examples locally\nThis example and more are also available as Julia scripts and Jupyter notebooks.See the how-to page for more information.","category":"page"},{"location":"examples/NPG/#Learning-a-control-policy-1","page":"Learning a control policy","title":"Learning a control policy","text":"","category":"section"},{"location":"examples/NPG/#Policy-Gradient-Example-1","page":"Learning a control policy","title":"Policy Gradient Example","text":"","category":"section"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"In this example we walk through the process of setting up an experiment that runs Natural Policy Gradient, or more recently in this work. This is an on-policy reinforcement learning method that is comparable to TRPO, PPO, and other policy gradient methods.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"First, let's go head and grab all the dependencies","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"using LinearAlgebra, Random, Statistics # From Stdlib\nusing LyceumAI         # For the NPG controller\nusing LyceumMuJoCo     # For the Hopper environment\nusing LyceumBase.Tools # For the ControllerIterator discussed below\nusing Flux             # For our Neural Network Needs\nusing Flux: glorot_uniform\nusing UniversalLogger\nusing Plots","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"We first configure and instantiate of our Hopper environment to grab useful environment specific values such as the size of the observation and action vectors.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"env = LyceumMuJoCo.HopperV2();\ndobs, dact = length(obsspace(env)), length(actionspace(env));\nnothing #hide","category":"page"},{"location":"examples/NPG/#Policy-Gradient-Components-1","page":"Learning a control policy","title":"Policy Gradient Components","text":"","category":"section"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"Policy Gradient methods require a policy: a function that takes in the state/observations of the agent, and output an action. a = π(obs). In much of Deep RL, the policy takes the form of a neural network, which we instantiate on top of the Flux.jl library. The network below is two layers, mapping from our observation space to 32 hidden units, to the second 32 hidden units layer, before emitting a vector of actions. The activations are all tanh functions, and we initialize the network with Glorot Uniform initializations. The policy is more than just a feed forward nerual network, however. It's treated as a stochastic variable, and thus we track the log of the standard deviation of noise to apply to the action sampling; this is final zero vector of size 'dact'.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"policy = DiagGaussianPolicy(multilayer_perceptron(dobs, 32, 32, dact;\n                                                  σ=tanh,\n                                                  initb=glorot_uniform,\n                                                  initb_final=glorot_uniform),\n                            zeros(dact))\npolicy = Flux.paramtype(Float32, policy); # We make sure the Policy is a consistent type\nnothing #hide","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"This NPG implementation uses Generalized Advantaged Estimation, where we subract the estimate of the current policy's performance from an estimate of the value function on the same inputs. The calculation of the advangate is more stable in for gradient descent. We represent the value function as a neural network as well.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"value = multilayer_perceptron(dobs, 128, 128, 1;\n                              σ=Flux.relu,\n                              initb=glorot_uniform,\n                              initb_final=glorot_uniform)\nvalue = Flux.paramtype(Float32, value); # Again, consistent type; imporant for performance","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"FluxTrainer is an iterator that loops on the Flux object provided. The result at each loop is passed to stopcb below, so you can quit after a number of epochs, convergence, or other criteria; here it's capped at two epochs as a lambda function.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"valueloss(bl, X, Y) = Flux.mse(vec(bl(X)), vec(Y))\nvaluetrainer = FluxTrainer(optimiser = ADAM(1e-3),\n                           szbatch = 64,\n                           lossfn = valueloss,\n                           stopcb = s -> s.nepochs > 2);\nnothing #hide","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"The NaturalPolicyGradient iterator is a struct that contains relevant data objects for learning a policy. We first pass in a constructor that, given an input Int (in this case the thread ID), will construct an env; this is for thread-safe multi-threading. The policy and value objects are passed as well as a number of parameters; shown here are generally good defaults but could change depending on the environment and problem. Finally, the max trajectory length is set as 'Hmax', and the total number of samples 'N', specified. Multi-threading happens to collect the 'N' samples using as many threads as possible, up to a trajectory length of Hmax.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"npg = NaturalPolicyGradient((i)->tconstruct(LyceumMuJoCo.HopperV2, i),\n                            policy,\n                            value,\n                            valuetrainer;\n                            gamma = 0.995,\n                            gaelambda = 0.97,\n                            norm_step_size = 0.05,\n                            Hmax=1000,\n                            N=10000);\nnothing #hide","category":"page"},{"location":"examples/NPG/#Running-Experiments-1","page":"Learning a control policy","title":"Running Experiments","text":"","category":"section"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"Finally, let's spin on our iterator 200 times, plotting every 20 iterations. This lets us break out of the loop if certain conditions are met, or re-start training manually if needed. We of course wish to track results, so we create an Experiment to which we can save data. We also have useful timing information displayed every 20 iterations to understand CPU performance.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"exper = Experiment(\"/tmp/hopper_example.jlso\", overwrite=true)\nlg = ULogger() # walks, talks, and acts like a Julia logger\nfor (i, state) in enumerate(npg)\n    if i > 200\n        # serialize some stuff and quit\n        exper[:policy]     = npg.policy\n        exper[:value]      = npg.value\n        exper[:etype]      = LyceumMuJoCo.HopperV2\n        exper[:meanstates] = state.meanbatch\n        exper[:stocstates] = state.stocbatch\n        break\n    end\n\n    # log everything in `state` except meanbatch and stocbatch\n    push!(lg, :algstate, filter_nt(state, exclude=(:meanbatch, :stocbatch)))\n\n    if mod(i, 20) == 0\n        x = lg[:algstate]\n        # The following are helper functions for plotting to the terminal.\n        # The first plot renders the 'Eval' function associated with the env.\n        display(expplot(Line(x[:stocterminal_eval], \"StocLastE\"),\n                        Line(x[:meanterminal_eval], \"MeanLastE\"),\n                        title=\"Evaluation Score, Iter=$i\", width=60, height=8\n                       ));\n\n        display(expplot(Line(x[:stoctraj_reward], \"StocR\"),\n                        Line(x[:meantraj_reward], \"MeanR\"),\n                        title=\"Reward, Iter=$i\", width=60, height=8\n                       ));\n\n        # The following is timing values for each component of the last iteration.\n        # It's useful to see where the compute is going.\n        println(\"elapsed_sampled  = \", state.elapsed_sampled); #md\n        println(\"elapsed_gradll   = \", state.elapsed_gradll);  #md\n        println(\"elapsed_vpg      = \", state.elapsed_vpg);     #md\n        println(\"elapsed_cg       = \", state.elapsed_cg);      #md\n        println(\"elapsed_valuefit = \", state.elapsed_valuefit);#md\n    end\nend","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"Let's go ahead and plot the final reward trajectory and see how we did. The two lines is a property of a Policy Gradient method: there is a stochastic policy that takes the actions of the policy and adds noise to explore for better behavior.","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"plot!(plot(lg[:algstate][:meantraj_reward], label=\"Mean Policy\", title=\"HopperV2 Reward\"),\n      lg[:algstate][:stoctraj_reward], label=\"Stochastic Policy\")","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"and save the logged results to the experiment's JLSO","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"for (k, v) in get(lg)\n    exper[k] = v\nend\nfinish!(exper);\nnothing #hide","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"","category":"page"},{"location":"examples/NPG/#","page":"Learning a control policy","title":"Learning a control policy","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"EditURL = \"https://github.com/Lyceum/LyceumDocs.jl/blob/master/docs/src/examples/visualize.jl\"","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"note: Running examples locally\nThis example and more are also available as Julia scripts and Jupyter notebooks.See the how-to page for more information.","category":"page"},{"location":"examples/visualize/#Visualizing-Results-1","page":"Visualizing Results","title":"Visualizing Results","text":"","category":"section"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"using LyceumBase, LyceumAI, MuJoCo, Shapes, FastClosures\nusing LyceumMuJoCo    # We need to bring the Pointmass, Hopper Envs into the workspace\nusing LyceumMuJoCoViz # This provides our visualization needs.\nusing JLSO\nusing FastClosures","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"The following functions present two modes of visualizing results of NPG or MPPI packaged in LyceumAI. They both work by passing into the 'visualize' function a control function and/or trajectory data to be rendered. These inputs select from available visualization modes TODO list the visualiza3tion modes here. that operate on either data passed in (the trajectory option) or will simulate the forward dynamics of the environment, and call the pass in control function appropriately. This allows for interactive policy evaluation or full model predictive control with MPPI.","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"function viz_mppi(mppi::MPPI, env::AbstractMuJoCoEnvironment)\n    a = allocate(actionspace(env))\n    o = allocate(obsspace(env))\n    s = allocate(statespace(env))\n\n    ctrlfn = @closure env -> (getstate!(s, env); getaction!(a, s, o, mppi); setaction!(env, a))\n\n    # The above line is functionally the same as:\n    # ctrlfn(env) = begin\n    #     getstate!(s, env)\n    #     getaction!(a, s, o, mppi)\n    #     setaction!(env, a)\n    # end\n    visualize(env, controller=ctrlfn)\nend\n\nfunction viz_policy(path::AbstractString, etype::Union{Nothing, Type{<:AbstractMuJoCoEnvironment}}=nothing)\n    x = JLSO.load(path)\n    etype = isnothing(etype) ? x[\"etype\"] : etype\n    env = etype() # Load the environment based on what was in the JLSO file.\n\n    # Check if the JLSO has a policy saved, if so, load it and prepare to render\n    # the stochastic rollouts of the policy, and form a control function that is the policy's\n    # mean outputs w.r.t. observations.\n    # Otherwise, just render states, such as the output of MPPI.\n    pol = haskey(x, \"policy\") ? x[\"policy\"] : nothing\n\n    if pol == nothing\n        visualize(env; trajectories=[x[\"states\"]])\n    else\n        a = allocate(actionspace(env))\n        o = allocate(obsspace(env))\n        ctrlfn = @closure (env) -> (getobs!(o, env); a .= pol(o); setaction!(env, a))\n\n        states = x[\"stocstates\"].states\n        visualize(env, controller=ctrlfn, trajectories=states)\n    end\nend","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"Assuming we have the saved jlso files from the previous examples, we can call the above functions as such:","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"TODO the following lines are bullshit; don't run on build, add to md scripts and notebook","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"viz_policy(\"/tmp/hopper_example.jlso\", LyceumMuJoCo.HopperV2)\nviz_policy(\"/tmp/opt_humanoid.jlso\", Humanoid)\nviz_mppi(mppi, LyceumMuJoCo.PointMass())","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"","category":"page"},{"location":"examples/visualize/#","page":"Visualizing Results","title":"Visualizing Results","text":"This page was generated using Literate.jl.","category":"page"},{"location":"example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"EditURL = \"@__REPO_ROOT_URL__/docs/src/example_howto.md\"","category":"page"},{"location":"example_howto/#Running-Examples-Locally-1","page":"Running Examples Locally","title":"Running Examples Locally","text":"","category":"section"},{"location":"example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"All examples can be found as Julia scripts and Jupyter notebooks in a self-contained Julia project which is available here: examples.tar.gz.","category":"page"},{"location":"example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"Once downloaded, extract the archive with your tool of choice. On Linux machines, you can run:","category":"page"},{"location":"example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"tar xzf examples.tar.gz","category":"page"},{"location":"example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"which will produce a folder in the same directory named \"LyceumExamples\". Inside, you'll find a README.md, reproduced below, with further instructions.","category":"page"},{"location":"example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"","category":"page"},{"location":"example_howto/#","page":"Running Examples Locally","title":"Running Examples Locally","text":"@EXAMPLES_README","category":"page"},{"location":"#","page":"Home","title":"Home","text":"EditURL = \"@__REPO_ROOT_URL__/docs/src/index.md\"","category":"page"},{"location":"#Home-1","page":"Home","title":"Home","text":"","category":"section"},{"location":"#Lyceum-1","page":"Home","title":"Lyceum","text":"","category":"section"},{"location":"#Package-Statuses-1","page":"Home","title":"Package Statuses","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"import LyceumDocs: package_table_markdown_nodocs, LYCEUM_PACKAGE_DEFS\npackage_table_markdown_nodocs(LYCEUM_PACKAGE_DEFS)","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"EditURL = \"https://github.com/Lyceum/LyceumDocs.jl/blob/master/docs/src/examples/MPPI.jl\"","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"note: Running examples locally\nThis example and more are also available as Julia scripts and Jupyter notebooks.See the how-to page for more information.","category":"page"},{"location":"examples/MPPI/#Running-a-simple-controller-1","page":"Running a simple controller","title":"Running a simple controller","text":"","category":"section"},{"location":"examples/MPPI/#Introduction-1","page":"Running a simple controller","title":"Introduction","text":"","category":"section"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"In this example we walk through the process of setting up an experiment that runs Model-Predictive Path Integral Control, or \"MPPI\", a Model-Predictive Control method, on a simple PointMass environment.","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"First, let's go head and grab all the dependencies","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"using LinearAlgebra, Random, Statistics # From Stdlib\nusing LyceumAI # For the MPPI controller\nusing LyceumMuJoCo # For the PointMass environment\nusing LyceumBase.Tools # For the ControllerIterator discussed below\nusing Plots","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"TODO LINK REF Next, we'll define an Experiment which we'll use to log the trajectory executed by our controller and save the results to \"/tmp/pointmass.jlso\".","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"exper = Experiment(\"/tmp/pointmass.jlso\", overwrite = true);\nnothing #hide","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"Then we configure and instantiate of our PointMass environment and MPPI controller. See the documention for MPPI to learn more about its parameters. TODO LINK REF","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"env = LyceumMuJoCo.PointMass();\nmppi = MPPI(\n    env_tconstructor = i -> tconstruct(LyceumMuJoCo.PointMass, i),\n    covar0 = Diagonal(0.1^2 * I, size(actionspace(env), 1)),\n    lambda = 0.01,\n    K = 32,\n    H = 10,\n    gamma = 0.99,\n);\nnothing #hide","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"Finally, let's rollout our controller for 1000 timesteps. As discused in the algorithms section TODO LINK REF, AbstractController's are by themselves not iterable, so we wrap them in a ControllerIterator which will apply the controls generated by MPPI to the environment at each timestep. We'll also plot the progress so far to the terminal every 100 timesteps. For clarity, we do not reproduce these plots here, but you'll see them when you run this example locally!","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"iter = ControllerIterator(mppi, env; T = 300, plotiter = 50);\nfor (t, traj) in iter\nend","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"Let's go ahead and plot the final reward trajectory and see how we did:","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"plot(iter.trajectory.rewards)","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"and save the results:","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"for (k, v) in pairs(iter.trajectory)\n    exper[k] = v\nend\nfinish!(exper);\nnothing #hide","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"","category":"page"},{"location":"examples/MPPI/#","page":"Running a simple controller","title":"Running a simple controller","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"EditURL = \"https://github.com/Lyceum/LyceumDocs.jl/blob/master/docs/src/examples/humanoid.jl\"","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"note: Running examples locally\nThis example and more are also available as Julia scripts and Jupyter notebooks.See the how-to page for more information.","category":"page"},{"location":"examples/humanoid/#Creating-a-MuJoCo-Environment-1","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"Using Mujoco, we can create an environment that mostly relies on the defaults of LyceumBase and LyceumMuJoCo to propagate state, action, and observation data. We will have to include a reward and evaluation function, of course, and also show how the functions can be customized for different tasks.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"First we grab our dependencies of the Lyceum ecosystem and other helpful packages","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"using LinearAlgebra, Random, Statistics, UnicodePlots, JLSO\nusing LyceumBase, LyceumAI, LyceumMuJoCo, MuJoCo, UniversalLogger, Shapes\nusing LyceumBase.Tools\nusing Shapes: AbstractVectorShape\nimport LyceumBase: tconstruct\nusing Plots","category":"page"},{"location":"examples/humanoid/#Humanoid-Type-1","page":"Creating a MuJoCo Environment","title":"Humanoid Type","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"This struct is our primary entry into the environment API. As you can see, this simple example only wraps around the underlying simulator. The functions of the LyceumBase API will dispatch on this struct. When an algorithm calls a function such as getobs!, Julia will select from all functions with that name depending on the type of environment that is passed in.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"struct Humanoid{S} <: AbstractMuJoCoEnvironment\n    sim::S\nend","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"The following lines facilliatate construction of the simulation such that multi-threading performance is enabled; we will construct multiple instances of MuJoCo mjData structures which at run time will share the same mjModel struct. Primarily, this points to the xml file and how many mujoco timesteps to skip when doing steps at the environment level.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"Humanoid() = first(tconstruct(Humanoid, 1))\nfunction tconstruct(::Type{Humanoid}, n::Integer)\n    modelpath = joinpath(@__DIR__, \"humanoid.xml\")\n    Tuple(Humanoid(s) for s in tconstruct(MJSim, n, modelpath, skip=2))\nend;\nnothing #hide","category":"page"},{"location":"examples/humanoid/#Customizing-1","page":"Creating a MuJoCo Environment","title":"Customizing","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"The following are helpers for the tasks we'd like to consider. We want the humanoid to stand up, thus we need to grab the model's height, as well as record a laying down position that we can use to set the state to. By exploring the model in the REPL or xml we can see that qpos[3] is the index for the z-axis (height) of the root joint. The LAYING_QPOS data was collected externally by posing the model into a supine pose; one can use simulate.cpp included with a MuJoCo release to do this, if desired, or use LyceumMuJoCoViz as well.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"_getheight(shapedstate::ShapedView, ::Humanoid) = shapedstate.qpos[3]\nconst LAYING_QPOS=[-0.164158, 0.0265899, 0.101116, 0.684044, -0.160277,\n                   -0.70823, -0.0693176, -0.1321, 0.0203937, 0.298099,\n                   0.0873523, 0.00634907, 0.117343, -0.0320319, -0.619764,\n                   0.0204114, -0.157038, 0.0512385, 0.115817, -0.0320437,\n                   -0.617078, -0.00153819, 0.13926, -1.01785, -1.57189,\n                   -0.0914509, 0.708539, -1.57187];\nnothing #hide","category":"page"},{"location":"examples/humanoid/#Lyceum-API-Simple-Setup-1","page":"Creating a MuJoCo Environment","title":"Lyceum API Simple Setup","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"LyceumBase requires access to the underlying simulator, thus any LyceumMuJoCo environments need to point to the correct field in the env struct that is the simulator; in our case here there's only one field.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"LyceumMuJoCo.getsim(env::Humanoid) = env.sim","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"Normally we could rely on MuJoCo to reset the model to the default configuration when the model XML is loaded; in humanoid.xml's case, it is in a vertical position. To reset the model to our laying down or supine pose, we can copy in the data from the const array above to d.qpos. Calling forward! here is the same as mj_forward(m,d), for a pure MuJoCo reference.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"function LyceumMuJoCo.reset!(env::Humanoid)\n    reset!(env.sim)\n    env.sim.d.qpos .= LAYING_QPOS\n    forward!(env.sim)\n    env\nend","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"This reward function uses the _getheight helper function above to get the model's height when the function is called. We also specify a target height of 1.25, and penalize the agent for deviating from the target height. There is also a small penalty for using large control activations; if the coefficient is made larger, the agent may not move at all!","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"function LyceumMuJoCo.getreward(state, action, obs, env::Humanoid)\n    height = _getheight(statespace(env)(state), env)\n    target = 1.25\n\n    reward = 1.0\n    if height < target\n        reward -= 2.0*abs(target - height)\n    end\n\n    reward -= 1e-3*norm(action)^2\n\n    return reward\nend","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"Finally, we can specify an evaluation function. The difference between the eval and reward functions are that we can track a useful value, such as height with geteval, but an algorithm like MPPI or NPG may need a shaped function to guide any optimization. Plotting this eval function will show the agent's height over time: this is very useful for reviewing actual desired behavior regardless of the reward achieved, as it can be used to diagnose reward specification problems. The function signature isn't typed to allow for flexibility with algorithms. In this case, because we know what data we will extract, we can specify that there are two Any type inputs that are not labelled just to match the function signature.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"function LyceumMuJoCo.geteval(state, ::Any, ::Any, env::Humanoid)\n    return _getheight(statespace(env)(state), env)\nend","category":"page"},{"location":"examples/humanoid/#Running-Experiments-1","page":"Creating a MuJoCo Environment","title":"Running Experiments","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"Julia performs better when functions are well scoped. Here we construct the MPPI and ControllerIterator objects within a function so they are not global. The MPPI struct accepts an environment constructor and algorithm parameters, and runs the controller. Putting the algorithm in a function allows a user to quickly iterate through parameter searching, or using packages such as Revise can seemlessly allow for reloading of algorithms in development.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"function hmMPPI(etype=Humanoid; T=200, H=64, K=64)\n    env = etype()\n\n    # The following parameters work well for this get-up tasks, and make work for\n    # other similar tasks, but is not invariant to the model.\n    mppi = MPPI(\n                env_tconstructor = i -> tconstruct(etype, i),\n                covar0 = Diagonal(0.05^2*I, size(actionspace(env), 1)),\n                lambda = 0.4,\n                H = H,\n                K = K,\n                gamma = 1.0\n               )\n\n    iter = ControllerIterator(mppi, env; T=T, plotiter=div(T, 10))\n    # We can time the following loop; if it ends up less than the time the\n    # MuJoCo models integrated forward in, then one could conceivably run this\n    # MPPI MPC controller interactively...\n    @time for (t, traj) in iter\n    end\n\n    savepath = \"/tmp/opt_humanoid.jlso\"\n    exper = Experiment(savepath, overwrite=true)\n    exper[:etype] = etype\n\n    for (k, v) in pairs(iter.trajectory)\n        exper[k] = v\n    end\n    finish!(exper)\n\n    return mppi, iter\nend\n\nm, d = hmMPPI()","category":"page"},{"location":"examples/humanoid/#Checking-Results-1","page":"Creating a MuJoCo Environment","title":"Checking Results","text":"","category":"section"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"The MPPI algorithm, and any that you develop, can and should use plotting tools to track progress as they go. IF one wanted to review the results after training, or prepare plots for presentations, one can load the data from disk instead.","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"md x = JLSO.load(\"/tmp/opt_humanoid.jlso\") # one can load the results as such\nplot!(plot(d.trajectory.rewards, label=\"Inst. Reward\", title=\"Humanoid Standup\"),\n      d.trajectory.evaluations, label=\"Evaluation\")","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"","category":"page"},{"location":"examples/humanoid/#","page":"Creating a MuJoCo Environment","title":"Creating a MuJoCo Environment","text":"This page was generated using Literate.jl.","category":"page"}]
}
