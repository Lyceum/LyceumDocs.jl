{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "In this example we walk through the process of setting up an experiment\n",
    "that runs [Natural Policy Gradient](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf)\n",
    "(or more recently [in this work](https://arxiv.org/pdf/1703.02660.pdf)).\n",
    "This is an on-policy reinforcement learning method that is comparable to TRPO,\n",
    "PPO, and other policy gradient methods. See the documentation for `NaturalPolicyGradient`\n",
    "for full implementation details."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "First, let's go head and grab all the dependencies:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using LinearAlgebra, Random, Statistics # From Stdlib\n",
    "using LyceumAI                          # For the NPG controller\n",
    "using LyceumMuJoCo                      # For the Hopper environment\n",
    "using LyceumBase.Tools                  # For the ControllerIterator discussed below\n",
    "using Flux                              # For our neural networks needs\n",
    "using UniversalLogger                   # For logging experiment data\n",
    "using Plots                             # For plotting the results"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "We first instantiate a `HopperV2` environment to grab useful\n",
    "environment-specific values, such as the size of the observation and action vectors:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "env = LyceumMuJoCo.HopperV2();\n",
    "dobs, dact = length(obsspace(env)), length(actionspace(env));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "We'll also seed the per-thread global RNGs:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "seed_threadrngs!(1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Policy Gradient methods require a policy: a function that takes in the state/observations\n",
    "of the agent, and outputs an action i.e. `action = π(obs)`. In much of Deep RL, the\n",
    "policy takes the form of a neural network which can be built on top of the\n",
    "[Flux.jl](https://github.com/FluxML/Flux.jl) library. We utilize a stochastic policy in\n",
    "this example. Specifically, our policy is represented as a multivariate Gaussian\n",
    "distribution of the form:\n",
    "$$\n",
    "\\pi(a | s) = \\mathcal{N}(\\mu_{\\theta_1}(\\text{obs}), \\Sigma_{\\theta_2})\n",
    "$$\n",
    "where ``\\mu_{\\theta_1}`` is a neural network, parameterized by ``\\theta_1``, that maps\n",
    "an observation `obs` to a mean action and ``\\Sigma_{\\theta_2}`` is a diagonal\n",
    "covariance matrix parameterized by ``\\theta_2``, the diagonal entries of the matrix.\n",
    "For ``\\mu_{\\theta_1}`` we utilize a 2-layer neural network, where each layer has a \"width\"\n",
    "of 32. We use tanh activations for each hidden layer and initialize the network weights\n",
    "with Glorot Uniform initializations. Rather than tracking ``\\Sigma_{\\theta_2}`` directly,\n",
    "we track the log standard deviations, which are easier to learn. We initialize\n",
    "``\\log \\text{diag}(\\Sigma_{\\theta_2})`` as `zeros(dact)`, i.e. a `Vector` of length `dact`,\n",
    "initialized to 0. Both ``\\theta_1`` and ``\\theta_2`` are learned in this example.\n",
    "Note that ``\\mu_{\\theta_1}`` is a _state-dependent_ mean while ``\\Sigma_{\\theta_2}``\n",
    "is a _global_ covariance."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "const policy = DiagGaussianPolicy(\n",
    "    multilayer_perceptron(\n",
    "        dobs,\n",
    "        32,\n",
    "        32,\n",
    "        dact;\n",
    "        σ = tanh,\n",
    "        initb = Flux.glorot_uniform,\n",
    "        initb_final = Flux.glorot_uniform,\n",
    "        dtype = Float32,\n",
    "    ),\n",
    "    zeros(Float32, dact),\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "This NPG implementation uses [Generalized Advantaged Estimation](https://arxiv.org/pdf/1506.02438.pdf),\n",
    "which requires an estimate of the value function, `value(state)`, which we\n",
    "represent using a 2-layer, feedforward neural network where each layer has a width of\n",
    "128 and uses the ReLU activation function. The model weights are initialized using\n",
    "Glorot Uniform initialization as above."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "const value = multilayer_perceptron(\n",
    "    dobs,\n",
    "    128,\n",
    "    128,\n",
    "    1;\n",
    "    σ = Flux.relu,\n",
    "    initb = Flux.glorot_uniform,\n",
    "    initb_final = Flux.glorot_uniform,\n",
    "    dtype = Float32,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Next, we set up the optimization pipeline for `value`. We use a mini-batch size of 64\n",
    "and the [ADAM](https://arxiv.org/pdf/1412.6980.pdf) optimizer. `FluxTrainer` is an\n",
    "iterator that loops on the model provided, performing a single step of gradient\n",
    "descent at each iteration. The result at each loop is passed to `stopcb` below, so you\n",
    "can quit after a number of epochs, convergence, or other criteria; here it's capped at\n",
    "two epochs. See the documentation for `FluxTrainer` for more information."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "valueloss(bl, X, Y) = Flux.mse(vec(bl(X)), vec(Y))\n",
    "stopcb(x) = x.nepochs > 2\n",
    "const valuetrainer = FluxTrainer(\n",
    "    optimiser = ADAM(1e-3),\n",
    "    szbatch = 64,\n",
    "    lossfn = valueloss,\n",
    "    stopcb = stopcb\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The `NaturalPolicyGradient` iterator is a type that pre-allocates all necesary\n",
    "data structures and performs one gradient update to `policy` at each iteration.\n",
    "We first pass in a constructor that given `n` returns `n` instances of\n",
    "`LyceumMuJoCo.HopperV2`, all sharing the same `jlModel`, to allow `NaturalPolicyGradient`\n",
    "to allocate per-thread environments and enable performant, parallel sampling from\n",
    "`policy`. We then pass in the `policy`, `value`, and `valuetrainer` instances\n",
    "constructed above and override a few of the default `NaturalPolicyGradient` parameters:\n",
    "`gamma`, `gaelambda`, and `norm_step_size`. Finally, we set the max trajectory length\n",
    "`Hmax` and total number of samples per iteration, `N`. Under the hood,\n",
    "`NaturalPolicyGradient` will use approximately `div(N, Hmax)` threads to perform the\n",
    "sampling."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "const npg = NaturalPolicyGradient(\n",
    "    n -> tconstruct(LyceumMuJoCo.HopperV2, n),\n",
    "    policy,\n",
    "    value,\n",
    "    valuetrainer;\n",
    "    gamma = 0.995,\n",
    "    gaelambda = 0.97,\n",
    "    norm_step_size = 0.05,\n",
    "    Hmax = 1000,\n",
    "    N = 10240,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Finally, let's spin on our iterator 200 times, plotting every 20 iterations.\n",
    "This lets us break out of the loop if certain conditions are met, or re-start training\n",
    "manually if needed. We of course wish to track results, so we create a `ULogger` and\n",
    "`Experiment` to which we can save data. We also have useful timing information displayed\n",
    "every 20 iterations to better understand the performance of our algorithm and identify\n",
    "any potential bottlenecks. Rather than iterating on `npg` at the global scope, we'll\n",
    "do it inside of a function to avoid the performance issues associated with global\n",
    "variables as discussed in the\n",
    "[Julia performance tips](https://docs.julialang.org/en/v1/manual/performance-tips/).\n",
    "Note, to keep the Markdown version of this tutorial readable, we skip the plots\n",
    "and performance statistics. To enable them, simply call `hopper_NPG(npg, true)`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function hopper_NPG(npg::NaturalPolicyGradient, plot::Bool)\n",
    "    exper = Experiment(\"/tmp/hopper_example.jlso\", overwrite = true)\n",
    "    # Walks, talks, and acts like a Julia logger. See the UniversalLogger.jl docs for more info.\n",
    "    lg = ULogger()\n",
    "    for (i, state) in enumerate(npg)\n",
    "        if i > 200\n",
    "            # serialize some stuff and quit\n",
    "            exper[:policy] = npg.policy\n",
    "            exper[:value] = npg.value\n",
    "            exper[:etype] = LyceumMuJoCo.HopperV2\n",
    "            exper[:meanstates] = state.meanbatch\n",
    "            exper[:stocstates] = state.stocbatch\n",
    "            break\n",
    "        end\n",
    "\n",
    "        # log everything in `state` except meanbatch and stocbatch\n",
    "        push!(lg, :algstate, filter_nt(state, exclude = (:meanbatch, :stocbatch)))\n",
    "\n",
    "        if plot && mod(i, 20) == 0\n",
    "            x = lg[:algstate]\n",
    "            # The following are helper functions for plotting to the terminal.\n",
    "            # The first plot displays the `geteval` function for our stochastic\n",
    "            # and mean policy rollouts.\n",
    "            display(expplot(\n",
    "                Line(x[:stocterminal_eval], \"StocLastE\"),\n",
    "                Line(x[:meanterminal_eval], \"MeanLastE\"),\n",
    "                title = \"Evaluation Score, Iter=$i\",\n",
    "                width = 60,\n",
    "                height = 8,\n",
    "            ))\n",
    "            # While the second one similarly plots `getreward`.\n",
    "            display(expplot(\n",
    "                Line(x[:stoctraj_reward], \"StocR\"),\n",
    "                Line(x[:meantraj_reward], \"MeanR\"),\n",
    "                title = \"Reward, Iter=$i\",\n",
    "                width = 60,\n",
    "                height = 8,\n",
    "            ))\n",
    "\n",
    "            # The following are timing values for various parts of the Natural Policy Gradient\n",
    "            # algorithm at the last iteration, useful for finding performance bottlenecks\n",
    "            # in the algorithm.\n",
    "            println(\"elapsed_sampled  = \", state.elapsed_sampled)\n",
    "            println(\"elapsed_gradll   = \", state.elapsed_gradll)\n",
    "            println(\"elapsed_vpg      = \", state.elapsed_vpg)\n",
    "            println(\"elapsed_cg       = \", state.elapsed_cg)\n",
    "            println(\"elapsed_valuefit = \", state.elapsed_valuefit)\n",
    "        end\n",
    "    end\n",
    "    exper, lg\n",
    "end\n",
    "exper, lg = hopper_NPG(npg, false);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Let's go ahead and plot the final reward trajectory for our stochastic and mean policies\n",
    "to see how we did:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(\n",
    "    [lg[:algstate][:meantraj_reward] lg[:algstate][:stoctraj_reward]],\n",
    "    labels = [\"Mean Policy\" \"Stochastic Policy\"],\n",
    "    title = \"HopperV2 Reward\",\n",
    "    legend = :bottomright,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "We'll also plot the evaluations:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(\n",
    "    [lg[:algstate][:meantraj_eval] lg[:algstate][:stoctraj_eval]],\n",
    "    labels = [\"Mean Policy\" \"Stochastic Policy\"],\n",
    "    title = \"HopperV2 Eval\",\n",
    "    legend = :bottomright,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Finally, we save the logged results to `exper` for later review:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "exper[:logs] = get(lg)\n",
    "finish!(exper); # flushes everything to disk"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  },
  "kernelspec": {
   "name": "julia-1.3",
   "display_name": "Julia 1.3.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
