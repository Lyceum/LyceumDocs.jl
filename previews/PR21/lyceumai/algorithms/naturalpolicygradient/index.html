<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Natural Policy Gradient · Lyceum</title><link rel="canonical" href="https://docs.lyceum.ml/dev/lyceumai/algorithms/naturalpolicygradient/index.html"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link href="../../../assets/custom.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img src="../../../assets/logo.png" alt="Lyceum logo"/></a><div class="docs-package-name"><span class="docs-autofit">Lyceum</span></div><form class="docs-search" action="../../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../../">Home</a></li><li><span class="tocitem">LyceumBase</span><ul><li><a class="tocitem" href="../../../lyceumbase/abstractenvironment/">AbstractEnvironment</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../../examples/example_howto/">Running Examples Locally</a></li><li><a class="tocitem" href="../../../examples/humanoid/">Creating a MuJoCo Environment</a></li><li><a class="tocitem" href="../../../examples/NPG/">Learning a Control Policy</a></li><li><a class="tocitem" href="../../../examples/visualize/">Using the Visualizer</a></li></ul></li><li><span class="tocitem">LyceumAI</span><ul><li><input class="collapse-toggle" id="menuitem-4-1" type="checkbox"/><label class="tocitem" for="menuitem-4-1"><span class="docs-label">Models</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../models/policies/">Policies</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-2" type="checkbox" checked/><label class="tocitem" for="menuitem-4-2"><span class="docs-label">Algorithms</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Natural Policy Gradient</a></li><li><a class="tocitem" href="../mppi/">MPPI</a></li></ul></li></ul></li><li><span class="tocitem">LyceumMuJoCo</span><ul><li><a class="tocitem" href="../../../lyceummujoco/lyceummujoco/">LyceumMuJoCo</a></li><li><a class="tocitem" href="../../../lyceummujoco/environments/">Environments</a></li></ul></li><li><span class="tocitem">LyceumMuJoCoViz</span><ul><li><a class="tocitem" href="../../../lyceummujocoviz/lyceummujocoviz/">LyceumMuJoCoViz</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">LyceumAI</a></li><li><a class="is-disabled">Algorithms</a></li><li class="is-active"><a href>Natural Policy Gradient</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Natural Policy Gradient</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Natural-Policy-Gradient-1"><a class="docs-heading-anchor" href="#Natural-Policy-Gradient-1">Natural Policy Gradient</a><a class="docs-heading-anchor-permalink" href="#Natural-Policy-Gradient-1" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="LyceumAI.NaturalPolicyGradient" href="#LyceumAI.NaturalPolicyGradient"><code>LyceumAI.NaturalPolicyGradient</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">NaturalPolicyGradient{DT&lt;:AbstractFloat}(args...; kwargs...) -&gt; NaturalPolicyGradient
NaturalPolicyGradient(args...; kwargs...) -&gt; NaturalPolicyGradient</code></pre><p>Construct an instance of <code>NaturalPolicyGradient</code> with <code>args</code> and <code>kwargs</code>, where <code>DT &lt;: AbstractFloat</code> is the element type used for pre-allocated buffers, which defaults to Float32.</p><p>In the following explanation of the <code>NaturalPolicyGradient</code> constructor, we use the following notation/definitions:</p><ul><li><code>dim_o = length(obsspace(env))</code></li><li><code>dim_a = length(actionspace(env))</code></li><li>&quot;terminal&quot; (e.g. terminal observation) refers to timestep <code>T + 1</code> for a length <code>T</code> trajectory.</li></ul><p><strong>Arguments</strong></p><ul><li><code>env_tconstructor</code>: a function with signature <code>env_tconstructor(n)</code> that returns <code>n</code>   instances of <code>T</code>, where <code>T &lt;: AbstractEnvironment</code>.</li><li><code>policy</code>: a function mapping observations to actions, with the following signatures:<ul><li><code>policy(obs::AbstractVector)</code> –&gt; <code>action::AbstractVector</code>,   where <code>size(obs) == (dim_o, )</code> and <code>size(action) == (dim_a, )</code>.</li><li><code>policy(obs::AbstractMatrix)</code> –&gt; <code>action::AbstractMatrix</code>,   where <code>size(obs) == (dim_o, N)</code> and <code>size(action) == (dim_a, N)</code>.</li></ul></li><li><code>value</code>: a function mapping observations to scalar rewards, with the following signatures:<ul><li><code>value(obs::AbstractVector)</code> –&gt; <code>reward::Real</code>, where <code>size(obs) == (dim_o, )</code></li><li><code>value(obs::AbstractMatrix)</code> –&gt; <code>reward::AbstractVector</code>,   where <code>size(obs) == (dim_o, N)</code> and <code>size(reward) == (N, )</code>.</li></ul></li><li><code>valuefit!</code>: a function with signature <code>valuefit!(value, obs::AbstractMatrix, returns::AbstractVector)</code>,   where <code>size(obs) == (dim_o, N)</code> and <code>size(returns) == (N, )</code>, that fits <code>value</code> to   <code>obs</code> and <code>returns</code>.</li></ul><p><strong>Keywords</strong></p><ul><li><code>Hmax::Integer</code>: Maximum trajectory length for environments rollouts.</li><li><code>N::Integer</code>: Total number of data samples used for each policy gradient step.</li><li><code>Nmean::Integer</code>: Total number of data samples for the mean policy (without stochastic   noise). Mean rollouts are used for evaluating <code>policy</code> and not used to improve <code>policy</code>   in any form.</li><li><code>norm_step_size::Real</code>: Scaling for the applied gradient update after gradient normalization   has occured. This process makes training much more stable to step sizes;   see equation 5 in <a href="https://arxiv.org/pdf/1703.02660.pdf">this paper</a> for more details.</li><li><code>gamma::Real</code>: Reward discount, applied as <code>gamma^(t - 1) * reward[t]</code>.</li><li><code>gaelambda::Real</code>: Generalized Advantage Estimate parameter, balances bias and variance when   computing advantages. See <a href="https://arxiv.org/pdf/1506.02438.pdf">this paper</a> for details.</li><li><code>max_cg_iter::Integer</code>: Maximum number of Conjugate Gradient iterations when estimating   <code>natural_gradient = alpha * inv(FIM) * gradient</code>, where <code>FIM</code> is the Fisher Information Matrix.</li><li><code>cg_tol::Real</code>: Numerical tolerance for Conjugate Gradient convergence.</li><li><code>whiten_advantages::Bool</code>: if <code>true</code>, apply statistical whitening to calculated advantages   (resulting in <code>mean(returns) ≈ 0 &amp;&amp; std(returns) ≈ 1</code>).</li><li><code>bootstrapped_nstep_returns::Bool</code>: if <code>true</code>, bootstrap the returns calculation starting   <code>value(terminal_observation)</code> instead of 0. See &quot;Reinforcement Learning&quot; by   Sutton &amp; Barto for further information.</li><li><code>value_feature_op</code>: a function with the below signatures that transforms environment   observations to a set of &quot;features&quot; to be consumed by <code>value</code> and <code>valuefit!</code>:<ul><li><code>value_feature_op(observations::AbstractVector{&lt;:AbstractMatrix}) --&gt; AbstractMatrix</code></li><li><code>value_feature_op(terminal_observations::AbstractMatrix, trajlengths::Vector{&lt;:Integer}) --&gt; AbstractMatrix</code>,   where <code>observations</code> is a vector of observations from each trajectory,   <code>terminal_observations</code> has size <code>(dim_o, number_of_trajectories)</code>, and <code>trajlengths</code>   contains the lengths of each trajectory   (such that <code>trajlengths[i] == size(observations[i], 2)</code>).</li></ul></li></ul><p>For some continuous control tasks, one may consider the following notes when applying <code>NaturalPolicyGradient</code> to new tasks and environments:</p><ol><li>For two policies that both learn to complete a task satisfactorially, the larger one  may not perform significantly better. A minimum amount of representational power is  necessary, but larger networks may not offer quantitative benefits. The same goes for  the value function approximator.</li><li><code>Hmax</code> needs to be sufficiently long for the correct behavior to emerge; <code>N</code> needs to be  sufficiently large that the agent samples useful data. They may also be surprisingly  small for simple tasks. These parameters are the main tunables when applying <code>NaturalPolicyGradient</code>.</li><li>One might consider the <code>norm_step_size</code> and <code>max_cg_iter</code> parameters as the next most  important when initially testing <code>NaturalPolicyGradient</code> on new tasks, assuming <code>Hmax</code> and <code>N</code> are  appropriately chosen for the task. <code>gamma</code> has interaction with <code>Hmax</code>,  while the default value for <code>gaelambda</code> has been empirically found to be stable for a  wide range of tasks.</li></ol><p>For more details, see Algorithm 1 in <a href="https://arxiv.org/pdf/1703.02660.pdf">Towards Generalization and Simplicity in Continuous Control</a>.</p></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../models/policies/">« Policies</a><a class="docs-footer-nextpage" href="../mppi/">MPPI »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 13 February 2020 00:44">Thursday 13 February 2020</span>. Using Julia version 1.3.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
